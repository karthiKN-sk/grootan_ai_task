{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTmbZZnKxhjQ"
      },
      "source": [
        "# Cloning the Fine Tuned Model From my GitHub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0--KYU2xxdsA"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/karthiKN-sk/grootan_ai_task.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdujpcM3xtm9"
      },
      "source": [
        "# Download the Library/Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhF1tps9wXBU"
      },
      "outputs": [],
      "source": [
        "!pip3 install opencv-python ultralytics supervision numpy pandas matplotlib rich.progress transformers gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting HF Token in Environment variable"
      ],
      "metadata": {
        "id": "hdipK6N5IKIA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH4YRyCdIGvS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "token = None\n",
        "with open(\"variables.py\", \"r\") as f:\n",
        "    for line in f:\n",
        "        if line.startswith(\"HF_TOKEN=\"):\n",
        "            token = line.strip().split(\"=\", 1)[1]\n",
        "            break\n",
        "os.environ[\"HF_TOKEN\"]= token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwBPpqTxQ9HG"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nfkW4B2RBa6"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import supervision as sv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from rich.progress import Progress\n",
        "from typing import Dict, Iterable, List, Optional, Set, Any\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95wvjpMzQqAk"
      },
      "source": [
        "# Configuration for Zone Definitions, Color Palette, and Vehicle Turn Classification.\n",
        "\n",
        "This code snippet sets up essential constants and configurations for a vehicle tracking and turn analysis system, including color settings, polygonal zone definitions, naming schemes, turn mapping logic, and a utility function for turn-based color annotation.\n",
        "\n",
        "1. Color Palette Definition\n",
        "    * Defines a reusable color palette used for drawing zones, bounding boxes, and labels.\n",
        "\n",
        "2. Zone Definitions (as Polygons)\n",
        "\n",
        "    * ZONE_IN_POLYGONS: List of polygonal coordinates marking entry zones for vehicles.\n",
        "\n",
        "    * ZONE_OUT_POLYGONS: List of polygonal coordinates marking exit zones for vehicles.\n",
        "\n",
        "    * Each polygon is an array of 2D points (x, y).\n",
        "\n",
        "3. Zone Naming\n",
        "    * Provides readable labels for each entry and exit zone, mapped in order to the defined polygons.\n",
        "4. Vehicle Turn Color Function(**get_color_for_turn**)\n",
        "\n",
        "    * Returns a specific color for each type of turn:\n",
        "\n",
        "        * Right Turn → Red\n",
        "\n",
        "        * Left Turn → Green\n",
        "\n",
        "        * U-turn → Black\n",
        "\n",
        "        * Straight → Blue\n",
        "5. Turn Mapping Logic\n",
        "\n",
        "      * Maps combinations of entry (In1,2,3,4) and exit (Out1,2,3,4) zones to specific turn types (right_turn, left_turn, u_turn, straight).\n",
        "\n",
        "      * Used to classify vehicle movement patterns across zones.\n",
        "      \n",
        "This setup provides a foundational configuration layer for visual annotation, spatial zone management, and logical turn analysis in a computer vision-based traffic monitoring system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkVRIaZiQmA-"
      },
      "outputs": [],
      "source": [
        "COLORS = sv.ColorPalette.from_hex([\"#E6194B\", \"#3CB44B\", \"#FFE119\", \"#3C76D1\"])\n",
        "\n",
        "ZONE_IN_POLYGONS = [\n",
        "    np.array([[1220, 360], [1500,600], [1600, 420], [1350, 200]]),\n",
        "    np.array([[680, 260], [820, 380], [1020, 220], [880, 80]]),\n",
        "    np.array([[480, 700], [680, 900], [800, 800], [660, 600]]),\n",
        "    np.array([[1180, 1060], [1380, 880], [1200, 740], [1050, 1010]]),\n",
        "]\n",
        "\n",
        "ZONE_OUT_POLYGONS = [\n",
        "    np.array([[950, 120], [1200, 350], [1340, 180], [1160, 20]]),\n",
        "    np.array([[650, 260], [420, 480], [560, 620], [820, 400]]),\n",
        "    np.array([[680, 920], [850, 1050], [1050, 1000], [820, 800]]),\n",
        "    np.array([[1380, 860],[1220, 720],[1500, 620],[1640, 720],]),\n",
        "]\n",
        "\n",
        "ZONE_IN_NAMES = [\"In1\", \"In2\", \"In3\", \"In4\"]\n",
        "ZONE_OUT_NAMES = [\"Out1\", \"Out2\", \"Out3\", \"Out4\"]\n",
        "\n",
        "def get_color_for_turn(tracker_id, vehicle_turns):\n",
        "    turn = vehicle_turns.get(tracker_id)\n",
        "    if turn == \"right_turn\":\n",
        "        return sv.Color.RED\n",
        "    elif turn == \"left_turn\":\n",
        "        return sv.Color.GREEN\n",
        "    elif turn == \"u_turn\":\n",
        "        return sv.Color.BLACK\n",
        "    else:\n",
        "        return sv.Color.BLUE\n",
        "\n",
        "TURN_MAPPING = {\n",
        "    \"In1\": {\"Out2\": \"right_turn\", \"Out4\": \"left_turn\", \"Out3\": \"straight\", \"Out1\": \"u_turn\"},\n",
        "    \"In2\": {\"Out3\": \"right_turn\", \"Out1\": \"left_turn\", \"Out4\": \"straight\", \"Out2\": \"u_turn\"},\n",
        "    \"In3\": {\"Out4\": \"right_turn\", \"Out2\": \"left_turn\", \"Out1\": \"straight\", \"Out3\": \"u_turn\"},\n",
        "    \"In4\": {\"Out1\": \"right_turn\", \"Out3\": \"left_turn\", \"Out2\": \"straight\", \"Out4\": \"u_turn\"},\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkO7vlUM-63E"
      },
      "source": [
        "# Global Detection State Initialization for Vehicle Turn Tracking.\n",
        "\n",
        "This snippet initializes a global dictionary named detections_state that is used to persist and manage tracking data across video frames in a vehicle monitoring pipeline.\n",
        "\n",
        "Dictionary Keys:\n",
        "\n",
        "* \"**tracker_id_to_zone_id**\":\n",
        "   Maps each unique vehicle (tracker_id) to the zone ID it first appeared in.\n",
        "   Used to group and identify vehicles during processing.\n",
        "\n",
        "* \"**vehicle_paths**\":\n",
        "Tracks both the entry (in) and exit (out) zones for each vehicle.\n",
        "Format: **{tracker_id: {\"in\": zone_name, \"out\": zone_name}}**.\n",
        "\n",
        "* \"**vehicle_turns**\":\n",
        "Stores the classified type of turn for each vehicle, such as \"left_turn\", \"right_turn\", \"u_turn\", or \"straight\".\n",
        "\n",
        "This stateful object is referenced and updated throughout the processing pipeline to enable accurate vehicle path tracking and turn classification across video frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tWD_02a-7cS"
      },
      "outputs": [],
      "source": [
        "detections_state = {\n",
        "    \"tracker_id_to_zone_id\": {},\n",
        "    \"vehicle_paths\": {},\n",
        "    \"vehicle_turns\": {},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7if-apVfOx0H"
      },
      "source": [
        "# Vehicle Entry-Exit Tracking and Turn Classification Logic\n",
        "\n",
        "This function, update_detections_state, manages and updates the internal state used to track vehicle movement through predefined zones, and classifies the type of turn each vehicle makes (left, right, U-turn, or straight).\n",
        "\n",
        "Key Responsibilities:\n",
        "\n",
        "1. Track Entry Zones:\n",
        "\n",
        "    * Maps each vehicle (via its tracker ID) to the in zone where it first appeared.\n",
        "\n",
        "    * Ensures the entry point is recorded only once per vehicle.\n",
        "\n",
        "2. Track Exit Zones:\n",
        "\n",
        "    * Detects and records the out zone where the vehicle exits.\n",
        "\n",
        "3. Turn Detection:\n",
        "\n",
        "    * Uses predefined TURN_MAPPING to determine the turn type based on zone pairs (in → out).\n",
        "\n",
        "    * Updates the vehicle_turns dictionary only when both zones are known.\n",
        "\n",
        "4. Class ID Assignment:\n",
        "\n",
        "    * Associates each detection with a zone ID for visual annotation by mapping tracker_id to its zone_id.\n",
        "\n",
        "5. Filtering Valid Detections:\n",
        "\n",
        "    * Returns only detections that have been successfully associated with zones (i.e., not class ID -1).\n",
        "\n",
        "This function plays a central role in transforming low-level detection data into high-level vehicle movement understanding, essential for turn analysis in traffic videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IULBAHSO5Cc"
      },
      "outputs": [],
      "source": [
        "def update_detections_state(\n",
        "    detections_all: sv.Detections,\n",
        "    detections_in_zones: List[sv.Detections],\n",
        "    detections_out_zones: List[sv.Detections],\n",
        "    config: Dict[str, Any],\n",
        "    state: Dict[str, Any] = detections_state\n",
        ") -> sv.Detections:\n",
        "    tracker_id_to_zone_id = state[\"tracker_id_to_zone_id\"]\n",
        "    vehicle_paths = state[\"vehicle_paths\"]\n",
        "    vehicle_turns = state.setdefault(\"vehicle_turns\", {})\n",
        "\n",
        "    # --- Assign entry zones and track vehicle IN zone names ---\n",
        "    zone_in_names = list(config[\"zones_in\"].keys())  # cache the keys list\n",
        "    for zone_in_id, detections_in_zone in enumerate(detections_in_zones):\n",
        "        zone_name = zone_in_names[zone_in_id]\n",
        "        for tracker_id in detections_in_zone.tracker_id:\n",
        "            tracker_id_to_zone_id.setdefault(tracker_id, zone_in_id)\n",
        "\n",
        "            # Initialize vehicle path if not already present\n",
        "            vehicle_paths.setdefault(tracker_id, {\"in\": None, \"out\": None})\n",
        "            if vehicle_paths[tracker_id][\"in\"] is None:\n",
        "                vehicle_paths[tracker_id][\"in\"] = zone_name\n",
        "\n",
        "    # --- Count exits grouped by entry zone and track vehicle OUT zone names ---\n",
        "    zone_out_names = list(config[\"zones_out\"].keys())\n",
        "    for zone_out_id, detections_out_zone in enumerate(detections_out_zones):\n",
        "        zone_name = zone_out_names[zone_out_id]\n",
        "        for tracker_id in detections_out_zone.tracker_id:\n",
        "            if tracker_id in tracker_id_to_zone_id:\n",
        "                vehicle_paths.setdefault(tracker_id, {\"in\": None, \"out\": None})\n",
        "                if vehicle_paths[tracker_id][\"out\"] is None:\n",
        "                    vehicle_paths[tracker_id][\"out\"] = zone_name\n",
        "\n",
        "    # --- Detect turns ---\n",
        "    for tracker_id, path in vehicle_paths.items():\n",
        "        in_zone = path[\"in\"]\n",
        "        out_zone = path[\"out\"]\n",
        "        if in_zone and out_zone and tracker_id not in vehicle_turns:\n",
        "            turn_type = TURN_MAPPING.get(in_zone, {}).get(out_zone)\n",
        "            if turn_type:\n",
        "                vehicle_turns[tracker_id] = turn_type\n",
        "\n",
        "    # Assign class_id for drawing/annotation\n",
        "    if len(detections_all) > 0:\n",
        "        detections_all.class_id = np.vectorize(\n",
        "            lambda x: tracker_id_to_zone_id.get(x, -1)\n",
        "        )(detections_all.tracker_id)\n",
        "    else:\n",
        "        detections_all.class_id = np.array([], dtype=int)\n",
        "\n",
        "    return detections_all[detections_all.class_id != -1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEUvpM3lQydq"
      },
      "source": [
        "# Zone Initialization and Centroid Calculation Utilities.\n",
        "\n",
        "This code provides two utility functions used in the vehicle turn detection system:\n",
        "1. **initiate_polygon_zones**:\n",
        "\n",
        "    * Takes a list of zone names and corresponding polygon coordinates.\n",
        "    * Initializes and returns a dictionary mapping each name to a PolygonZone object.\n",
        "    * Each PolygonZone uses the specified polygon shape and a list of triggering_anchors (default: CENTER) to detect when vehicles enter the zone.\n",
        "\n",
        "2. **compute_centroid**:\n",
        "\n",
        "    * Computes the centroid (geometric center) of a given polygon using the mean of its vertex coordinates.\n",
        "    * Returns the result as a sv.Point, which can be used for positioning labels or visual markers in the frame.\n",
        "\n",
        "These functions are key to defining spatial zones for detecting vehicle movement and labeling them appropriately in the video annotation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6ViGjiZQy6w"
      },
      "outputs": [],
      "source": [
        "\n",
        "def initiate_polygon_zones(\n",
        "    names: List[str],\n",
        "    polygons: List[np.ndarray],\n",
        "    triggering_anchors: Iterable[sv.Position] = [sv.Position.CENTER],\n",
        ") -> Dict[str, sv.PolygonZone]:\n",
        "    return {\n",
        "        name: sv.PolygonZone(polygon=polygon, triggering_anchors=triggering_anchors)\n",
        "        for name, polygon in zip(names, polygons)\n",
        "    }\n",
        "\n",
        "def compute_centroid(polygon):\n",
        "    \"\"\"Compute the centroid (mean point) of a polygon.\"\"\"\n",
        "    centroid = np.mean(polygon, axis=0).astype(int)\n",
        "    return sv.Point(*centroid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS4KoQo1PlXc"
      },
      "source": [
        "# Setup Configuration for Vehicle Turn Detection Pipeline.\n",
        "\n",
        "The **setup_video_processor** function initializes and returns a configuration dictionary containing all essential components and parameters needed to process a video for vehicle turn detection. Here's what it sets up:\n",
        "\n",
        "1. Video Input/Output Paths:\n",
        "\n",
        "      * **source_video_path**: Path to the input video.\n",
        "\n",
        "      * **target_video_path**: Optional path to save the processed output.\n",
        "\n",
        "2. Detection Parameters:\n",
        "\n",
        "      * **confidence_threshold**: Minimum confidence level for YOLO model detections.\n",
        "\n",
        "      * **iou_threshold**: IOU threshold used during object tracking.\n",
        "\n",
        "3. Detection and Tracking Tools:\n",
        "\n",
        "      * **model**: A fine-tuned YOLO model for vehicle detection.\n",
        "\n",
        "      * **tracker**: ByteTrack tracker for maintaining vehicle identities.\n",
        "\n",
        "4. Video Metadata:\n",
        "\n",
        "      * **video_info**: Extracts frame rate, resolution, and frame count from the input video.\n",
        "\n",
        "5. Zone Definitions:\n",
        "\n",
        "      * **zones_in** and **zones_out**: Defined polygon areas to detect entry and exit for turn classification.\n",
        "\n",
        "6. Annotation Tools:\n",
        "\n",
        "      * **box_annotator**: Draws bounding boxes on detected vehicles.\n",
        "\n",
        "      * **label_annotator**: Displays vehicle IDs.\n",
        "\n",
        "      * **trace_annotator**: Adds trajectory traces to show vehicle movement paths.\n",
        "\n",
        "7. Detection State Handler:\n",
        "\n",
        "      * **detections_manager**: A function to manage turn state updates.\n",
        "\n",
        "This setup function centralizes the configuration, making it easy to pass all necessary components to the video processing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n6qfJvMPqPB"
      },
      "outputs": [],
      "source": [
        "def setup_video_processor(\n",
        "    source_video_path: str,\n",
        "    target_video_path: Optional[str] = None,\n",
        "    confidence_threshold: float = 0.4,\n",
        "    iou_threshold: float = 0.7,\n",
        ") -> Dict[str, Any]:\n",
        "    return {\n",
        "        \"conf_threshold\": confidence_threshold,\n",
        "        \"iou_threshold\": iou_threshold,\n",
        "        \"source_video_path\": source_video_path,\n",
        "        \"target_video_path\": target_video_path,\n",
        "        \"model\": YOLO(\"/content/grootan_ai_task/models/YoloFineTunedV1.pt\"),\n",
        "        \"tracker\": sv.ByteTrack(),\n",
        "        \"video_info\": sv.VideoInfo.from_video_path(source_video_path),\n",
        "        \"zones_in\": initiate_polygon_zones(ZONE_IN_NAMES,ZONE_IN_POLYGONS),\n",
        "        \"zones_out\": initiate_polygon_zones(ZONE_OUT_NAMES,ZONE_OUT_POLYGONS),\n",
        "        \"box_annotator\": sv.BoxAnnotator(color=COLORS),\n",
        "        \"label_annotator\": sv.LabelAnnotator(color=COLORS, text_color=sv.Color.BLACK),\n",
        "        \"trace_annotator\": sv.TraceAnnotator(\n",
        "            color=COLORS, position=sv.Position.CENTER, trace_length=100, thickness=2\n",
        "        ),\n",
        "        \"detections_manager\": update_detections_state,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HffnMCTqKCu"
      },
      "source": [
        "# Analyze and Visualize Vehicle Turn Statistics\n",
        "\n",
        "The **analyze_turns** function evaluates vehicle turn data to generate a summary of turn behavior and visual insights. Here's what it does:\n",
        "\n",
        "1. Summary Computation:\n",
        "    *  Counts total tracked vehicles.\n",
        "    *  Computes how many made right turns, left turns, U-turns, or went straight.\n",
        "\n",
        "2. Console Output:\n",
        "    *   Prints a summary report of the turn counts to the terminal.\n",
        "\n",
        "3. Data Packaging:\n",
        "    *   Creates a JSON-style dictionary (**turn_message**) containing:\n",
        "        *   A message,\n",
        "        *   Overall turn statistics (**turn_counts**),\n",
        "        *   Individual vehicle turn details by tracker ID (**turn_details**).\n",
        "\n",
        "\n",
        "4. Visualization:\n",
        "\n",
        "      *   Plots a bar chart using **matplotlib** to visually represent the count of each turn type.\n",
        "      *   Saves the chart as **turn_analysis.png** for later use (e.g., appending to video).\n",
        "\n",
        "5. Return:\n",
        "      *   Outputs the structured **turn_message**, suitable for downstream use in reports or QA systems.\n",
        "\n",
        "This function bridges raw detection data with user-friendly output, supporting both analysis and visualization of vehicle behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x9VkqF5qKm2"
      },
      "outputs": [],
      "source": [
        "def analyze_turns(vehicle_turns):\n",
        "    \"\"\"Analyze the turns and create summary statistics\"\"\"\n",
        "    total_vehicles = len(vehicle_turns)\n",
        "    if total_vehicles == 0:\n",
        "        print(\"No vehicles were detected or tracked.\")\n",
        "        return\n",
        "    # Count the different types of turns\n",
        "    right_turns = sum(1 for turns in vehicle_turns.values() if turns == \"right_turn\" )\n",
        "    left_turns = sum(1 for turns in vehicle_turns.values() if turns == \"left_turn\" )\n",
        "    u_turns = sum(1 for turns in vehicle_turns.values() if turns == \"u_turn\" )\n",
        "    no_turns = sum(1 for turns in vehicle_turns.values() if turns == \"straight\")\n",
        "\n",
        "    print(\"\\n--- Turn Analysis Results ---\")\n",
        "    print(f\"Total unique vehicles tracked: {total_vehicles}\")\n",
        "    print(f\"Vehicles making right turns: {right_turns} \")\n",
        "    print(f\"Vehicles making left turns: {left_turns}\")\n",
        "    print(f\"Vehicles making U-turns: {u_turns}\")\n",
        "    print(f\"Vehicles with no detected turns: {no_turns}\")\n",
        "\n",
        "\n",
        "    # Create a visualization\n",
        "    turn_counts = {\n",
        "        'Right Turn': right_turns,\n",
        "        'Left Turn': left_turns,\n",
        "        'U-Turn': u_turns,\n",
        "        'No Turn': no_turns\n",
        "    }\n",
        "\n",
        "    turn_message = {\n",
        "    \"message\": \"Turn Analysis Results completed.\",\n",
        "    \"total_vehicles\": total_vehicles,\n",
        "    \"turn_counts\": {\n",
        "        \"Vehicles making right turns\" : right_turns,\n",
        "        \"Vehicles making left turns\": left_turns,\n",
        "        \"Vehicles making U-turns\": u_turns,\n",
        "        \"Vehicles with no detected turns (Straight)\": no_turns\n",
        "    },\n",
        "     \"turn_details\": [\n",
        "        {\"tracker_id\": tracker_id, \"turn\": turn}\n",
        "        for tracker_id, turn in vehicle_turns.items()\n",
        "     ]\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    colors = ['red', 'green', 'black', 'blue']\n",
        "    plt.bar(turn_counts.keys(), turn_counts.values(), color=colors)\n",
        "    plt.title('Vehicle Turn Analysis')\n",
        "    plt.ylabel('Number of Vehicles')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Add count labels on top of each bar\n",
        "    for i, (key, value) in enumerate(turn_counts.items()):\n",
        "        plt.text(i, value + 0.3, str(value), ha='center')\n",
        "\n",
        "    plt.savefig('turn_analysis.png')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    return turn_message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-dsim3RcgV"
      },
      "source": [
        "# Process and Annotate Video Frames for Vehicle Turn Detection\n",
        "\n",
        "The **process_video** function reads a video frame-by-frame, processes each frame to detect and annotate vehicles, and outputs the results either to a video file or a live display window. Here's what it does:\n",
        "\n",
        "1. Frame Extraction: Uses a frame generator to read frames from the source video.\n",
        "\n",
        "2. Progress Tracking: Displays a live progress bar using the rich library to monitor video processing.\n",
        "\n",
        "3. Annotation Pipeline:\n",
        "    *   For each frame, it calls **process_frame**() to detect vehicles, determine turn behavior, and apply annotations.\n",
        "\n",
        "4. Output Handling:\n",
        "\n",
        "    *   If output path is specified: Saves the annotated video to disk using VideoSink.\n",
        "    *   Otherwise: Displays annotated frames live using OpenCV (**cv2.imshow**).\n",
        "\n",
        "5. Sample Frame Export: Saves a single annotated frame as an image (annotated_output.png) for preview or debugging.\n",
        "\n",
        "6. Returns the dictionary of vehicle turn states (vehicle_turns) tracked during the video processing.\n",
        "\n",
        "This function is the core executor of the turn detection pipeline, enabling both real-time display and file output of the analyzed results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za412PJPRcA4"
      },
      "outputs": [],
      "source": [
        "def process_video(config: Dict[str, Any]) -> None:\n",
        "    frame_generator = sv.get_video_frames_generator(config[\"source_video_path\"])\n",
        "    total = config[\"video_info\"].total_frames\n",
        "\n",
        "    with Progress() as progress:\n",
        "        task = progress.add_task(\"[green]Processing video...\", total=total)\n",
        "\n",
        "        if config[\"target_video_path\"]:\n",
        "            with sv.VideoSink(config[\"target_video_path\"], config[\"video_info\"]) as sink:\n",
        "                saved_sample = False\n",
        "                for frame in frame_generator:\n",
        "                    annotated = process_frame(frame, config)\n",
        "                    sink.write_frame(annotated)\n",
        "                    if not saved_sample:\n",
        "                        cv2.imwrite(\"annotated_output.png\", annotated)\n",
        "                        saved_sample = True\n",
        "                    progress.advance(task)\n",
        "        else:\n",
        "            for frame in frame_generator:\n",
        "                annotated = process_frame(frame, config)\n",
        "                cv2.imshow(\"Processed Video\", annotated)\n",
        "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "                    break\n",
        "                progress.advance(task)\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "    return detections_state[\"vehicle_turns\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNh4F1DGRnav"
      },
      "source": [
        "# Annotate Video Frames with Zone Information and Vehicle Turn Statistics\n",
        "\n",
        "\n",
        "The **annotate_frame** function overlays comprehensive visual annotations on each video frame to aid in understanding vehicle movement and behavior. Here's what it does:\n",
        "\n",
        "1. Draws entry and exit zones (**zones_in and zones_out**) on the frame using polygons and labels with distinct colors.\n",
        "\n",
        "2. Generates labels for each detected vehicle using their tracker IDs (e.g., **\"Car #12\"**).\n",
        "\n",
        "3. Applies multiple annotation layers:\n",
        "    *   Trajectory traces via **trace_annotator**\n",
        "    *   Bounding boxes via **box_annotator**\n",
        "    *   Vehicle ID labels via **label_annotator**\n",
        "\n",
        "4. Computes turn statistics from the global detections_state:\n",
        "    *   Total vehicles tracked\n",
        "    *   Counts of right turns, left turns, U-turns, and straight movements\n",
        "\n",
        "5. Displays summary metrics visually on the frame, including:\n",
        "    *   A detection count badge\n",
        "    *   Fixed-position statistics on turn types, color-coded for clarity (e.g., red for right turns, green for left turns, etc.)\n",
        "\n",
        "This function is central to making the video output interpretable by overlaying both spatial (zones) and behavioral (turn types) information for each detected vehicle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Puz5Hu3mRlus"
      },
      "outputs": [],
      "source": [
        "def annotate_frame(frame: np.ndarray, detections: sv.Detections, config: Dict[str, Any]) -> np.ndarray:\n",
        "    frame_ = frame.copy()\n",
        "\n",
        "    # Draw zones\n",
        "    for i, ((zin_name, zin), (zout_name, zout)) in enumerate(zip(config[\"zones_in\"].items(), config[\"zones_out\"].items())):\n",
        "        color = COLORS.colors[i % len(COLORS.colors)]\n",
        "        zin_anchor = compute_centroid(zin.polygon)\n",
        "        zout_anchor = compute_centroid(zout.polygon)\n",
        "        frame_ = sv.draw_polygon(frame_, zin.polygon, color)\n",
        "        frame_ = sv.draw_text(frame_, text=zin_name, text_anchor=zin_anchor, text_color=color)\n",
        "        frame_ = sv.draw_polygon(frame_, zout.polygon, color)\n",
        "        frame_ = sv.draw_text(frame_, text=zout_name, text_anchor=zout_anchor, text_color=color)\n",
        "\n",
        "    labels = [f\"Car #{id_}\" for id_ in detections.tracker_id]\n",
        "\n",
        "    frame_ = config[\"trace_annotator\"].annotate(frame_, detections)\n",
        "    frame_ = config[\"box_annotator\"].annotate(frame_, detections)\n",
        "    frame_ = config[\"label_annotator\"].annotate(frame_, detections, labels)\n",
        "\n",
        "    # Count the different types of turns\n",
        "    vehicle_turns= detections_state[\"vehicle_turns\"]\n",
        "    total_vehicles = len(vehicle_turns)\n",
        "    right_turns = sum(1 for turns in vehicle_turns.values() if turns == \"right_turn\" )\n",
        "    left_turns = sum(1 for turns in vehicle_turns.values() if turns == \"left_turn\" )\n",
        "    u_turns = sum(1 for turns in vehicle_turns.values() if turns == \"u_turn\" )\n",
        "    no_turns = sum(1 for turns in vehicle_turns.values() if turns == \"straight\")\n",
        "\n",
        "    # Add detection count info\n",
        "    total_count = len(detections)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        f\"Detected: {total_count}\",\n",
        "        sv.Point(50, 50),\n",
        "        background_color=sv.Color.from_hex(\"#FF7F50\")\n",
        "    )\n",
        "    # Draw fixed turn statistics on the center-left of the frame\n",
        "    start_x = 80\n",
        "    start_y = 350\n",
        "    line_spacing = 40\n",
        "    text_color = sv.Color(r=255, g=255, b=255)\n",
        "\n",
        "    # Line 1: Total vehicles tracked\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"Total vehicles tracked: {total_vehicles}\",\n",
        "        text_anchor=sv.Point(start_x + 30, start_y),\n",
        "        background_color=sv.Color.from_hex(\"#DDDDDD\"),\n",
        "    )\n",
        "\n",
        "    # Line 2: Right turns (Red)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"Right turns: {right_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + line_spacing),\n",
        "        background_color=sv.Color(r=255, g=0, b=0),\n",
        "        text_color=text_color\n",
        "    )\n",
        "\n",
        "    # Line 3: Left turns (Green)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"Left turns: {left_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + 2 * line_spacing),\n",
        "        background_color=sv.Color(r=0, g=255, b=0),\n",
        "\n",
        "    )\n",
        "\n",
        "    # Line 4: U-turns (Black)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"U-turns: {u_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + 3 * line_spacing),\n",
        "        background_color=sv.Color(r=0, g=0, b=0),\n",
        "        text_color=text_color\n",
        "    )\n",
        "\n",
        "    # Line 5: No turns (Blue)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"No turns: {no_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + 4 * line_spacing),\n",
        "        background_color=sv.Color(r=0, g=0, b=255),\n",
        "        text_color=text_color\n",
        "    )\n",
        "\n",
        "\n",
        "    return frame_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFjmcj3g-wSx"
      },
      "source": [
        "# Process and Annotate Video Frame for Vehicle Turn Detection\n",
        "\n",
        "The function **process_frame** handles a single video frame in the vehicle turn detection pipeline. Here's a breakdown of its functionality:\n",
        "\n",
        "1. Runs object detection on the input frame using a **YOLO model (from config[\"model\"])**, with specified confidence and IoU thresholds.\n",
        "\n",
        "2. Converts detection results into a standardized format (**sv.Detections**) and forces all detected class IDs to zero (indicating a single-class tracking scenario, like vehicles).\n",
        "\n",
        "3. Updates object tracks using a tracking algorithm (**config[\"tracker\"]**).\n",
        "\n",
        "4. Checks zone entry/exit: For each pair of entry (**zone_in**) and exit (**zone_out**) zones, it filters detections currently inside these zones.\n",
        "\n",
        "5. Filters detections further using a custom **detections_manager** function that processes zone-based transitions to determine vehicle turns.\n",
        "\n",
        "6. Annotates the frame (e.g., drawing bounding boxes and turn labels) using the **annotate_frame** function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RAl0Wo5-vF2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_frame(frame: np.ndarray, config: Dict[str, Any]) -> np.ndarray:\n",
        "    result = config[\"model\"](frame, verbose=False, conf=config[\"conf_threshold\"], iou=config[\"iou_threshold\"])[0]\n",
        "    detections = sv.Detections.from_ultralytics(result)\n",
        "    detections.class_id = np.zeros(len(detections))\n",
        "    detections = config[\"tracker\"].update_with_detections(detections)\n",
        "    detections_in_zones, detections_out_zones = [], []\n",
        "    for zone_in, zone_out in zip(config[\"zones_in\"].values(), config[\"zones_out\"].values()):\n",
        "        in_zone = detections[zone_in.trigger(detections)]\n",
        "        out_zone = detections[zone_out.trigger(detections)]\n",
        "        detections_in_zones.append(in_zone)\n",
        "        detections_out_zones.append(out_zone)\n",
        "\n",
        "    filtered = config[\"detections_manager\"](detections, detections_in_zones, detections_out_zones,config)\n",
        "    return annotate_frame(frame, filtered, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NG4ESNuq9Zs"
      },
      "source": [
        "# Append Turn Analysis Summary Chart to Video Output\n",
        "\n",
        "The function add_final_summary_to_video enhances a processed video by appending a visual summary of vehicle turn statistics at the end. Here's what it does:\n",
        "\n",
        "1. Analyzes turn data using the provided vehicle turn state.\n",
        "\n",
        "2. Loads a bar chart image (turn_analysis.png) that visually represents turn statistics.\n",
        "\n",
        "3. Reads and copies all frames from the original processed video.\n",
        "\n",
        "4. Appends the chart image as static frames for 5 seconds at the end of the video.\n",
        "\n",
        "5. Saves the new video with the summary chart to the specified output path.\n",
        "\n",
        "6. Returns a structured JSON summary of the vehicle turn data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm56mopSq6ts"
      },
      "outputs": [],
      "source": [
        "\n",
        "def add_final_summary_to_video(video_path, vehicle_turns, output_path=\"final_output.mp4\"):\n",
        "    \"\"\"Add a final summary frame to the end of the video\"\"\"\n",
        "\n",
        "    # First analyze the turns\n",
        "    vehicle_turn_json = analyze_turns(vehicle_turns)\n",
        "\n",
        "    # Load the bar chart image\n",
        "    chart_img = cv2.imread(\"turn_analysis.png\")\n",
        "    if chart_img is None:\n",
        "        raise FileNotFoundError(\"turn_analysis.png not found.\")\n",
        "\n",
        "    # Read the original video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Resize chart to match video resolution\n",
        "    chart_img = cv2.resize(chart_img, (width, height))\n",
        "\n",
        "    # Create the output video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Copy all frames from the original video\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        out.write(frame)\n",
        "\n",
        "    # Append chart image as 5 seconds of frames\n",
        "    for _ in range(int(fps * 5)):\n",
        "        out.write(chart_img)\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Final video with chart saved as '{output_path}'\")\n",
        "\n",
        "    return vehicle_turn_json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdmIGrtbq5eV"
      },
      "source": [
        "# Init Video Processing (Full Video-Based Vehicle Turn Detection and Summary Pipeline)\n",
        "\n",
        "This function **run_full_vehicle_turn_pipeline** performs the complete pipeline for analyzing vehicle movements in a video. It:\n",
        "\n",
        "1. Processes the input video using a configured video processor to detect and trace vehicle movements.\n",
        "\n",
        "2. Analyzes vehicle turns (left, right, U-turn, straight) and records them.\n",
        "\n",
        "3. Generates a summary chart of the turn statistics and appends it to the output video.\n",
        "\n",
        "4. Returns a JSON summary of vehicle turn analytics for further use (e.g., visualization or question answering).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek-5O71YrFnp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_full_vehicle_turn_pipeline(\n",
        "    source_video_path: str,\n",
        "    final_output_path: str = \"final_output-sat1.mp4\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs the full pipeline: processes video, tracks turns, and appends summary.\n",
        "    \"\"\"\n",
        "    # Step 1: Setup and process the video\n",
        "    config = setup_video_processor(\n",
        "        source_video_path=source_video_path,\n",
        "        target_video_path=\"output_traced.mp4\"\n",
        "    )\n",
        "    vehicle_turns_state = process_video(config)\n",
        "\n",
        "    # Step 2: Append summary chart to the traced video\n",
        "    vehicle_turn_json = add_final_summary_to_video(\n",
        "        video_path=\"output_traced.mp4\",\n",
        "        vehicle_turns=vehicle_turns_state,\n",
        "        output_path=final_output_path\n",
        "    )\n",
        "    return vehicle_turn_json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewXOvNlM-_26"
      },
      "source": [
        "# Vehicle Turn Detection Summary & AI-Powered Question Answering.\n",
        "**convert_turn_stats_to_text(analysis_result)**:\n",
        "Converts the vehicle turn detection results (a JSON dictionary) into a readable text summary, including:\n",
        "\n",
        "1. Total vehicle count\n",
        "\n",
        "2. Turn type counts (right, left, U-turn, straight)\n",
        "\n",
        "3. Per-vehicle turn information.\n",
        "\n",
        "**Use:**\n",
        "This summary is later passed to a language model for answering questions.\n",
        "\n",
        "**create_pipeline(text_data)**:\n",
        "Creates a custom question-answering function qa_pipeline(question) that:\n",
        "\n",
        "1. Takes a natural language question\n",
        "\n",
        "2. Feeds it to Qwen along with the vehicle turn summary\n",
        "\n",
        "3. Returns only the assistant's reply from the model output\n",
        "\n",
        "**Purpose:**\n",
        "This abstracts the model usage so the user can ask follow-up questions based on video analytics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es4286u__GMU"
      },
      "outputs": [],
      "source": [
        "def convert_turn_stats_to_text(analysis_result):\n",
        "    turn_counts = analysis_result.get(\"turn_counts\", {})\n",
        "    turn_details = analysis_result.get(\"turn_details\", [])\n",
        "\n",
        "    summary_text = (\n",
        "        f\"Total Vehicles Tracked: {analysis_result.get('total_vehicles', 0)}\\n\"\n",
        "        f\"Vehicles making right turns: {turn_counts.get('Vehicles making right turns', 0)}\\n\"\n",
        "        f\"Vehicles making left turns: {turn_counts.get('Vehicles making left turns', 0)}\\n\"\n",
        "        f\"Vehicles making U-turns: {turn_counts.get('Vehicles making U-turns', 0)}\\n\"\n",
        "        f\"Vehicles with no detected turns (Straight): {turn_counts.get('Vehicles with no detected turns (Straight)', 0)}\\n\"\n",
        "    )\n",
        "\n",
        "    detail_lines = [\n",
        "        f\"Tracker/Vehicle ID {item['tracker_id']}: {item['turn'].replace('_', ' ').capitalize()}\"\n",
        "        for item in turn_details\n",
        "    ]\n",
        "\n",
        "    details_text = \"\\n\".join(detail_lines)\n",
        "\n",
        "    return f\"{summary_text}\\nIndividual Vehicle Turns:\\n{details_text}\"\n",
        "\n",
        "\n",
        "\n",
        "# Load Qwen model and tokenizer (only once globally)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "generation_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "def create_pipeline(text_data):\n",
        "    \"\"\"\n",
        "    Create a simple function to handle QA using Qwen with the full text_data\n",
        "    \"\"\"\n",
        "    def qa_pipeline(question):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert on vehicle turn analysis.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Here is the analysis report:\\n{text_data}\"},\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ]\n",
        "        response = generation_pipe(messages, max_new_tokens=100)[0]\n",
        "        assistant_response = \"\"\n",
        "        for msg in response['generated_text']:\n",
        "            if msg.get(\"role\") == \"assistant\":\n",
        "                assistant_response = msg.get(\"content\", \"\")\n",
        "                break\n",
        "        return assistant_response\n",
        "\n",
        "    return qa_pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyJSSXxLS5VZ"
      },
      "source": [
        "# Main execution and Gradio UI\n",
        "\n",
        "---\n",
        "This Code Block allows users to:\n",
        "\n",
        "1. Upload a video for vehicle turn analysis.\n",
        "\n",
        "2. View a processed version of the video.\n",
        "\n",
        "3. Ask natural language questions (e.g., \"How many U-turns?\") and receive answers from an LLM based on the analysis.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "---\n",
        "**gradio** is used to create an interactive web UI for uploading a video, processing it, and asking questions.\n",
        "\n",
        "**tempfile** is used to handle temporary storage of the uploaded video.\n",
        "\n",
        "global variables store:\n",
        "\n",
        "*  **global_turn_json**: the analysis results (as a dictionary).\n",
        "*  **global_pipeline:** the question-answering pipeline based on the analysis text.\n",
        "\n",
        "Analysis Video Block:\n",
        "\n",
        "1. Reads the uploaded video and saves it temporarily.\n",
        "\n",
        "2. Runs the vehicle turn detection pipeline (**run_full_vehicle_turn_pipeline**).\n",
        "\n",
        "3. Converts the resulting data to a readable summary using **convert_turn_stats_to_text**.\n",
        "\n",
        "4. Sets up the question-answering pipeline with **create_pipeline**.\n",
        "\n",
        "5. Returns the path to the processed video and a status message.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ICAxAcnukpS"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import tempfile\n",
        "\n",
        "# Persistent state for document store and pipeline\n",
        "\n",
        "global_pipeline = None\n",
        "global_turn_json = None\n",
        "\n",
        "def analyze_video(video_file_path):\n",
        "    global global_pipeline, global_turn_json\n",
        "\n",
        "    if not video_file_path:\n",
        "        return None, \"Please upload a video file.\"\n",
        "\n",
        "    with open(video_file_path, \"rb\") as source_file:\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as tmp_file:\n",
        "            tmp_file.write(source_file.read())\n",
        "            tmp_video_path = tmp_file.name\n",
        "\n",
        "    final_output_path = \"final_output_with_summary.mp4\"\n",
        "\n",
        "    # Run the full vehicle turn detection pipeline\n",
        "    global_turn_json = run_full_vehicle_turn_pipeline(\n",
        "        source_video_path=tmp_video_path,\n",
        "        final_output_path=final_output_path\n",
        "    )\n",
        "\n",
        "    # # Create document store and pipeline for QA\n",
        "    text_data = convert_turn_stats_to_text(global_turn_json)\n",
        "    global_pipeline = create_pipeline(text_data)\n",
        "\n",
        "\n",
        "    return final_output_path, \"Video analyzed successfully! You can now ask questions.\"\n",
        "\n",
        "def answer_question(user_question):\n",
        "    if not global_pipeline or not global_turn_json:\n",
        "        return \"Please analyze a video first.\"\n",
        "\n",
        "    return global_pipeline(user_question)\n",
        "\n",
        "# Gradio UI\n",
        "video_input = gr.Video(label=\"Upload a video\")\n",
        "analyze_btn = gr.Button(\"Submit & Analyze\")\n",
        "video_output = gr.Video(label=\"Processed Video\")\n",
        "status_output = gr.Textbox(label=\"Status\")\n",
        "\n",
        "question_input = gr.Textbox(label=\"Ask a question (e.g., How many U-turns were made?)\")\n",
        "answer_output = gr.Textbox(label=\"Answer\")\n",
        "\n",
        "with gr.Blocks(title=\"Vehicle Turn Detection and QA\") as demo:\n",
        "    gr.Markdown(\"## Vehicle Turn Detection and Q&A\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            video_input.render()\n",
        "            analyze_btn.render()\n",
        "        with gr.Column():\n",
        "            video_output.render()\n",
        "            status_output.render()\n",
        "\n",
        "    analyze_btn.click(fn=analyze_video, inputs=video_input, outputs=[video_output, status_output])\n",
        "\n",
        "    gr.Markdown(\"### Ask a question after analysis:\")\n",
        "    question_input.render()\n",
        "    answer_output.render()\n",
        "\n",
        "    question_input.change(fn=answer_question, inputs=question_input, outputs=answer_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}