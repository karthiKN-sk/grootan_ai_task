{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTmbZZnKxhjQ"
      },
      "source": [
        "# Cloning the Fine Tuned Model From my GitHub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0--KYU2xxdsA"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/karthiKN-sk/grootan_ai_task.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdujpcM3xtm9"
      },
      "source": [
        "# Download the Library/Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhF1tps9wXBU"
      },
      "outputs": [],
      "source": [
        "!pip3 install opencv-python ultralytics supervision numpy matplotlib rich.progress transformers gradio pillow huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwBPpqTxQ9HG"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nfkW4B2RBa6"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import supervision as sv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from rich.progress import Progress\n",
        "from typing import Dict, Iterable, List, Optional, Set, Any\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import os\n",
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdipK6N5IKIA"
      },
      "source": [
        "# Setting HF Token in Environment variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH4YRyCdIGvS"
      },
      "outputs": [],
      "source": [
        "token = None\n",
        "token_file = \"/content/grootan_ai_task/variables.py\"\n",
        "\n",
        "with open(token_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        if line.startswith(\"HF_TOKEN=\"):\n",
        "            token = line.strip().split(\"=\", 1)[1].strip('\"').strip(\"'\")\n",
        "            break\n",
        "\n",
        "if token:\n",
        "    os.environ[\"HF_TOKEN\"] = token\n",
        "    login(token=token)\n",
        "else:\n",
        "    raise ValueError(\"HF_TOKEN not found in variables.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95wvjpMzQqAk"
      },
      "source": [
        "# Configuration for Zone Definitions, Color Palette, and Vehicle Turn Classification.\n",
        "\n",
        "This code snippet sets up essential constants and configurations for a vehicle tracking and turn analysis system, including color settings, polygonal zone definitions, naming schemes, turn mapping logic, and a utility function for turn-based color annotation.\n",
        "\n",
        "1. Color Palette Definition\n",
        "    * Defines a reusable color palette used for drawing zones, bounding boxes, and labels.\n",
        "\n",
        "2. Zone Definitions (as Polygons)\n",
        "\n",
        "    * ZONE_IN_POLYGONS: List of polygonal coordinates marking entry zones for vehicles.\n",
        "\n",
        "    * ZONE_OUT_POLYGONS: List of polygonal coordinates marking exit zones for vehicles.\n",
        "\n",
        "    * Each polygon is an array of 2D points (x, y).\n",
        "\n",
        "3. Turn Mapping Logic\n",
        "\n",
        "      * Maps combinations of entry (In1,2,3,4) and exit (Out1,2,3,4) zones to specific turn types (right_turn, left_turn, u_turn, straight).\n",
        "\n",
        "      * Used to classify vehicle movement patterns across zones.\n",
        "\n",
        "      * **generate_turn_mapping**: Generates a flexible turn mapping for 1‚Äì3 way intersections.\n",
        "        It assigns right_turn, left_turn, straight, or u_turn based on available exits for each entry zone.\n",
        "        The logic is order-based and adapts automatically to the number of zones.\n",
        "    \n",
        "This setup provides a foundational configuration layer for visual annotation, spatial zone management, and logical turn analysis in a computer vision-based traffic monitoring system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkVRIaZiQmA-"
      },
      "outputs": [],
      "source": [
        "COLORS = sv.ColorPalette.from_hex([\"#E6194B\", \"#3CB44B\", \"#FFE119\", \"#3C76D1\"])\n",
        "\n",
        "# Four way turn mapping\n",
        "FOUR_WAY_TURN_MAPPING = {\n",
        "    \"In1\": {\"Out2\": \"right_turn\", \"Out4\": \"left_turn\", \"Out3\": \"straight\", \"Out1\": \"u_turn\"},\n",
        "    \"In2\": {\"Out3\": \"right_turn\", \"Out1\": \"left_turn\", \"Out4\": \"straight\", \"Out2\": \"u_turn\"},\n",
        "    \"In3\": {\"Out4\": \"right_turn\", \"Out2\": \"left_turn\", \"Out1\": \"straight\", \"Out3\": \"u_turn\"},\n",
        "    \"In4\": {\"Out1\": \"right_turn\", \"Out3\": \"left_turn\", \"Out2\": \"straight\", \"Out4\": \"u_turn\"},\n",
        "}\n",
        "\n",
        "# TURN MAPPING FOR 1, 2, 3 WAY\n",
        "def generate_turn_mapping(in_names, out_names):\n",
        "    turn_mapping = {}\n",
        "\n",
        "    for i, in_zone in enumerate(in_names):\n",
        "        turn_mapping[in_zone] = {}\n",
        "        u_turn_out = f\"Out{i+1}\"\n",
        "        other_outs = [out for out in out_names if out != u_turn_out]\n",
        "\n",
        "        if len(other_outs) == 3:\n",
        "            # Assume order: right, straight, left (can be improved with geometric angle mapping)\n",
        "            turn_mapping[in_zone][other_outs[0]] = \"right_turn\"\n",
        "            turn_mapping[in_zone][other_outs[1]] = \"straight\"\n",
        "            turn_mapping[in_zone][other_outs[2]] = \"left_turn\"\n",
        "        elif len(other_outs) == 2:\n",
        "            turn_mapping[in_zone][other_outs[0]] = \"right_turn\"\n",
        "            turn_mapping[in_zone][other_outs[1]] = \"left_turn\"\n",
        "        elif len(other_outs) == 1:\n",
        "            turn_mapping[in_zone][other_outs[0]] = \"straight\"\n",
        "\n",
        "        # Assign u_turn only if other exits exist AND same-numbered Out exists\n",
        "        if u_turn_out in out_names and len(out_names) > 1:\n",
        "            turn_mapping[in_zone][u_turn_out] = \"u_turn\"\n",
        "        elif u_turn_out in out_names and len(out_names) == 1:\n",
        "            # Only one way in and out: it's just a straight path\n",
        "            turn_mapping[in_zone][u_turn_out] = \"straight\"\n",
        "\n",
        "    return turn_mapping\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkO7vlUM-63E"
      },
      "source": [
        "# Global Detection State Initialization for Vehicle Turn Tracking.\n",
        "\n",
        "This snippet initializes a global dictionary named detections_state that is used to persist and manage tracking data across video frames in a vehicle monitoring pipeline.\n",
        "\n",
        "Dictionary Keys:\n",
        "\n",
        "* \"**tracker_id_to_zone_id**\":\n",
        "   Maps each unique vehicle (tracker_id) to the zone ID it first appeared in.\n",
        "   Used to group and identify vehicles during processing.\n",
        "\n",
        "* \"**vehicle_paths**\":\n",
        "Tracks both the entry (in) and exit (out) zones for each vehicle.\n",
        "Format: **{tracker_id: {\"in\": zone_name, \"out\": zone_name}}**.\n",
        "\n",
        "* \"**vehicle_turns**\":\n",
        "Stores the classified type of turn for each vehicle, such as \"left_turn\", \"right_turn\", \"u_turn\", or \"straight\".\n",
        "\n",
        "This stateful object is referenced and updated throughout the processing pipeline to enable accurate vehicle path tracking and turn classification across video frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tWD_02a-7cS"
      },
      "outputs": [],
      "source": [
        "detections_state = {\n",
        "    \"tracker_id_to_zone_id\": {},\n",
        "    \"vehicle_paths\": {},\n",
        "    \"vehicle_turns\": {},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7if-apVfOx0H"
      },
      "source": [
        "# Vehicle Entry-Exit Tracking and Turn Classification Logic\n",
        "\n",
        "This function, update_detections_state, manages and updates the internal state used to track vehicle movement through predefined zones, and classifies the type of turn each vehicle makes (left, right, U-turn, or straight).\n",
        "\n",
        "Key Responsibilities:\n",
        "\n",
        "1. Track Entry Zones:\n",
        "\n",
        "    * Maps each vehicle (via its tracker ID) to the in zone where it first appeared.\n",
        "\n",
        "    * Ensures the entry point is recorded only once per vehicle.\n",
        "\n",
        "2. Track Exit Zones:\n",
        "\n",
        "    * Detects and records the out zone where the vehicle exits.\n",
        "\n",
        "3. Turn Detection:\n",
        "\n",
        "    * Uses predefined TURN_MAPPING to determine the turn type based on zone pairs (in ‚Üí out).\n",
        "\n",
        "    * Updates the vehicle_turns dictionary only when both zones are known.\n",
        "\n",
        "4. Class ID Assignment:\n",
        "\n",
        "    * Associates each detection with a zone ID for visual annotation by mapping tracker_id to its zone_id.\n",
        "\n",
        "5. Filtering Valid Detections:\n",
        "\n",
        "    * Returns only detections that have been successfully associated with zones (i.e., not class ID -1).\n",
        "\n",
        "This function plays a central role in transforming low-level detection data into high-level vehicle movement understanding, essential for turn analysis in traffic videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IULBAHSO5Cc"
      },
      "outputs": [],
      "source": [
        "def update_detections_state(\n",
        "    detections_all: sv.Detections,\n",
        "    detections_in_zones: List[sv.Detections],\n",
        "    detections_out_zones: List[sv.Detections],\n",
        "    config: Dict[str, Any],\n",
        "    state: Dict[str, Any] = detections_state\n",
        ") -> sv.Detections:\n",
        "    tracker_id_to_zone_id = state[\"tracker_id_to_zone_id\"]\n",
        "    vehicle_paths = state[\"vehicle_paths\"]\n",
        "    vehicle_turns = state.setdefault(\"vehicle_turns\", {})\n",
        "\n",
        "    # --- Assign entry zones and track vehicle IN zone names ---\n",
        "    zone_in_names = list(config[\"zones_in\"].keys())  # cache the keys list\n",
        "    for zone_in_id, detections_in_zone in enumerate(detections_in_zones):\n",
        "        zone_name = zone_in_names[zone_in_id]\n",
        "        for tracker_id in detections_in_zone.tracker_id:\n",
        "            tracker_id_to_zone_id.setdefault(tracker_id, zone_in_id)\n",
        "\n",
        "            # Initialize vehicle path if not already present\n",
        "            vehicle_paths.setdefault(tracker_id, {\"in\": None, \"out\": None})\n",
        "            if vehicle_paths[tracker_id][\"in\"] is None:\n",
        "                vehicle_paths[tracker_id][\"in\"] = zone_name\n",
        "\n",
        "    # --- Count exits grouped by entry zone and track vehicle OUT zone names ---\n",
        "    zone_out_names = list(config[\"zones_out\"].keys())\n",
        "    for zone_out_id, detections_out_zone in enumerate(detections_out_zones):\n",
        "        zone_name = zone_out_names[zone_out_id]\n",
        "        for tracker_id in detections_out_zone.tracker_id:\n",
        "            if tracker_id in tracker_id_to_zone_id:\n",
        "                vehicle_paths.setdefault(tracker_id, {\"in\": None, \"out\": None})\n",
        "                if vehicle_paths[tracker_id][\"out\"] is None:\n",
        "                    vehicle_paths[tracker_id][\"out\"] = zone_name\n",
        "\n",
        "    # --- Detect turns ---\n",
        "    for tracker_id, path in vehicle_paths.items():\n",
        "        in_zone = path[\"in\"]\n",
        "        out_zone = path[\"out\"]\n",
        "        if in_zone and out_zone and tracker_id not in vehicle_turns:\n",
        "            turn_type = config[\"turn_mapping\"].get(in_zone, {}).get(out_zone)\n",
        "            if turn_type:\n",
        "                vehicle_turns[tracker_id] = turn_type\n",
        "\n",
        "    # Assign class_id for drawing/annotation\n",
        "    if len(detections_all) > 0:\n",
        "        detections_all.class_id = np.vectorize(\n",
        "            lambda x: tracker_id_to_zone_id.get(x, -1)\n",
        "        )(detections_all.tracker_id)\n",
        "    else:\n",
        "        detections_all.class_id = np.array([], dtype=int)\n",
        "\n",
        "    return detections_all[detections_all.class_id != -1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEUvpM3lQydq"
      },
      "source": [
        "# Zone Initialization and Centroid Calculation Utilities.\n",
        "\n",
        "This code provides two utility functions used in the vehicle turn detection system:\n",
        "1. **initiate_polygon_zones**:\n",
        "\n",
        "    * Takes a list of zone names and corresponding polygon coordinates.\n",
        "    * Initializes and returns a dictionary mapping each name to a PolygonZone object.\n",
        "    * Each PolygonZone uses the specified polygon shape and a list of triggering_anchors (default: CENTER) to detect when vehicles enter the zone.\n",
        "\n",
        "2. **compute_centroid**:\n",
        "\n",
        "    * Computes the centroid (geometric center) of a given polygon using the mean of its vertex coordinates.\n",
        "    * Returns the result as a sv.Point, which can be used for positioning labels or visual markers in the frame.\n",
        "\n",
        "These functions are key to defining spatial zones for detecting vehicle movement and labeling them appropriately in the video annotation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6ViGjiZQy6w"
      },
      "outputs": [],
      "source": [
        "def initiate_polygon_zones(\n",
        "    names: List[str],\n",
        "    polygons: List[np.ndarray],\n",
        "    triggering_anchors: Iterable[sv.Position] = [sv.Position.CENTER],\n",
        ") -> Dict[str, sv.PolygonZone]:\n",
        "    return {\n",
        "        name: sv.PolygonZone(polygon=polygon, triggering_anchors=triggering_anchors)\n",
        "        for name, polygon in zip(names, polygons)\n",
        "    }\n",
        "\n",
        "def compute_centroid(polygon):\n",
        "    \"\"\"Compute the centroid (mean point) of a polygon.\"\"\"\n",
        "    centroid = np.mean(polygon, axis=0).astype(int)\n",
        "    return sv.Point(*centroid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS4KoQo1PlXc"
      },
      "source": [
        "# Setup Configuration for Vehicle Turn Detection Pipeline.\n",
        "\n",
        "The **setup_video_processor** function initializes and returns a configuration dictionary containing all essential components and parameters needed to process a video for vehicle turn detection. Here's what it sets up:\n",
        "\n",
        "1. Video Input/Output Paths:\n",
        "\n",
        "      * **source_video_path**: Path to the input video.\n",
        "\n",
        "      * **target_video_path**: Optional path to save the processed output.\n",
        "\n",
        "2. Detection Parameters:\n",
        "\n",
        "      * **confidence_threshold**: Minimum confidence level for YOLO model detections.\n",
        "\n",
        "      * **iou_threshold**: IOU threshold used during object tracking.\n",
        "\n",
        "3. Detection and Tracking Tools:\n",
        "\n",
        "      * **model**: A fine-tuned YOLO model for vehicle detection.\n",
        "\n",
        "      * **tracker**: ByteTrack tracker for maintaining vehicle identities.\n",
        "\n",
        "4. Video Metadata:\n",
        "\n",
        "      * **video_info**: Extracts frame rate, resolution, and frame count from the input video.\n",
        "\n",
        "5. Zone Definitions:\n",
        "\n",
        "      * **zones_in** and **zones_out**: Defined polygon areas to detect entry and exit for turn classification.\n",
        "\n",
        "6. Annotation Tools:\n",
        "\n",
        "      * **box_annotator**: Draws bounding boxes on detected vehicles.\n",
        "\n",
        "      * **label_annotator**: Displays vehicle IDs.\n",
        "\n",
        "      * **trace_annotator**: Adds trajectory traces to show vehicle movement paths.\n",
        "\n",
        "7. Detection State Handler:\n",
        "\n",
        "      * **detections_manager**: A function to manage turn state updates.\n",
        "\n",
        "This setup function centralizes the configuration, making it easy to pass all necessary components to the video processing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n6qfJvMPqPB"
      },
      "outputs": [],
      "source": [
        "def setup_video_processor(\n",
        "    source_video_path: str,\n",
        "    target_video_path: Optional[str] = None,\n",
        "    zones: Dict[str, list] = {},\n",
        "    confidence_threshold: float = 0.4,\n",
        "    iou_threshold: float = 0.7,\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    if not zones or not zones.get(\"entry\") or not zones.get(\"exit\"):\n",
        "        raise ValueError(\"'zones' must contain both non-empty 'entry' and 'exit' lists.\")\n",
        "\n",
        "    # Convert polygon lists to NumPy arrays\n",
        "    ZONE_IN_POLYGONS = [np.array(polygon, dtype=np.int32) for polygon in zones[\"entry\"]]\n",
        "    ZONE_OUT_POLYGONS = [np.array(polygon, dtype=np.int32) for polygon in zones[\"exit\"]]\n",
        "\n",
        "    ZONE_IN_NAMES = [f\"In{i+1}\" for i in range(len(ZONE_IN_POLYGONS))]\n",
        "    ZONE_OUT_NAMES = [f\"Out{i+1}\" for i in range(len(ZONE_OUT_POLYGONS))]\n",
        "\n",
        "    if len(ZONE_IN_NAMES) != len(ZONE_IN_POLYGONS):\n",
        "        raise ValueError(\n",
        "        f\"Mismatch in entry zones: {len(ZONE_IN_NAMES)} names vs {len(ZONE_IN_POLYGONS)} polygons.\"\n",
        "    )\n",
        "    if len(ZONE_OUT_NAMES) != len(ZONE_OUT_POLYGONS):\n",
        "        raise ValueError(\n",
        "        f\"Mismatch in exit zones: {len(ZONE_OUT_NAMES)} names vs {len(ZONE_OUT_POLYGONS)} polygons.\"\n",
        "    )\n",
        "\n",
        "\n",
        "    TURN_MAPPING = FOUR_WAY_TURN_MAPPING if len(ZONE_IN_NAMES) == 4 and len(ZONE_OUT_NAMES) == 4 else generate_turn_mapping(ZONE_IN_NAMES, ZONE_OUT_NAMES)\n",
        "\n",
        "    return {\n",
        "        \"conf_threshold\": confidence_threshold,\n",
        "        \"iou_threshold\": iou_threshold,\n",
        "        \"source_video_path\": source_video_path,\n",
        "        \"target_video_path\": target_video_path,\n",
        "        \"model\": YOLO(\"/content/grootan_ai_task/models/YoloFineTunedV1.pt\"),\n",
        "        \"tracker\": sv.ByteTrack(),\n",
        "        \"video_info\": sv.VideoInfo.from_video_path(source_video_path),\n",
        "        \"zones_in\": initiate_polygon_zones(ZONE_IN_NAMES,ZONE_IN_POLYGONS),\n",
        "        \"zones_out\": initiate_polygon_zones(ZONE_OUT_NAMES,ZONE_OUT_POLYGONS),\n",
        "        \"box_annotator\": sv.BoxAnnotator(color=COLORS),\n",
        "        \"label_annotator\": sv.LabelAnnotator(color=COLORS, text_color=sv.Color.BLACK),\n",
        "        \"trace_annotator\": sv.TraceAnnotator(\n",
        "            color=COLORS, position=sv.Position.CENTER, trace_length=100, thickness=2\n",
        "        ),\n",
        "        \"detections_manager\": update_detections_state,\n",
        "        \"turn_mapping\": TURN_MAPPING\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HffnMCTqKCu"
      },
      "source": [
        "# Analyze and Visualize Vehicle Turn Statistics\n",
        "\n",
        "The **analyze_turns** function evaluates vehicle turn data to generate a summary of turn behavior and visual insights. Here's what it does:\n",
        "\n",
        "1. Summary Computation:\n",
        "    *  Counts total tracked vehicles.\n",
        "    *  Computes how many made right turns, left turns, U-turns, or went straight.\n",
        "\n",
        "2. Console Output:\n",
        "    *   Prints a summary report of the turn counts to the terminal.\n",
        "\n",
        "3. Data Packaging:\n",
        "    *   Creates a JSON-style dictionary (**turn_message**) containing:\n",
        "        *   A message,\n",
        "        *   Overall turn statistics (**turn_counts**),\n",
        "        *   Individual vehicle turn details by tracker ID (**turn_details**).\n",
        "\n",
        "\n",
        "4. Visualization:\n",
        "\n",
        "      *   Plots a bar chart using **matplotlib** to visually represent the count of each turn type.\n",
        "      *   Saves the chart as **turn_analysis.png** for later use (e.g., appending to video).\n",
        "\n",
        "5. Return:\n",
        "      *   Outputs the structured **turn_message**, suitable for downstream use in reports or QA systems.\n",
        "\n",
        "This function bridges raw detection data with user-friendly output, supporting both analysis and visualization of vehicle behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x9VkqF5qKm2"
      },
      "outputs": [],
      "source": [
        "def analyze_turns(vehicle_turns):\n",
        "    \"\"\"Analyze the turns and create summary statistics\"\"\"\n",
        "    total_vehicles = len(vehicle_turns)\n",
        "    if total_vehicles == 0:\n",
        "        print(\"No vehicles were detected or tracked.\")\n",
        "        return\n",
        "    # Count the different types of turns\n",
        "    right_turns = sum(1 for turns in vehicle_turns.values() if turns == \"right_turn\" )\n",
        "    left_turns = sum(1 for turns in vehicle_turns.values() if turns == \"left_turn\" )\n",
        "    u_turns = sum(1 for turns in vehicle_turns.values() if turns == \"u_turn\" )\n",
        "    no_turns = sum(1 for turns in vehicle_turns.values() if turns == \"straight\")\n",
        "\n",
        "    print(\"\\n--- Turn Analysis Results ---\")\n",
        "    print(f\"Total unique vehicles tracked: {total_vehicles}\")\n",
        "    print(f\"Vehicles making right turns: {right_turns} \")\n",
        "    print(f\"Vehicles making left turns: {left_turns}\")\n",
        "    print(f\"Vehicles making U-turns: {u_turns}\")\n",
        "    print(f\"Vehicles with no detected turns: {no_turns}\")\n",
        "\n",
        "\n",
        "    # Create a visualization\n",
        "    turn_counts = {\n",
        "        'Right Turn': right_turns,\n",
        "        'Left Turn': left_turns,\n",
        "        'U-Turn': u_turns,\n",
        "        'No Turn': no_turns\n",
        "    }\n",
        "\n",
        "    turn_message = {\n",
        "    \"message\": \"Turn Analysis Results completed.\",\n",
        "    \"total_vehicles\": total_vehicles,\n",
        "    \"turn_counts\": {\n",
        "        \"Vehicles making right turns\" : right_turns,\n",
        "        \"Vehicles making left turns\": left_turns,\n",
        "        \"Vehicles making U-turns\": u_turns,\n",
        "        \"Vehicles with no detected turns (Straight)\": no_turns\n",
        "    },\n",
        "     \"turn_details\": [\n",
        "        {\"tracker_id\": tracker_id, \"turn\": turn}\n",
        "        for tracker_id, turn in vehicle_turns.items()\n",
        "     ]\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    colors = ['red', 'green', 'black', 'blue']\n",
        "    plt.bar(turn_counts.keys(), turn_counts.values(), color=colors)\n",
        "    plt.title('Vehicle Turn Analysis')\n",
        "    plt.ylabel('Number of Vehicles')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Add count labels on top of each bar\n",
        "    for i, (key, value) in enumerate(turn_counts.items()):\n",
        "        plt.text(i, value + 0.3, str(value), ha='center')\n",
        "\n",
        "    plt.savefig('turn_analysis.png')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    return turn_message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNh4F1DGRnav"
      },
      "source": [
        "# Annotate Video Frames with Zone Information and Vehicle Turn Statistics\n",
        "\n",
        "\n",
        "The **annotate_frame** function overlays comprehensive visual annotations on each video frame to aid in understanding vehicle movement and behavior. Here's what it does:\n",
        "\n",
        "1. Draws entry and exit zones (**zones_in and zones_out**) on the frame using polygons and labels with distinct colors.\n",
        "\n",
        "2. Generates labels for each detected vehicle using their tracker IDs (e.g., **\"Car #12\"**).\n",
        "\n",
        "3. Applies multiple annotation layers:\n",
        "    *   Trajectory traces via **trace_annotator**\n",
        "    *   Bounding boxes via **box_annotator**\n",
        "    *   Vehicle ID labels via **label_annotator**\n",
        "\n",
        "4. Computes turn statistics from the global detections_state:\n",
        "    *   Total vehicles tracked\n",
        "    *   Counts of right turns, left turns, U-turns, and straight movements\n",
        "\n",
        "5. Displays summary metrics visually on the frame, including:\n",
        "    *   A detection count badge\n",
        "    *   Fixed-position statistics on turn types, color-coded for clarity (e.g., red for right turns, green for left turns, etc.)\n",
        "\n",
        "This function is central to making the video output interpretable by overlaying both spatial (zones) and behavioral (turn types) information for each detected vehicle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Puz5Hu3mRlus"
      },
      "outputs": [],
      "source": [
        "def annotate_frame(frame: np.ndarray, detections: sv.Detections, config: Dict[str, Any]) -> np.ndarray:\n",
        "    frame_ = frame.copy()\n",
        "\n",
        "    # Draw zones\n",
        "    for i, ((zin_name, zin), (zout_name, zout)) in enumerate(zip(config[\"zones_in\"].items(), config[\"zones_out\"].items())):\n",
        "        color = COLORS.colors[i % len(COLORS.colors)]\n",
        "        zin_anchor = compute_centroid(zin.polygon)\n",
        "        zout_anchor = compute_centroid(zout.polygon)\n",
        "        frame_ = sv.draw_polygon(frame_, zin.polygon, color)\n",
        "        frame_ = sv.draw_text(frame_, text=zin_name, text_anchor=zin_anchor, text_color=color)\n",
        "        frame_ = sv.draw_polygon(frame_, zout.polygon, color)\n",
        "        frame_ = sv.draw_text(frame_, text=zout_name, text_anchor=zout_anchor, text_color=color)\n",
        "\n",
        "    labels = [f\"Car #{id_}\" for id_ in detections.tracker_id]\n",
        "\n",
        "    frame_ = config[\"trace_annotator\"].annotate(frame_, detections)\n",
        "    frame_ = config[\"box_annotator\"].annotate(frame_, detections)\n",
        "    frame_ = config[\"label_annotator\"].annotate(frame_, detections, labels)\n",
        "\n",
        "    # Count the different types of turns\n",
        "    vehicle_turns= detections_state[\"vehicle_turns\"]\n",
        "    total_vehicles = len(vehicle_turns)\n",
        "    right_turns = sum(1 for turns in vehicle_turns.values() if turns == \"right_turn\" )\n",
        "    left_turns = sum(1 for turns in vehicle_turns.values() if turns == \"left_turn\" )\n",
        "    u_turns = sum(1 for turns in vehicle_turns.values() if turns == \"u_turn\" )\n",
        "    no_turns = sum(1 for turns in vehicle_turns.values() if turns == \"straight\")\n",
        "\n",
        "    # Add detection count info\n",
        "    total_count = len(detections)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        f\"Detected: {total_count}\",\n",
        "        sv.Point(50, 50),\n",
        "        background_color=sv.Color.from_hex(\"#FF7F50\")\n",
        "    )\n",
        "    # Draw fixed turn statistics on the center-left of the frame\n",
        "    start_x = 80\n",
        "    start_y = 350\n",
        "    line_spacing = 40\n",
        "    text_color = sv.Color(r=255, g=255, b=255)\n",
        "\n",
        "    # Line 1: Total vehicles tracked\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"Total vehicles tracked: {total_vehicles}\",\n",
        "        text_anchor=sv.Point(start_x + 30, start_y),\n",
        "        background_color=sv.Color.from_hex(\"#DDDDDD\"),\n",
        "    )\n",
        "\n",
        "    # Line 2: Right turns (Red)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"Right turns: {right_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + line_spacing),\n",
        "        background_color=sv.Color(r=255, g=0, b=0),\n",
        "        text_color=text_color\n",
        "    )\n",
        "\n",
        "    # Line 3: Left turns (Green)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"Left turns: {left_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + 2 * line_spacing),\n",
        "        background_color=sv.Color(r=0, g=255, b=0),\n",
        "\n",
        "    )\n",
        "\n",
        "    # Line 4: U-turns (Black)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"U-turns: {u_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + 3 * line_spacing),\n",
        "        background_color=sv.Color(r=0, g=0, b=0),\n",
        "        text_color=text_color\n",
        "    )\n",
        "\n",
        "    # Line 5: No turns (Blue)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"No turns: {no_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + 4 * line_spacing),\n",
        "        background_color=sv.Color(r=0, g=0, b=255),\n",
        "        text_color=text_color\n",
        "    )\n",
        "\n",
        "\n",
        "    return frame_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFjmcj3g-wSx"
      },
      "source": [
        "# Process and Annotate Video Frame for Vehicle Turn Detection\n",
        "\n",
        "The function **process_frame** handles a single video frame in the vehicle turn detection pipeline. Here's a breakdown of its functionality:\n",
        "\n",
        "1. Runs object detection on the input frame using a **YOLO model (from config[\"model\"])**, with specified confidence and IoU thresholds.\n",
        "\n",
        "2. Converts detection results into a standardized format (**sv.Detections**) and forces all detected class IDs to zero (indicating a single-class tracking scenario, like vehicles).\n",
        "\n",
        "3. Updates object tracks using a tracking algorithm (**config[\"tracker\"]**).\n",
        "\n",
        "4. Checks zone entry/exit: For each pair of entry (**zone_in**) and exit (**zone_out**) zones, it filters detections currently inside these zones.\n",
        "\n",
        "5. Filters detections further using a custom **detections_manager** function that processes zone-based transitions to determine vehicle turns.\n",
        "\n",
        "6. Annotates the frame (e.g., drawing bounding boxes and turn labels) using the **annotate_frame** function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RAl0Wo5-vF2"
      },
      "outputs": [],
      "source": [
        "def process_frame(frame: np.ndarray, config: Dict[str, Any]) -> np.ndarray:\n",
        "    result = config[\"model\"](frame, verbose=False, conf=config[\"conf_threshold\"], iou=config[\"iou_threshold\"])[0]\n",
        "    detections = sv.Detections.from_ultralytics(result)\n",
        "    detections.class_id = np.zeros(len(detections))\n",
        "    detections = config[\"tracker\"].update_with_detections(detections)\n",
        "    detections_in_zones, detections_out_zones = [], []\n",
        "    for zone_in, zone_out in zip(config[\"zones_in\"].values(), config[\"zones_out\"].values()):\n",
        "        in_zone = detections[zone_in.trigger(detections)]\n",
        "        out_zone = detections[zone_out.trigger(detections)]\n",
        "        detections_in_zones.append(in_zone)\n",
        "        detections_out_zones.append(out_zone)\n",
        "\n",
        "    filtered = config[\"detections_manager\"](detections, detections_in_zones, detections_out_zones,config)\n",
        "    return annotate_frame(frame, filtered, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-dsim3RcgV"
      },
      "source": [
        "# Process and Annotate Video Frames for Vehicle Turn Detection\n",
        "\n",
        "The **process_video** function reads a video frame-by-frame, processes each frame to detect and annotate vehicles, and outputs the results either to a video file or a live display window. Here's what it does:\n",
        "\n",
        "1. Frame Extraction: Uses a frame generator to read frames from the source video.\n",
        "\n",
        "2. Progress Tracking: Displays a live progress bar using the rich library to monitor video processing.\n",
        "\n",
        "3. Annotation Pipeline:\n",
        "    *   For each frame, it calls **process_frame**() to detect vehicles, determine turn behavior, and apply annotations.\n",
        "\n",
        "4. Output Handling:\n",
        "\n",
        "    *   If output path is specified: Saves the annotated video to disk using VideoSink.\n",
        "    *   Otherwise: Displays annotated frames live using OpenCV (**cv2.imshow**).\n",
        "\n",
        "5. Sample Frame Export: Saves a single annotated frame as an image (annotated_output.png) for preview or debugging.\n",
        "\n",
        "6. Returns the dictionary of vehicle turn states (vehicle_turns) tracked during the video processing.\n",
        "\n",
        "This function is the core executor of the turn detection pipeline, enabling both real-time display and file output of the analyzed results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za412PJPRcA4"
      },
      "outputs": [],
      "source": [
        "def process_video(config: Dict[str, Any]) -> None:\n",
        "    frame_generator = sv.get_video_frames_generator(config[\"source_video_path\"])\n",
        "    total = config[\"video_info\"].total_frames\n",
        "\n",
        "    with Progress() as progress:\n",
        "        task = progress.add_task(\"[green]Processing video...\", total=total)\n",
        "        if config[\"target_video_path\"]:\n",
        "            with sv.VideoSink(config[\"target_video_path\"], config[\"video_info\"]) as sink:\n",
        "                saved_sample = False\n",
        "                for frame in frame_generator:\n",
        "                    annotated = process_frame(frame, config)\n",
        "                    sink.write_frame(annotated)\n",
        "                    if not saved_sample:\n",
        "                        cv2.imwrite(\"annotated_output.png\", annotated)\n",
        "                        saved_sample = True\n",
        "                    progress.advance(task)\n",
        "        else:\n",
        "            for frame in frame_generator:\n",
        "                annotated = process_frame(frame, config)\n",
        "                cv2.imshow(\"Processed Video\", annotated)\n",
        "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "                    break\n",
        "                progress.advance(task)\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "    return detections_state[\"vehicle_turns\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NG4ESNuq9Zs"
      },
      "source": [
        "# Append Turn Analysis Summary Chart to Video Output\n",
        "\n",
        "The function add_final_summary_to_video enhances a processed video by appending a visual summary of vehicle turn statistics at the end. Here's what it does:\n",
        "\n",
        "1. Analyzes turn data using the provided vehicle turn state.\n",
        "\n",
        "2. Loads a bar chart image (turn_analysis.png) that visually represents turn statistics.\n",
        "\n",
        "3. Reads and copies all frames from the original processed video.\n",
        "\n",
        "4. Appends the chart image as static frames for 5 seconds at the end of the video.\n",
        "\n",
        "5. Saves the new video with the summary chart to the specified output path.\n",
        "\n",
        "6. Returns a structured JSON summary of the vehicle turn data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm56mopSq6ts"
      },
      "outputs": [],
      "source": [
        "def add_final_summary_to_video(video_path, vehicle_turns, output_path=\"final_output.mp4\"):\n",
        "    \"\"\"Add a final summary frame to the end of the video\"\"\"\n",
        "\n",
        "    # First analyze the turns\n",
        "    vehicle_turn_json = analyze_turns(vehicle_turns)\n",
        "\n",
        "    # Load the bar chart image\n",
        "    chart_img = cv2.imread(\"turn_analysis.png\")\n",
        "    if chart_img is None:\n",
        "        raise FileNotFoundError(\"turn_analysis.png not found.\")\n",
        "\n",
        "    # Read the original video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Resize chart to match video resolution\n",
        "    chart_img = cv2.resize(chart_img, (width, height))\n",
        "\n",
        "    # Create the output video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') #mp4v h264\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Copy all frames from the original video\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        out.write(frame)\n",
        "\n",
        "    # Append chart image as 5 seconds of frames\n",
        "    for _ in range(int(fps * 5)):\n",
        "        out.write(chart_img)\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Final video with chart saved as '{output_path}'\")\n",
        "\n",
        "    return vehicle_turn_json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdmIGrtbq5eV"
      },
      "source": [
        "# Init Video Processing (Full Video-Based Vehicle Turn Detection and Summary Pipeline)\n",
        "\n",
        "This function **run_full_vehicle_turn_pipeline** performs the complete pipeline for analyzing vehicle movements in a video. It:\n",
        "\n",
        "1. Processes the input video using a configured video processor to detect and trace vehicle movements.\n",
        "\n",
        "2. Analyzes vehicle turns (left, right, U-turn, straight) and records them.\n",
        "\n",
        "3. Generates a summary chart of the turn statistics and appends it to the output video.\n",
        "\n",
        "4. Returns a JSON summary of vehicle turn analytics for further use (e.g., visualization or question answering).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek-5O71YrFnp"
      },
      "outputs": [],
      "source": [
        "def run_full_vehicle_turn_pipeline(\n",
        "    source_video_path: str,\n",
        "    final_output_path: str = \"final_output.mp4\",\n",
        "    zones: Dict[str, list] = {},\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs the full pipeline: processes video, tracks turns, and appends summary.\n",
        "    \"\"\"\n",
        "    if not zones or not zones.get(\"entry\") or not zones.get(\"exit\"):\n",
        "        raise ValueError(\"'zones' must contain both non-empty 'entry' and 'exit' lists.\")\n",
        "\n",
        "    # Step 1: Setup and process the video\n",
        "    config = setup_video_processor(\n",
        "        source_video_path=source_video_path,\n",
        "        target_video_path=\"output_traced.mp4\",\n",
        "        zones=zones\n",
        "    )\n",
        "    vehicle_turns_state = process_video(config)\n",
        "\n",
        "    # Step 2: Append summary chart to the traced video\n",
        "    vehicle_turn_json = add_final_summary_to_video(\n",
        "        video_path=\"output_traced.mp4\",\n",
        "        vehicle_turns=vehicle_turns_state,\n",
        "        output_path=final_output_path\n",
        "    )\n",
        "    return vehicle_turn_json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewXOvNlM-_26"
      },
      "source": [
        "# Vehicle Turn Detection Summary & AI-Powered Question Answering.\n",
        "**convert_turn_stats_to_text(analysis_result)**:\n",
        "Converts the vehicle turn detection results (a JSON dictionary) into a readable text summary, including:\n",
        "\n",
        "1. Total vehicle count\n",
        "\n",
        "2. Turn type counts (right, left, U-turn, straight)\n",
        "\n",
        "3. Per-vehicle turn information.\n",
        "\n",
        "**Use:**\n",
        "This summary is later passed to a language model for answering questions.\n",
        "\n",
        "**create_pipeline(text_data)**:\n",
        "Creates a custom question-answering function qa_pipeline(question) that:\n",
        "\n",
        "1. Takes a natural language question\n",
        "\n",
        "2. Feeds it to Qwen along with the vehicle turn summary\n",
        "\n",
        "3. Returns only the assistant's reply from the model output\n",
        "\n",
        "**Purpose:**\n",
        "This abstracts the model usage so the user can ask follow-up questions based on video analytics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es4286u__GMU"
      },
      "outputs": [],
      "source": [
        "def convert_turn_stats_to_text(analysis_result):\n",
        "    turn_counts = analysis_result.get(\"turn_counts\", {})\n",
        "    turn_details = analysis_result.get(\"turn_details\", [])\n",
        "\n",
        "    total = analysis_result.get(\"total_vehicles\", 0)\n",
        "    right = turn_counts.get(\"Vehicles making right turns\", 0)\n",
        "    left = turn_counts.get(\"Vehicles making left turns\", 0)\n",
        "    u_turn = turn_counts.get(\"Vehicles making U-turns\", 0)\n",
        "    straight = turn_counts.get(\"Vehicles with no detected turns (Straight)\", 0)\n",
        "\n",
        "    summary_text = (\n",
        "    f\"A total of {total} cars were tracked during the analysis. \"\n",
        "    f\"Among them, {right} made right turns, {left} made left turns, \"\n",
        "    f\"{u_turn} performed U-turns (also referred to as 'uturns' or 'reverse turns'), and {straight} continued straight without making any turns. (also referred to as 'no turns')\"\n",
        "    )\n",
        "\n",
        "    if turn_details:\n",
        "        detail_sentences = [\n",
        "            f\"Vehicle ID {item['tracker_id']} made a {item['turn'].replace('_', ' ').lower()}.\"\n",
        "            for item in turn_details\n",
        "        ]\n",
        "        details_text = \" \".join(detail_sentences)\n",
        "        return f\"{summary_text}{details_text}\"\n",
        "    else:\n",
        "        return f\"{summary_text}. No individual vehicle turn details were recorded.\"\n",
        "\n",
        "\n",
        "\n",
        "# Load Qwen model and tokenizer (only once globally)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "generation_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "def create_pipeline(text_data):\n",
        "    \"\"\"\n",
        "    Create a simple function to handle QA using Qwen with the full text_data\n",
        "    \"\"\"\n",
        "    def qa_pipeline(question):\n",
        "        text = f\"\"\"\n",
        "        You are an expert in analyzing traffic video data, specifically vehicle turn behavior.\n",
        "\n",
        "        Answer the question as thoroughly as possible using only the provided context. If the answer is not present in the context, respond with: \"Answer is not available in the context.\" Do not provide fabricated or assumed information.\n",
        "\n",
        "        Context:\n",
        "        {text_data}\n",
        "\n",
        "        Based on the above context, answer the following question clearly and concisely:\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert in vehicle turn analysis. Respond clearly and accurately using only the provided context.\"},\n",
        "            {\"role\": \"user\", \"content\": text},\n",
        "        ]\n",
        "        response = generation_pipe(messages, max_new_tokens=1000)[0]\n",
        "        print(response)\n",
        "        assistant_response = \"\"\n",
        "        for msg in response['generated_text']:\n",
        "            if msg.get(\"role\") == \"assistant\":\n",
        "                assistant_response = msg.get(\"content\", \"\")\n",
        "                break\n",
        "        return assistant_response\n",
        "\n",
        "    return qa_pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyJSSXxLS5VZ"
      },
      "source": [
        "# Main execution and Gradio UI\n",
        "Interactive Vehicle Turn Detection with Zone-Based Video Analysis\n",
        "\n",
        "**gradio** is used to create an interactive web UI for uploading a video, processing it, and asking questions.\n",
        "\n",
        "**tempfile** is used to handle temporary storage of the uploaded video.\n",
        "\n",
        "**Description:**\n",
        "This Gradio-powered app enables users to upload traffic videos, draw entry/exit zones on the first frame, and analyze vehicle turn behavior (e.g., left turn, right turn, U-turn, straight). Users can:\n",
        "\n",
        "üñºÔ∏è Draw custom polygon zones for vehicle entry and exit\n",
        "\n",
        "üé• Run automated video analysis with a pre-defined vehicle turn detection pipeline\n",
        "\n",
        "‚ùì Ask natural language questions (e.g., \"How many U-turns?\") based on turn statistics\n",
        "\n",
        "üìä Visualize processed videos and get turn summaries\n",
        "\n",
        "global variables store:\n",
        "\n",
        "*  **global_turn_json**: the analysis results (as a dictionary).\n",
        "*  **global_pipeline:** the question-answering pipeline based on the analysis text.\n",
        "\n",
        "Analysis Video Block: (**Took 250 seconds to process**)\n",
        "\n",
        "1. Reads the uploaded video and saves it temporarily.\n",
        "\n",
        "2. Runs the vehicle turn detection pipeline (**run_full_vehicle_turn_pipeline**).\n",
        "\n",
        "3. Converts the resulting data to a readable summary using **convert_turn_stats_to_text**.\n",
        "\n",
        "4. Sets up the question-answering pipeline with **create_pipeline**.\n",
        "\n",
        "5. Returns the path to the processed video and a status message.\n",
        "\n",
        "\n",
        "(**encode_to_browser_safe_mp4**): This function converts a video file to a browser-safe MP4 format using **ffmpeg**.To transcode a video (any format) into an MP4 file that's optimized for web playback in browsers (like Chrome, Firefox, Safari, etc.).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ICAxAcnukpS"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import tempfile\n",
        "import subprocess\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Persistent state for document store and pipeline\n",
        "global_turn_json = None\n",
        "global_pipeline = None\n",
        "\n",
        "class ZoneDrawer:\n",
        "    def __init__(self):\n",
        "        self.zones = {'entry': [], 'exit': []}\n",
        "        self.current_frame = None\n",
        "        self.current_zone_type = 'entry'\n",
        "        self.video_path = None\n",
        "\n",
        "    def process_video(self, video_file):\n",
        "        \"\"\"Extract first frame from uploaded video\"\"\"\n",
        "        if video_file is None:\n",
        "            return None, \"Please upload a video file first.\"\n",
        "\n",
        "        try:\n",
        "            # Store video path for later use\n",
        "            self.video_path = video_file\n",
        "\n",
        "            # Read video and extract first frame\n",
        "            cap = cv2.VideoCapture(video_file)\n",
        "            ret, frame = cap.read()\n",
        "            cap.release()\n",
        "\n",
        "            if not ret:\n",
        "                return None, \"‚ùå Could not extract frame from video.\"\n",
        "\n",
        "            # Convert BGR to RGB for display\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            self.current_frame = frame_rgb\n",
        "\n",
        "            return frame_rgb, f\"‚úÖ Video loaded! Frame size: {frame_rgb.shape[1]}x{frame_rgb.shape[0]}. Draw zones then analyze.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"‚ùå Error processing video: {str(e)}\"\n",
        "\n",
        "    def set_zone_type(self, zone_type):\n",
        "        \"\"\"Set current zone type\"\"\"\n",
        "        self.current_zone_type = zone_type\n",
        "        color = \"üü¢ GREEN\" if zone_type == 'entry' else \"üî¥ RED\"\n",
        "        return f\"Drawing mode: {zone_type.upper()} zones ({color})\"\n",
        "\n",
        "    def process_drawing(self, image_data):\n",
        "        \"\"\"Process the drawn image and extract polygon points\"\"\"\n",
        "        if image_data is None:\n",
        "            return None, \"No drawing data received.\"\n",
        "\n",
        "        try:\n",
        "            # Convert the drawing to numpy array\n",
        "            if isinstance(image_data, dict) and 'layers' in image_data:\n",
        "                drawing = image_data['layers'][0] if image_data['layers'] else image_data['background']\n",
        "            else:\n",
        "                drawing = image_data\n",
        "\n",
        "            # Convert PIL Image to numpy array\n",
        "            if isinstance(drawing, Image.Image):\n",
        "                drawing_array = np.array(drawing)\n",
        "            else:\n",
        "                drawing_array = drawing\n",
        "\n",
        "            # Extract polygon points from the drawing\n",
        "            points = self.extract_polygon_points(drawing_array)\n",
        "\n",
        "            if len(points) < 3:\n",
        "                return self.show_current_zones(), \"‚ö†Ô∏è Please draw a polygon with at least 3 points.\"\n",
        "\n",
        "            # Add to current zone type\n",
        "            self.zones[self.current_zone_type].append(points)\n",
        "\n",
        "            # Create updated visualization\n",
        "            result_image = self.create_zone_visualization()\n",
        "\n",
        "            zone_count = len(self.zones[self.current_zone_type])\n",
        "            return result_image, f\"‚úÖ Added {self.current_zone_type} zone #{zone_count}! Total: Entry={len(self.zones['entry'])}, Exit={len(self.zones['exit'])}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return self.show_current_zones(), f\"‚ùå Error processing drawing: {str(e)}\"\n",
        "\n",
        "    def extract_polygon_points(self, drawing_array):\n",
        "        \"\"\"Extract polygon points from drawn image\"\"\"\n",
        "        # Convert to grayscale for processing\n",
        "        if len(drawing_array.shape) == 3:\n",
        "            gray = cv2.cvtColor(drawing_array, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = drawing_array\n",
        "\n",
        "        # Find contours in the drawing\n",
        "        contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        if not contours:\n",
        "            return []\n",
        "\n",
        "        # Get the largest contour\n",
        "        largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "        # Approximate the contour to reduce points\n",
        "        epsilon = 0.02 * cv2.arcLength(largest_contour, True)\n",
        "        approx = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
        "\n",
        "        # Convert to list of [x, y] points\n",
        "        points = [[int(point[0][0]), int(point[0][1])] for point in approx]\n",
        "\n",
        "        return points\n",
        "\n",
        "    def create_zone_visualization(self):\n",
        "        \"\"\"Create visualization with all zones\"\"\"\n",
        "        if self.current_frame is None:\n",
        "            return None\n",
        "\n",
        "        # Create a copy of the frame\n",
        "        vis_frame = self.current_frame.copy()\n",
        "\n",
        "        # Draw all zones\n",
        "        for zone_type, zone_list in self.zones.items():\n",
        "            color = (0, 255, 0) if zone_type == 'entry' else (255, 0, 0)  # Green for entry, Red for exit\n",
        "\n",
        "            for zone_points in zone_list:\n",
        "                if len(zone_points) >= 3:\n",
        "                    # Convert points to numpy array\n",
        "                    pts = np.array(zone_points, np.int32)\n",
        "\n",
        "                    # Draw filled polygon with transparency\n",
        "                    overlay = vis_frame.copy()\n",
        "                    cv2.fillPoly(overlay, [pts], color)\n",
        "                    cv2.addWeighted(vis_frame, 0.7, overlay, 0.3, 0, vis_frame)\n",
        "\n",
        "                    # Draw border\n",
        "                    cv2.polylines(vis_frame, [pts], isClosed=True, color=color, thickness=3)\n",
        "\n",
        "        return vis_frame\n",
        "\n",
        "    def show_current_zones(self):\n",
        "        \"\"\"Show current zones without adding new ones\"\"\"\n",
        "        return self.create_zone_visualization()\n",
        "\n",
        "    def clear_current_zones(self):\n",
        "        \"\"\"Clear all zones of current type\"\"\"\n",
        "        count = len(self.zones[self.current_zone_type])\n",
        "        self.zones[self.current_zone_type] = []\n",
        "        result_image = self.create_zone_visualization()\n",
        "        return result_image, f\"üóëÔ∏è Cleared {count} {self.current_zone_type} zones.\"\n",
        "\n",
        "    def clear_all_zones(self):\n",
        "        \"\"\"Clear all zones\"\"\"\n",
        "        total = len(self.zones['entry']) + len(self.zones['exit'])\n",
        "        self.zones = {'entry': [], 'exit': []}\n",
        "        result_image = self.show_current_zones()\n",
        "        return result_image, f\"üîÑ Cleared all {total} zones.\"\n",
        "\n",
        "    def get_zone_info(self):\n",
        "        \"\"\"Get current zone information\"\"\"\n",
        "        entry_count = len(self.zones['entry'])\n",
        "        exit_count = len(self.zones['exit'])\n",
        "\n",
        "        info = (\n",
        "        f\"üìä **Zone Summary:**\\n\\n\"\n",
        "        f\"üü¢ **Entry Zones:** {entry_count}\\n\"\n",
        "        f\"üî¥ **Exit Zones:** {exit_count}\\n\"\n",
        "        f\"üìç **Total Zones:** {entry_count + exit_count}\\n\\n\"\n",
        "        f\"üìã **Zone Coordinates:**\\n\"\n",
        "        f\"```json\\n\"\n",
        "        f\"{json.dumps(self.zones, indent=2)}\\n\"\n",
        "        f\"```\\n\\n\"\n",
        "        f\"üéØ **Ready for Analysis:** {'‚úÖ Yes' if (entry_count + exit_count) > 0 else '‚ùå Draw zones first'}\"\n",
        "        )\n",
        "        return info\n",
        "\n",
        "    def get_zones_for_pipeline(self):\n",
        "        \"\"\"Get zones in format expected by pipeline\"\"\"\n",
        "        return self.zones\n",
        "\n",
        "# Initialize the zone drawer\n",
        "zone_drawer = ZoneDrawer()\n",
        "\n",
        "def analyze_video_with_zones(video_file_path):\n",
        "    \"\"\"Analyze video using drawn zones\"\"\"\n",
        "    global global_pipeline, global_turn_json\n",
        "\n",
        "    if not video_file_path:\n",
        "        return None, \"Please upload a video file.\"\n",
        "\n",
        "    # Check if zones are drawn\n",
        "    zones = zone_drawer.get_zones_for_pipeline()\n",
        "    if not zones['entry'] and not zones['exit']:\n",
        "        return None, \"‚ùå Please draw entry and/or exit zones before analyzing.\"\n",
        "\n",
        "    try:\n",
        "        # Create temporary file\n",
        "        with open(video_file_path, \"rb\") as source_file:\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as tmp_file:\n",
        "                tmp_file.write(source_file.read())\n",
        "                tmp_video_path = tmp_file.name\n",
        "\n",
        "        raw_output_path = \"raw_output.mp4\"\n",
        "        browser_safe_path = \"final_output_with_summary.mp4\"\n",
        "\n",
        "        # Save zones to temporary file for pipeline\n",
        "        zones_file = \"temp_zones.json\"\n",
        "        with open(zones_file, 'w') as f:\n",
        "            json.dump(zones, f, indent=2)\n",
        "\n",
        "        # Run the full vehicle turn detection pipeline with zones\n",
        "        global_turn_json = run_full_vehicle_turn_pipeline(\n",
        "            source_video_path=tmp_video_path,\n",
        "            final_output_path=raw_output_path,\n",
        "            zones_config=zones  # Pass zones to pipeline\n",
        "        )\n",
        "\n",
        "        # Re-encode video\n",
        "        encode_to_browser_safe_mp4(raw_output_path, browser_safe_path)\n",
        "\n",
        "        # Create document store and pipeline for QA\n",
        "        text_data = convert_turn_stats_to_text(global_turn_json)\n",
        "        global_pipeline = create_pipeline(text_data)\n",
        "\n",
        "        # Clean up temporary files\n",
        "        if os.path.exists(zones_file):\n",
        "            os.remove(zones_file)\n",
        "        if os.path.exists(tmp_video_path):\n",
        "            os.remove(tmp_video_path)\n",
        "\n",
        "        zone_summary = f\"Entry zones: {len(zones['entry'])}, Exit zones: {len(zones['exit'])}\"\n",
        "        return browser_safe_path, f\"‚úÖ Video analyzed successfully with {zone_summary}! You can now ask questions.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"‚ùå Error during analysis: {str(e)}\"\n",
        "\n",
        "def answer_question(user_question):\n",
        "    \"\"\"Answer questions about the analyzed video\"\"\"\n",
        "    if not global_pipeline or not global_turn_json:\n",
        "        return \"Please analyze a video first.\"\n",
        "    return global_pipeline(user_question)\n",
        "\n",
        "def encode_to_browser_safe_mp4(input_path: str, output_path: str):\n",
        "    \"\"\"Convert video to browser-safe format\"\"\"\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-y\", \"-i\", input_path,\n",
        "        \"-vcodec\", \"libx264\", \"-preset\", \"ultrafast\",\n",
        "        \"-acodec\", \"aac\", \"-movflags\", \"+faststart\",\n",
        "        output_path\n",
        "    ]\n",
        "    try:\n",
        "        subprocess.run(cmd, check=True)\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"Error: ffmpeg failed to convert video to browser-safe format.\")\n",
        "\n",
        "# Create Gradio interface\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"üöó Vehicle Turn Detection with Zone Drawing\", theme=gr.themes.Soft()) as interface:\n",
        "        gr.HTML(\"\"\"\n",
        "        <h1 style=\"text-align: center;\">üöó Vehicle Turn Detection with Zone Drawing</h1>\n",
        "        <p style=\"text-align: center;\">Upload video ‚Üí Draw zones ‚Üí Analyze turns ‚Üí Ask questions</p>\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tab(\"üìπ Step 1: Upload & Draw Zones\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=2):\n",
        "                    # Video upload\n",
        "                    video_input = gr.File(\n",
        "                        label=\"üìπ Upload Video\",\n",
        "                        file_types=[\".mp4\", \".avi\", \".mov\"],\n",
        "                        type=\"filepath\"\n",
        "                    )\n",
        "\n",
        "                    # Frame display and drawing area\n",
        "                    frame_display = gr.ImageEditor(\n",
        "                        label=\"üé® Draw Zones (Click and drag to draw polygons)\",\n",
        "                        type=\"pil\",\n",
        "                        # tool=\"sketch\",\n",
        "                        height=400\n",
        "                    )\n",
        "\n",
        "                    # Zone type selector\n",
        "                    zone_type = gr.Radio(\n",
        "                        choices=[\"entry\", \"exit\"],\n",
        "                        value=\"entry\",\n",
        "                        label=\"üéØ Zone Type\",\n",
        "                        info=\"Select whether to draw entry (green) or exit (red) zones\"\n",
        "                    )\n",
        "\n",
        "                    # Control buttons\n",
        "                    with gr.Row():\n",
        "                        save_zone_btn = gr.Button(\"‚úÖ Save Zone\", variant=\"primary\")\n",
        "                        clear_current_btn = gr.Button(\"üóëÔ∏è Clear Current Type\")\n",
        "                        clear_all_btn = gr.Button(\"üîÑ Reset All\")\n",
        "\n",
        "                    # Status message\n",
        "                    zone_status = gr.Textbox(\n",
        "                        label=\"üì¢ Zone Status\",\n",
        "                        interactive=False,\n",
        "                        max_lines=2\n",
        "                    )\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    # Zone information\n",
        "                    zone_info = gr.Markdown(\n",
        "                        value=\"Upload a video to start drawing zones.\",\n",
        "                        label=\"üìä Zone Information\"\n",
        "                    )\n",
        "\n",
        "                    # Instructions\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ### üìñ Instructions:\n",
        "\n",
        "                    1. **Upload** a video file\n",
        "                    2. **Select** zone type (Entry/Exit)\n",
        "                    3. **Draw** polygons on the frame\n",
        "                    4. **Save** each zone after drawing\n",
        "                    5. Go to **Step 2** to analyze\n",
        "\n",
        "                    ### üé® Drawing Tips:\n",
        "                    - Use sketch tool to draw polygons\n",
        "                    - Entry zones ‚Üí Green overlay\n",
        "                    - Exit zones ‚Üí Red overlay\n",
        "                    - Draw closed shapes for best results\n",
        "                    \"\"\")\n",
        "\n",
        "        with gr.Tab(\"üîç Step 2: Analyze Video\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    analyze_btn = gr.Button(\"üöÄ Analyze Video with Zones\", variant=\"primary\", size=\"lg\")\n",
        "                    analysis_status = gr.Textbox(label=\"üìä Analysis Status\", interactive=False)\n",
        "\n",
        "                with gr.Column():\n",
        "                    video_output = gr.Video(label=\"üìπ Processed Video\")\n",
        "\n",
        "        with gr.Tab(\"‚ùì Step 3: Ask Questions\"):\n",
        "            gr.Markdown(\"### Ask questions about the analyzed video:\")\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    question_input = gr.Textbox(\n",
        "                        label=\"üí¨ Your Question\",\n",
        "                        placeholder=\"e.g., How many U-turns were made? What was the most common turn type?\",\n",
        "                        lines=2\n",
        "                    )\n",
        "                with gr.Column():\n",
        "                    answer_output = gr.Textbox(\n",
        "                        label=\"ü§ñ Answer\",\n",
        "                        lines=4,\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "        # Event handlers for zone drawing\n",
        "        video_input.change(\n",
        "            fn=zone_drawer.process_video,\n",
        "            inputs=[video_input],\n",
        "            outputs=[frame_display, zone_status]\n",
        "        )\n",
        "\n",
        "        zone_type.change(\n",
        "            fn=zone_drawer.set_zone_type,\n",
        "            inputs=[zone_type],\n",
        "            outputs=[zone_status]\n",
        "        )\n",
        "\n",
        "        save_zone_btn.click(\n",
        "            fn=zone_drawer.process_drawing,\n",
        "            inputs=[frame_display],\n",
        "            outputs=[frame_display, zone_status]\n",
        "        ).then(\n",
        "            fn=zone_drawer.get_zone_info,\n",
        "            outputs=[zone_info]\n",
        "        )\n",
        "\n",
        "        clear_current_btn.click(\n",
        "            fn=zone_drawer.clear_current_zones,\n",
        "            outputs=[frame_display, zone_status]\n",
        "        ).then(\n",
        "            fn=zone_drawer.get_zone_info,\n",
        "            outputs=[zone_info]\n",
        "        )\n",
        "\n",
        "        clear_all_btn.click(\n",
        "            fn=zone_drawer.clear_all_zones,\n",
        "            outputs=[frame_display, zone_status]\n",
        "        ).then(\n",
        "            fn=zone_drawer.get_zone_info,\n",
        "            outputs=[zone_info]\n",
        "        )\n",
        "\n",
        "        # Event handler for video analysis\n",
        "        analyze_btn.click(\n",
        "            fn=lambda: analyze_video_with_zones(zone_drawer.video_path),\n",
        "            outputs=[video_output, analysis_status]\n",
        "        )\n",
        "\n",
        "        # Event handler for Q&A\n",
        "        question_input.submit(\n",
        "            fn=answer_question,\n",
        "            inputs=[question_input],\n",
        "            outputs=[answer_output]\n",
        "        )\n",
        "\n",
        "        question_input.change(\n",
        "            fn=answer_question,\n",
        "            inputs=[question_input],\n",
        "            outputs=[answer_output]\n",
        "        )\n",
        "\n",
        "    return interface\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface = create_interface()\n",
        "    interface.launch(\n",
        "        share=True,\n",
        "        show_error=True\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}