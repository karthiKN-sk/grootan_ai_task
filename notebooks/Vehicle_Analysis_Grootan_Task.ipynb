{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTmbZZnKxhjQ"
      },
      "source": [
        "# Cloning the Fine Tuned Model From my GitHub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0--KYU2xxdsA"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/karthiKN-sk/grootan_ai_task.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdujpcM3xtm9"
      },
      "source": [
        "# Download the Library/Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhF1tps9wXBU"
      },
      "outputs": [],
      "source": [
        "!pip3 install opencv-python ultralytics supervision numpy matplotlib rich.progress transformers gradio pillow huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwBPpqTxQ9HG"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nfkW4B2RBa6"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import supervision as sv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from rich.progress import Progress\n",
        "from typing import Dict, Iterable, List, Optional, Set, Any\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import os\n",
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdipK6N5IKIA"
      },
      "source": [
        "# Setting HF Token in Environment variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH4YRyCdIGvS"
      },
      "outputs": [],
      "source": [
        "token = None\n",
        "token_file = \"/content/grootan_ai_task/variables.py\"\n",
        "\n",
        "with open(token_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        if line.startswith(\"HF_TOKEN=\"):\n",
        "            token = line.strip().split(\"=\", 1)[1].strip('\"').strip(\"'\")\n",
        "            break\n",
        "\n",
        "if token:\n",
        "    os.environ[\"HF_TOKEN\"] = token\n",
        "    login(token=token)\n",
        "else:\n",
        "    raise ValueError(\"HF_TOKEN not found in variables.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95wvjpMzQqAk"
      },
      "source": [
        "# Configuration for Zone Definitions, Color Palette, and Vehicle Turn Classification.\n",
        "\n",
        "This code snippet sets up essential constants and configurations for a vehicle tracking and turn analysis system, including color settings, polygonal zone definitions, naming schemes, turn mapping logic, and a utility function for turn-based color annotation.\n",
        "\n",
        "1. Color Palette Definition\n",
        "    * Defines a reusable color palette used for drawing zones, bounding boxes, and labels.\n",
        "\n",
        "2. Turn Mapping Logic\n",
        "\n",
        "      * Maps combinations of entry (In1,2,3,4) and exit (Out1,2,3,4) zones to specific turn types (right_turn, left_turn, u_turn, straight).\n",
        "\n",
        "      * Used to classify vehicle movement patterns across zones.\n",
        "\n",
        "      * **generate_turn_mapping**: Generates a flexible turn mapping for 1–3 way intersections.\n",
        "        It assigns right_turn, left_turn, straight, or u_turn based on available exits for each entry zone.\n",
        "        The logic is order-based and adapts automatically to the number of zones.\n",
        "    \n",
        "This setup provides a foundational configuration layer for visual annotation, spatial zone management, and logical turn analysis in a computer vision-based traffic monitoring system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkVRIaZiQmA-"
      },
      "outputs": [],
      "source": [
        "COLORS = sv.ColorPalette.from_hex([\"#E6194B\", \"#3CB44B\", \"#FFE119\", \"#3C76D1\"])\n",
        "\n",
        "# Four way turn mapping\n",
        "FOUR_WAY_TURN_MAPPING =  {\n",
        "    # Entry from Z1\n",
        "    (\"Z1\", \"Z1\"): \"u_turn\",\n",
        "    (\"Z1\", \"Z2\"): \"right_turn\",\n",
        "    (\"Z1\", \"Z3\"): \"straight\",\n",
        "    (\"Z1\", \"Z4\"): \"left_turn\",\n",
        "\n",
        "    # Entry from Z2\n",
        "    (\"Z2\", \"Z2\"): \"u_turn\",\n",
        "    (\"Z2\", \"Z3\"): \"right_turn\",\n",
        "    (\"Z2\", \"Z4\"): \"straight\",\n",
        "    (\"Z2\", \"Z1\"): \"left_turn\",\n",
        "\n",
        "    # Entry from Z3\n",
        "    (\"Z3\", \"Z3\"): \"u_turn\",\n",
        "    (\"Z3\", \"Z4\"): \"right_turn\",\n",
        "    (\"Z3\", \"Z1\"): \"straight\",\n",
        "    (\"Z3\", \"Z2\"): \"left_turn\",\n",
        "\n",
        "    # Entry from Z4\n",
        "    (\"Z4\", \"Z4\"): \"u_turn\",\n",
        "    (\"Z4\", \"Z1\"): \"right_turn\",\n",
        "    (\"Z4\", \"Z2\"): \"straight\",\n",
        "    (\"Z4\", \"Z3\"): \"left_turn\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkO7vlUM-63E"
      },
      "source": [
        "# Global Detection State Initialization for Vehicle Turn Tracking.\n",
        "\n",
        "This snippet initializes a global dictionary named detections_state that is used to persist and manage tracking data across video frames in a vehicle monitoring pipeline.\n",
        "\n",
        "Dictionary Keys:\n",
        "\n",
        "* \"**tracker_id_to_zone_id**\":\n",
        "   Maps each unique vehicle (tracker_id) to the zone ID it first appeared in.\n",
        "   Used to group and identify vehicles during processing.\n",
        "\n",
        "* \"**vehicle_paths**\":\n",
        "Tracks both the entry (in) and exit (out) zones for each vehicle.\n",
        "Format: **{tracker_id: {\"in\": zone_name, \"out\": zone_name}}**.\n",
        "\n",
        "* \"**vehicle_turns**\":\n",
        "Stores the classified type of turn for each vehicle, such as \"left_turn\", \"right_turn\", \"u_turn\", or \"straight\".\n",
        "\n",
        "This stateful object is referenced and updated throughout the processing pipeline to enable accurate vehicle path tracking and turn classification across video frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tWD_02a-7cS"
      },
      "outputs": [],
      "source": [
        "detections_state = {\n",
        "    \"tracker_id_to_zone_id\": {},\n",
        "    \"vehicle_paths\": {},\n",
        "    \"vehicle_turns\": {},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7if-apVfOx0H"
      },
      "source": [
        "# Vehicle Entry-Exit Tracking and Turn Classification Logic\n",
        "\n",
        "This function, update_detections_state, manages and updates the internal state used to track vehicle movement through predefined zones, and classifies the type of turn each vehicle makes (left, right, U-turn, or straight).\n",
        "\n",
        "Key Responsibilities:\n",
        "\n",
        "1. Track Entry Zones:\n",
        "\n",
        "    * Maps each vehicle (via its tracker ID) to the in zone where it first appeared.\n",
        "\n",
        "    * Ensures the entry point is recorded only once per vehicle.\n",
        "\n",
        "2. Track Exit Zones:\n",
        "\n",
        "    * Detects and records the out zone where the vehicle exits.\n",
        "\n",
        "3. Turn Detection:\n",
        "\n",
        "    * Uses predefined TURN_MAPPING to determine the turn type based on zone pairs (in → out).\n",
        "\n",
        "    * Updates the vehicle_turns dictionary only when both zones are known.\n",
        "\n",
        "4. Class ID Assignment:\n",
        "\n",
        "    * Associates each detection with a zone ID for visual annotation by mapping tracker_id to its zone_id.\n",
        "\n",
        "5. Filtering Valid Detections:\n",
        "\n",
        "    * Returns only detections that have been successfully associated with zones (i.e., not class ID -1).\n",
        "\n",
        "This function plays a central role in transforming low-level detection data into high-level vehicle movement understanding, essential for turn analysis in traffic videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IULBAHSO5Cc"
      },
      "outputs": [],
      "source": [
        "def update_detections_state(\n",
        "    detections_all: sv.Detections,\n",
        "    detections_in_zones: List[sv.Detections],\n",
        "    config: Dict[str, Any],\n",
        "    state: Dict[str, Any] = detections_state\n",
        ") -> sv.Detections:\n",
        "    tracker_id_to_zone_id = state[\"tracker_id_to_zone_id\"]\n",
        "    vehicle_paths = state[\"vehicle_paths\"]\n",
        "    vehicle_turns = state.setdefault(\"vehicle_turns\", {})\n",
        "\n",
        "\n",
        "    # --- Assign entry zones and track vehicle IN zone names ---\n",
        "    zone_in_names = list(config[\"zones\"].keys())  \n",
        "    for zone_in_id, detections_in_zone in enumerate(detections_in_zones):\n",
        "        zone_name = zone_in_names[zone_in_id]\n",
        "        for tracker_id in detections_in_zone.tracker_id:\n",
        "            tracker_id_to_zone_id.setdefault(tracker_id, zone_in_id)\n",
        "\n",
        "            # Initialize vehicle path if not already present\n",
        "            vehicle_paths.setdefault(tracker_id, {\"in\": None, \"out\": None})\n",
        "            if vehicle_paths[tracker_id][\"in\"] is None:\n",
        "                vehicle_paths[tracker_id][\"in\"] = zone_name\n",
        "                \n",
        "            if tracker_id in tracker_id_to_zone_id:\n",
        "                vehicle_paths.setdefault(tracker_id, {\"in\": None, \"out\": None})\n",
        "                if vehicle_paths[tracker_id][\"out\"] is None and vehicle_paths[tracker_id][\"in\"] != zone_name:\n",
        "                    vehicle_paths[tracker_id][\"out\"] = zone_name\n",
        "\n",
        "    zone_centers = config[\"zone_centers\"]\n",
        "    # --- Detect turns ---\n",
        "    for tracker_id, path in vehicle_paths.items():\n",
        "        in_zone = path[\"in\"]\n",
        "        out_zone = path[\"out\"]\n",
        "        if in_zone and out_zone and tracker_id not in vehicle_turns:\n",
        "            turn_type = config[\"turn_mapping\"].get(in_zone, {}).get(out_zone)\n",
        "            if turn_type:\n",
        "                vehicle_turns[tracker_id] = turn_type\n",
        "\n",
        "    # Assign class_id for drawing/annotation\n",
        "    if len(detections_all) > 0:\n",
        "        detections_all.class_id = np.vectorize(\n",
        "            lambda x: tracker_id_to_zone_id.get(x, -1)\n",
        "        )(detections_all.tracker_id)\n",
        "    else:\n",
        "        detections_all.class_id = np.array([], dtype=int)\n",
        "\n",
        "    return detections_all[detections_all.class_id != -1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEUvpM3lQydq"
      },
      "source": [
        "# Zone Initialization and PolygonZone dictionary Utilities.\n",
        "\n",
        "This code provides two utility functions used in the vehicle turn detection system:\n",
        "1. **initiate_polygon_zones**:\n",
        "\n",
        "    * Takes a list of zone names and corresponding polygon coordinates.\n",
        "    * Initializes and returns a dictionary mapping each name to a PolygonZone object.\n",
        "\n",
        "\n",
        "These functions are key to defining spatial zones for detecting vehicle movement and labeling them appropriately in the video annotation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6ViGjiZQy6w"
      },
      "outputs": [],
      "source": [
        "def initiate_polygon_zones(\n",
        "    names: List[str],\n",
        "    polygons: List[np.ndarray],\n",
        "    triggering_anchors: Iterable[sv.Position] = [sv.Position.CENTER],\n",
        ") -> Dict[str, sv.PolygonZone]:\n",
        "    return {\n",
        "        name: sv.PolygonZone(polygon=polygon, triggering_anchors=triggering_anchors)\n",
        "        for name, polygon in zip(names, polygons)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS4KoQo1PlXc"
      },
      "source": [
        "# Setup Configuration for Vehicle Turn Detection Pipeline.\n",
        "\n",
        "The **setup_video_processor** function initializes and returns a configuration dictionary containing all essential components and parameters needed to process a video for vehicle turn detection. Here's what it sets up:\n",
        "\n",
        "1. Video Input/Output Paths:\n",
        "\n",
        "      * **source_video_path**: Path to the input video.\n",
        "\n",
        "      * **target_video_path**: Optional path to save the processed output.\n",
        "\n",
        "2. Detection Parameters:\n",
        "\n",
        "      * **confidence_threshold**: Minimum confidence level for YOLO model detections.\n",
        "\n",
        "      * **iou_threshold**: IOU threshold used during object tracking.\n",
        "\n",
        "3. Detection and Tracking Tools:\n",
        "\n",
        "      * **model**: A fine-tuned YOLO model for vehicle detection.\n",
        "\n",
        "      * **tracker**: ByteTrack tracker for maintaining vehicle identities.\n",
        "\n",
        "4. Video Metadata:\n",
        "\n",
        "      * **video_info**: Extracts frame rate, resolution, and frame count from the input video.\n",
        "\n",
        "5. Zone Definitions:\n",
        "\n",
        "      * **zones**: Get polygon areas from User to detect entry and exit for turn classification.\n",
        "\n",
        "6. Annotation Tools:\n",
        "\n",
        "      * **box_annotator**: Draws bounding boxes on detected vehicles.\n",
        "\n",
        "      * **label_annotator**: Displays vehicle IDs.\n",
        "\n",
        "      * **trace_annotator**: Adds trajectory traces to show vehicle movement paths.\n",
        "\n",
        "7. Detection State Handler:\n",
        "\n",
        "      * **detections_manager**: A function to manage turn state updates.\n",
        "\n",
        "8. Zone Definitions (Get From User Polygons )\n",
        "\n",
        "    * ZONE_IN_POLYGONS: List of polygonal coordinates marking entry zones for vehicles.\n",
        "\n",
        "    * ZONE_OUT_POLYGONS: List of polygonal coordinates marking exit zones for vehicles.\n",
        "\n",
        "    * Each polygon is an array of 2D points (x, y).\n",
        "    * Generate Zone Names Using Polygon\n",
        "\n",
        "This setup function centralizes the configuration, making it easy to pass all necessary components to the video processing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n6qfJvMPqPB"
      },
      "outputs": [],
      "source": [
        "def setup_video_processor(\n",
        "    source_video_path: str,\n",
        "    target_video_path: Optional[str] = None,\n",
        "    zones: List[list] = [],\n",
        "    confidence_threshold: float = 0.4,\n",
        "    iou_threshold: float = 0.7,\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    if not zones:\n",
        "        raise ValueError(\"'zones' must contain at least one polygon.\")\n",
        "\n",
        "    # Convert polygon lists to NumPy arrays\n",
        "    ZONE_POLYGONS = [np.array(polygon, dtype=np.int32) for polygon in zones]\n",
        "\n",
        "    ZONE_NAMES = [f\"Z{i+1}\" for i in range(len(ZONE_POLYGONS))]\n",
        "\n",
        "    TURN_MAPPING = FOUR_WAY_TURN_MAPPING\n",
        "\n",
        "    return {\n",
        "        \"conf_threshold\": confidence_threshold,\n",
        "        \"iou_threshold\": iou_threshold,\n",
        "        \"source_video_path\": source_video_path,\n",
        "        \"target_video_path\": target_video_path,\n",
        "        \"model\": YOLO(\"/content/grootan_ai_task/models/YoloFineTunedV1.pt\"),\n",
        "        \"tracker\": sv.ByteTrack(),\n",
        "        \"video_info\": sv.VideoInfo.from_video_path(source_video_path),\n",
        "        \"zones\": initiate_polygon_zones(ZONE_NAMES,ZONE_POLYGONS),\n",
        "        \"box_annotator\": sv.BoxAnnotator(color=COLORS),\n",
        "        \"label_annotator\": sv.LabelAnnotator(color=COLORS, text_color=sv.Color.BLACK),\n",
        "        \"trace_annotator\": sv.TraceAnnotator(\n",
        "            color=COLORS, position=sv.Position.CENTER, trace_length=100, thickness=2\n",
        "        ),\n",
        "        \"detections_manager\": update_detections_state,\n",
        "        \"turn_mapping\": TURN_MAPPING\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HffnMCTqKCu"
      },
      "source": [
        "# Analyze and Visualize Vehicle Turn Statistics\n",
        "\n",
        "The **analyze_turns** function evaluates vehicle turn data to generate a summary of turn behavior and visual insights. Here's what it does:\n",
        "\n",
        "1. Summary Computation:\n",
        "    *  Counts total tracked vehicles.\n",
        "    *  Computes how many made right turns, left turns, U-turns, or went straight.\n",
        "\n",
        "2. Console Output:\n",
        "    *   Prints a summary report of the turn counts to the terminal.\n",
        "\n",
        "3. Data Packaging:\n",
        "    *   Creates a JSON-style dictionary (**turn_message**) containing:\n",
        "        *   A message,\n",
        "        *   Overall turn statistics (**turn_counts**),\n",
        "        *   Individual vehicle turn details by tracker ID (**turn_details**).\n",
        "\n",
        "\n",
        "4. Visualization:\n",
        "\n",
        "      *   Plots a bar chart using **matplotlib** to visually represent the count of each turn type.\n",
        "      *   Saves the chart as **turn_analysis.png** for later use (e.g., appending to video).\n",
        "\n",
        "5. Return:\n",
        "      *   Outputs the structured **turn_message**, suitable for downstream use in reports or QA systems.\n",
        "\n",
        "This function bridges raw detection data with user-friendly output, supporting both analysis and visualization of vehicle behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x9VkqF5qKm2"
      },
      "outputs": [],
      "source": [
        "def analyze_turns(vehicle_turns):\n",
        "    \"\"\"Analyze the turns and create summary statistics\"\"\"\n",
        "    total_vehicles = len(vehicle_turns)\n",
        "    if total_vehicles == 0:\n",
        "        print(\"No vehicles were detected or tracked.\")\n",
        "        return\n",
        "    # Count the different types of turns\n",
        "    right_turns = sum(1 for turns in vehicle_turns.values() if turns == \"right_turn\" )\n",
        "    left_turns = sum(1 for turns in vehicle_turns.values() if turns == \"left_turn\" )\n",
        "    u_turns = sum(1 for turns in vehicle_turns.values() if turns == \"u_turn\" )\n",
        "    no_turns = sum(1 for turns in vehicle_turns.values() if turns == \"straight\")\n",
        "\n",
        "    print(\"\\n--- Turn Analysis Results ---\")\n",
        "    print(f\"Total unique vehicles tracked: {total_vehicles}\")\n",
        "    print(f\"Vehicles making right turns: {right_turns} \")\n",
        "    print(f\"Vehicles making left turns: {left_turns}\")\n",
        "    print(f\"Vehicles making U-turns: {u_turns}\")\n",
        "    print(f\"Vehicles with no detected turns: {no_turns}\")\n",
        "\n",
        "\n",
        "    # Create a visualization\n",
        "    turn_counts = {\n",
        "        'Right Turn': right_turns,\n",
        "        'Left Turn': left_turns,\n",
        "        'U-Turn': u_turns,\n",
        "        'No Turn': no_turns\n",
        "    }\n",
        "\n",
        "    turn_message = {\n",
        "    \"message\": \"Turn Analysis Results completed.\",\n",
        "    \"total_vehicles\": total_vehicles,\n",
        "    \"turn_counts\": {\n",
        "        \"Vehicles making right turns\" : right_turns,\n",
        "        \"Vehicles making left turns\": left_turns,\n",
        "        \"Vehicles making U-turns\": u_turns,\n",
        "        \"Vehicles with no detected turns (Straight)\": no_turns\n",
        "    },\n",
        "     \"turn_details\": [\n",
        "        {\"tracker_id\": tracker_id, \"turn\": turn}\n",
        "        for tracker_id, turn in vehicle_turns.items()\n",
        "     ]\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    colors = ['red', 'green', 'black', 'blue']\n",
        "    plt.bar(turn_counts.keys(), turn_counts.values(), color=colors)\n",
        "    plt.title('Vehicle Turn Analysis')\n",
        "    plt.ylabel('Number of Vehicles')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Add count labels on top of each bar\n",
        "    for i, (key, value) in enumerate(turn_counts.items()):\n",
        "        plt.text(i, value + 0.3, str(value), ha='center')\n",
        "\n",
        "    plt.savefig('turn_analysis.png')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    return turn_message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNh4F1DGRnav"
      },
      "source": [
        "# Annotate Video Frames with Zone Information and Vehicle Turn Statistics\n",
        "\n",
        "\n",
        "The **annotate_frame** function overlays comprehensive visual annotations on each video frame to aid in understanding vehicle movement and behavior. Here's what it does:\n",
        "\n",
        "1. Draws zones on the frame using polygons and labels with distinct colors.\n",
        "\n",
        "2. Generates labels for each detected vehicle using their tracker IDs (e.g., **\"Car #12\"**).\n",
        "\n",
        "3. Applies multiple annotation layers:\n",
        "    *   Trajectory traces via **trace_annotator**\n",
        "    *   Bounding boxes via **box_annotator**\n",
        "    *   Vehicle ID labels via **label_annotator**\n",
        "\n",
        "4. Computes turn statistics from the global detections_state:\n",
        "    *   Total vehicles tracked\n",
        "    *   Counts of right turns, left turns, U-turns, and straight movements\n",
        "\n",
        "5. Displays summary metrics visually on the frame, including:\n",
        "    *   A detection count badge\n",
        "    *   Fixed-position statistics on turn types, color-coded for clarity (e.g., red for right turns, green for left turns, etc.)\n",
        "\n",
        "This function is central to making the video output interpretable by overlaying both spatial (zones) and behavioral (turn types) information for each detected vehicle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Puz5Hu3mRlus"
      },
      "outputs": [],
      "source": [
        "def annotate_frame(frame: np.ndarray, detections: sv.Detections, config: Dict[str, Any]) -> np.ndarray:\n",
        "    frame_ = frame.copy()\n",
        "\n",
        "    # Zones Colors\n",
        "    color_palette = COLORS.colors\n",
        "    num_colors = len(color_palette)\n",
        "\n",
        "    # Process entry zones\n",
        "    for i, (zin_name, zin) in enumerate(config[\"zones\"].items()):\n",
        "        color = color_palette[i % num_colors]\n",
        "        zin_anchor = sv.get_polygon_center(zin.polygon)\n",
        "        frame_ = sv.draw_polygon(frame_, zin.polygon, color)\n",
        "        frame_ = sv.draw_text(frame_, text=zin_name, text_anchor=zin_anchor, text_color=color)\n",
        "\n",
        "    labels = [f\"Car #{id_}\" for id_ in detections.tracker_id]\n",
        "\n",
        "    frame_ = config[\"trace_annotator\"].annotate(frame_, detections)\n",
        "    frame_ = config[\"box_annotator\"].annotate(frame_, detections)\n",
        "    frame_ = config[\"label_annotator\"].annotate(frame_, detections, labels)\n",
        "\n",
        "    # Count the different types of turns\n",
        "    vehicle_turns= detections_state[\"vehicle_turns\"]\n",
        "    total_vehicles = len(vehicle_turns)\n",
        "    right_turns = sum(1 for turns in vehicle_turns.values() if turns == \"right_turn\" )\n",
        "    left_turns = sum(1 for turns in vehicle_turns.values() if turns == \"left_turn\" )\n",
        "    u_turns = sum(1 for turns in vehicle_turns.values() if turns == \"u_turn\" )\n",
        "    no_turns = sum(1 for turns in vehicle_turns.values() if turns == \"straight\")\n",
        "\n",
        "    # Add detection count info\n",
        "    total_count = len(detections)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        f\"Detected: {total_count}\",\n",
        "        sv.Point(50, 50),\n",
        "        background_color=sv.Color.from_hex(\"#FF7F50\")\n",
        "    )\n",
        "    # Draw fixed turn statistics on the center-left of the frame\n",
        "    start_x = 80\n",
        "    start_y = 350\n",
        "    line_spacing = 40\n",
        "    text_color = sv.Color(r=255, g=255, b=255)\n",
        "\n",
        "    # Line 1: Total vehicles tracked\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"Total vehicles tracked: {total_vehicles}\",\n",
        "        text_anchor=sv.Point(start_x + 30, start_y),\n",
        "        background_color=sv.Color.from_hex(\"#DDDDDD\"),\n",
        "    )\n",
        "\n",
        "    # Line 2: Right turns (Red)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"Right turns: {right_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + line_spacing),\n",
        "        background_color=sv.Color(r=255, g=0, b=0),\n",
        "        text_color=text_color\n",
        "    )\n",
        "\n",
        "    # Line 3: Left turns (Green)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"Left turns: {left_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + 2 * line_spacing),\n",
        "        background_color=sv.Color(r=0, g=255, b=0),\n",
        "\n",
        "    )\n",
        "\n",
        "    # Line 4: U-turns (Black)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"U-turns: {u_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + 3 * line_spacing),\n",
        "        background_color=sv.Color(r=0, g=0, b=0),\n",
        "        text_color=text_color\n",
        "    )\n",
        "\n",
        "    # Line 5: No turns (Blue)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"No turns: {no_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + 4 * line_spacing),\n",
        "        background_color=sv.Color(r=0, g=0, b=255),\n",
        "        text_color=text_color\n",
        "    )\n",
        "\n",
        "\n",
        "    return frame_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFjmcj3g-wSx"
      },
      "source": [
        "# Process and Annotate Video Frame for Vehicle Turn Detection\n",
        "\n",
        "The function **process_frame** handles a single video frame in the vehicle turn detection pipeline. Here's a breakdown of its functionality:\n",
        "\n",
        "1. Runs object detection on the input frame using a **YOLO model (from config[\"model\"])**, with specified confidence and IoU thresholds.\n",
        "\n",
        "2. Converts detection results into a standardized format (**sv.Detections**) and forces all detected class IDs to zero (indicating a single-class tracking scenario, like vehicles).\n",
        "\n",
        "3. Updates object tracks using a tracking algorithm (**config[\"tracker\"]**).\n",
        "\n",
        "4. Checks zone entry/exit: For each pair of entry (**zone_in**) and exit (**zone_out**) zones, it filters detections currently inside these zones.\n",
        "\n",
        "5. Filters detections further using a custom **detections_manager** function that processes zone-based transitions to determine vehicle turns.\n",
        "\n",
        "6. Annotates the frame (e.g., drawing bounding boxes and turn labels) using the **annotate_frame** function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RAl0Wo5-vF2"
      },
      "outputs": [],
      "source": [
        "def process_frame(frame: np.ndarray, config: Dict[str, Any]) -> np.ndarray:\n",
        "    result = config[\"model\"](frame, verbose=False, conf=config[\"conf_threshold\"], iou=config[\"iou_threshold\"])[0]\n",
        "    detections = sv.Detections.from_ultralytics(result)\n",
        "    detections.class_id = np.zeros(len(detections))\n",
        "    detections = config[\"tracker\"].update_with_detections(detections)\n",
        "\n",
        "    detections_in_zones = []\n",
        "    for zone_in in config[\"zones\"].values():\n",
        "        in_zone = detections[zone_in.trigger(detections)]\n",
        "        detections_in_zones.append(in_zone)\n",
        "\n",
        "    filtered = config[\"detections_manager\"](detections, detections_in_zones,config)\n",
        "    return annotate_frame(frame, filtered, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-dsim3RcgV"
      },
      "source": [
        "# Process and Annotate Video Frames for Vehicle Turn Detection\n",
        "\n",
        "The **process_video** function reads a video frame-by-frame, processes each frame to detect and annotate vehicles, and outputs the results either to a video file or a live display window. Here's what it does:\n",
        "\n",
        "1. Frame Extraction: Uses a frame generator to read frames from the source video.\n",
        "\n",
        "2. Progress Tracking: Displays a live progress bar using the rich library to monitor video processing.\n",
        "\n",
        "3. Annotation Pipeline:\n",
        "    *   For each frame, it calls **process_frame**() to detect vehicles, determine turn behavior, and apply annotations.\n",
        "\n",
        "4. Output Handling:\n",
        "\n",
        "    *   If output path is specified: Saves the annotated video to disk using VideoSink.\n",
        "    *   Otherwise: Displays annotated frames live using OpenCV (**cv2.imshow**).\n",
        "\n",
        "5. Sample Frame Export: Saves a single annotated frame as an image (annotated_output.png) for preview or debugging.\n",
        "\n",
        "6. Returns the dictionary of vehicle turn states (vehicle_turns) tracked during the video processing.\n",
        "\n",
        "This function is the core executor of the turn detection pipeline, enabling both real-time display and file output of the analyzed results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za412PJPRcA4"
      },
      "outputs": [],
      "source": [
        "def process_video(config: Dict[str, Any]) -> None:\n",
        "    frame_generator = sv.get_video_frames_generator(config[\"source_video_path\"])\n",
        "    total = config[\"video_info\"].total_frames\n",
        "\n",
        "    with Progress() as progress:\n",
        "        task = progress.add_task(\"[green]Processing video...\", total=total)\n",
        "        if config[\"target_video_path\"]:\n",
        "            with sv.VideoSink(config[\"target_video_path\"], config[\"video_info\"]) as sink:\n",
        "                saved_sample = False\n",
        "                for frame in frame_generator:\n",
        "                    annotated = process_frame(frame, config)\n",
        "                    sink.write_frame(annotated)\n",
        "                    if not saved_sample:\n",
        "                        cv2.imwrite(\"annotated_output.png\", annotated)\n",
        "                        saved_sample = True\n",
        "                    progress.advance(task)\n",
        "        else:\n",
        "            for frame in frame_generator:\n",
        "                annotated = process_frame(frame, config)\n",
        "                cv2.imshow(\"Processed Video\", annotated)\n",
        "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "                    break\n",
        "                progress.advance(task)\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "    return detections_state[\"vehicle_turns\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NG4ESNuq9Zs"
      },
      "source": [
        "# Append Turn Analysis Summary Chart to Video Output\n",
        "\n",
        "The function add_final_summary_to_video enhances a processed video by appending a visual summary of vehicle turn statistics at the end. Here's what it does:\n",
        "\n",
        "1. Analyzes turn data using the provided vehicle turn state.\n",
        "\n",
        "2. Loads a bar chart image (turn_analysis.png) that visually represents turn statistics.\n",
        "\n",
        "3. Reads and copies all frames from the original processed video.\n",
        "\n",
        "4. Appends the chart image as static frames for 5 seconds at the end of the video.\n",
        "\n",
        "5. Saves the new video with the summary chart to the specified output path.\n",
        "\n",
        "6. Returns a structured JSON summary of the vehicle turn data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm56mopSq6ts"
      },
      "outputs": [],
      "source": [
        "def add_final_summary_to_video(video_path, vehicle_turns, output_path=\"final_output.mp4\"):\n",
        "    \"\"\"Add a final summary frame to the end of the video\"\"\"\n",
        "\n",
        "    # First analyze the turns\n",
        "    vehicle_turn_json = analyze_turns(vehicle_turns)\n",
        "\n",
        "    # Load the bar chart image\n",
        "    chart_img = cv2.imread(\"turn_analysis.png\")\n",
        "    if chart_img is None:\n",
        "        raise FileNotFoundError(\"turn_analysis.png not found.\")\n",
        "\n",
        "    # Read the original video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Resize chart to match video resolution\n",
        "    chart_img = cv2.resize(chart_img, (width, height))\n",
        "\n",
        "    # Create the output video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') #mp4v h264\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Copy all frames from the original video\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        out.write(frame)\n",
        "\n",
        "    # Append chart image as 5 seconds of frames\n",
        "    for _ in range(int(fps * 5)):\n",
        "        out.write(chart_img)\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Final video with chart saved as '{output_path}'\")\n",
        "\n",
        "    return vehicle_turn_json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdmIGrtbq5eV"
      },
      "source": [
        "# Init Video Processing (Full Video-Based Vehicle Turn Detection and Summary Pipeline)\n",
        "\n",
        "This function **run_full_vehicle_turn_pipeline** performs the complete pipeline for analyzing vehicle movements in a video. It:\n",
        "\n",
        "1. Processes the input video using a configured video processor to detect and trace vehicle movements.\n",
        "\n",
        "2. Analyzes vehicle turns (left, right, U-turn, straight) and records them.\n",
        "\n",
        "3. Generates a summary chart of the turn statistics and appends it to the output video.\n",
        "\n",
        "4. Returns a JSON summary of vehicle turn analytics for further use (e.g., visualization or question answering).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek-5O71YrFnp"
      },
      "outputs": [],
      "source": [
        "def run_full_vehicle_turn_pipeline(\n",
        "    source_video_path: str,\n",
        "    final_output_path: str = \"final_output.mp4\",\n",
        "    zones: List[list] = [],\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs the full pipeline: processes video, tracks turns, and appends summary.\n",
        "    \"\"\"\n",
        "    if not zones:\n",
        "        raise ValueError(\"'zones' must contain at least one polygon.\")\n",
        "\n",
        "    # Step 1: Setup and process the video\n",
        "    config = setup_video_processor(\n",
        "        source_video_path=source_video_path,\n",
        "        target_video_path=\"output_traced.mp4\",\n",
        "        zones=zones\n",
        "    )\n",
        "    vehicle_turns_state = process_video(config)\n",
        "\n",
        "    # Step 2: Append summary chart to the traced video\n",
        "    vehicle_turn_json = add_final_summary_to_video(\n",
        "        video_path=\"output_traced.mp4\",\n",
        "        vehicle_turns=vehicle_turns_state,\n",
        "        output_path=final_output_path\n",
        "    )\n",
        "\n",
        "    return vehicle_turn_json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewXOvNlM-_26"
      },
      "source": [
        "# Vehicle Turn Detection Summary & AI-Powered Question Answering.\n",
        "**convert_turn_stats_to_text(analysis_result)**:\n",
        "Converts the vehicle turn detection results (a JSON dictionary) into a readable text summary, including:\n",
        "\n",
        "1. Total vehicle count\n",
        "\n",
        "2. Turn type counts (right, left, U-turn, straight)\n",
        "\n",
        "3. Per-vehicle turn information.\n",
        "\n",
        "**Use:**\n",
        "This summary is later passed to a language model for answering questions.\n",
        "\n",
        "**create_pipeline(text_data)**:\n",
        "Creates a custom question-answering function qa_pipeline(question) that:\n",
        "\n",
        "1. Takes a natural language question\n",
        "\n",
        "2. Feeds it to Qwen along with the vehicle turn summary\n",
        "\n",
        "3. Returns only the assistant's reply from the model output\n",
        "\n",
        "**Purpose:**\n",
        "This abstracts the model usage so the user can ask follow-up questions based on video analytics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es4286u__GMU"
      },
      "outputs": [],
      "source": [
        "def convert_turn_stats_to_text(analysis_result):\n",
        "    turn_counts = analysis_result.get(\"turn_counts\", {})\n",
        "    turn_details = analysis_result.get(\"turn_details\", [])\n",
        "\n",
        "    total = analysis_result.get(\"total_vehicles\", 0)\n",
        "    right = turn_counts.get(\"Vehicles making right turns\", 0)\n",
        "    left = turn_counts.get(\"Vehicles making left turns\", 0)\n",
        "    u_turn = turn_counts.get(\"Vehicles making U-turns\", 0)\n",
        "    straight = turn_counts.get(\"Vehicles with no detected turns (Straight)\", 0)\n",
        "\n",
        "    summary_text = (\n",
        "    f\"A total of {total} cars were tracked during the analysis. \"\n",
        "    f\"Among them, {right} made right turns, {left} made left turns, \"\n",
        "    f\"{u_turn} performed U-turns (also referred to as 'uturns' or 'reverse turns'), and {straight} continued straight without making any turns. (also referred to as 'no turns')\"\n",
        "    )\n",
        "\n",
        "    if turn_details:\n",
        "        detail_sentences = [\n",
        "            f\"Vehicle ID {item['tracker_id']} made a {item['turn'].replace('_', ' ').lower()}.\"\n",
        "            for item in turn_details\n",
        "        ]\n",
        "        details_text = \" \".join(detail_sentences)\n",
        "        return f\"{summary_text}{details_text}\"\n",
        "    else:\n",
        "        return f\"{summary_text}. No individual vehicle turn details were recorded.\"\n",
        "\n",
        "\n",
        "\n",
        "# Load Qwen model and tokenizer (only once globally)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "generation_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "def create_pipeline(text_data):\n",
        "    \"\"\"\n",
        "    Create a simple function to handle QA using Qwen with the full text_data\n",
        "    \"\"\"\n",
        "    def qa_pipeline(question):\n",
        "        text = f\"\"\"\n",
        "        You are an expert in analyzing traffic video data, specifically vehicle turn behavior.\n",
        "\n",
        "        Answer the question as thoroughly as possible using only the provided context. If the answer is not present in the context, respond with: \"Answer is not available in the context.\" Do not provide fabricated or assumed information.\n",
        "\n",
        "        Context:\n",
        "        {text_data}\n",
        "\n",
        "        Based on the above context, answer the following question clearly and concisely:\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert in vehicle turn analysis. Respond clearly and accurately using only the provided context.\"},\n",
        "            {\"role\": \"user\", \"content\": text},\n",
        "        ]\n",
        "        response = generation_pipe(messages, max_new_tokens=1000)[0]\n",
        "        print(response)\n",
        "        assistant_response = \"\"\n",
        "        for msg in response['generated_text']:\n",
        "            if msg.get(\"role\") == \"assistant\":\n",
        "                assistant_response = msg.get(\"content\", \"\")\n",
        "                break\n",
        "        return assistant_response\n",
        "\n",
        "    return qa_pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyJSSXxLS5VZ"
      },
      "source": [
        "# Vehicle Turn Detection with Interactive Zone Drawing Using Gradio\n",
        "Interactive Vehicle Turn Detection with Zone-Based Video Analysis\n",
        "\n",
        "**gradio** is used to create an interactive web UI for uploading a video, processing it, and asking questions.\n",
        "\n",
        "**tempfile** is used to handle temporary storage of the uploaded video.\n",
        "\n",
        "This Python application provides an interactive web interface to detect vehicle turns in traffic videos by allowing users to manually draw polygonal zones on the first video frame. Built using Gradio for the UI and OpenCV for video processing, the tool enables the following workflow:\n",
        "\n",
        "1. Upload a traffic video in common formats like MP4 or AVI.\n",
        "\n",
        "2. Extract and display the first frame of the video for zone drawing.\n",
        "\n",
        "3. Draw multiple polygonal zones (Z1, Z2, Z3, Z4, etc.) on the frame to define regions of interest in an intersection pattern.\n",
        "\n",
        "4. Visualize the drawn zones with distinct colors and labels.\n",
        "\n",
        "5. Analyze the video based on the defined zones to detect vehicle turns and movements.\n",
        "\n",
        "6. View the processed video highlighting vehicle turn events.\n",
        "\n",
        "7. Interact with the analysis by asking questions about vehicle turns through a natural language interface powered by a custom pipeline.\n",
        "\n",
        "\n",
        "(**encode_to_browser_safe_mp4**): This function converts a video file to a browser-safe MP4 format using **ffmpeg**.To transcode a video (any format) into an MP4 file that's optimized for web playback in browsers (like Chrome, Firefox, Safari, etc.).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ICAxAcnukpS"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import tempfile\n",
        "import subprocess\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Persistent state for document store and pipeline\n",
        "global_pipeline = None\n",
        "global_turn_json = None\n",
        "\n",
        "class ZoneDrawer:\n",
        "    def __init__(self):\n",
        "        self.zones = []  # Simple list of zones\n",
        "        self.current_frame = None\n",
        "        self.video_path = None\n",
        "\n",
        "    def process_video(self, video_file):\n",
        "        \"\"\"Extract first frame from uploaded video\"\"\"\n",
        "        if video_file is None:\n",
        "            return None, \"Please upload a video file first.\"\n",
        "\n",
        "        try:\n",
        "            # Store video path for later use\n",
        "            self.video_path = video_file\n",
        "\n",
        "            # Read video and extract first frame\n",
        "            cap = cv2.VideoCapture(video_file)\n",
        "            ret, frame = cap.read()\n",
        "            cap.release()\n",
        "\n",
        "            if not ret:\n",
        "                return None, \"❌ Could not extract frame from video.\"\n",
        "\n",
        "            # Convert BGR to RGB for display\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            self.current_frame = frame_rgb\n",
        "\n",
        "            return frame_rgb, f\"✅ Video loaded! Frame size: {frame_rgb.shape[1]}x{frame_rgb.shape[0]}. Draw zones then analyze.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"❌ Error processing video: {str(e)}\"\n",
        "\n",
        "    def process_drawing(self, image_data):\n",
        "        \"\"\"Process the drawn image and extract polygon points\"\"\"\n",
        "        if image_data is None:\n",
        "            return None, \"No drawing data received.\"\n",
        "\n",
        "        try:\n",
        "            # Convert the drawing to numpy array\n",
        "            if isinstance(image_data, dict) and 'layers' in image_data:\n",
        "                drawing = image_data['layers'][0] if image_data['layers'] else image_data['background']\n",
        "            else:\n",
        "                drawing = image_data\n",
        "\n",
        "            # Convert PIL Image to numpy array\n",
        "            if isinstance(drawing, Image.Image):\n",
        "                drawing_array = np.array(drawing)\n",
        "            else:\n",
        "                drawing_array = drawing\n",
        "\n",
        "            # Extract polygon points from the drawing\n",
        "            points = self.extract_polygon_points(drawing_array)\n",
        "\n",
        "            if len(points) < 3:\n",
        "                return self.show_current_zones(), \"⚠️ Please draw a polygon with at least 3 points.\"\n",
        "\n",
        "            # Add to zones list\n",
        "            self.zones.append(points)\n",
        "\n",
        "            # Create updated visualization\n",
        "            result_image = self.create_zone_visualization()\n",
        "\n",
        "            zone_count = len(self.zones)\n",
        "            return result_image, f\"✅ Added zone Z{zone_count}! Total zones: {zone_count}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return self.show_current_zones(), f\"❌ Error processing drawing: {str(e)}\"\n",
        "\n",
        "    def extract_polygon_points(self, drawing_array):\n",
        "        \"\"\"Extract polygon points from drawn image\"\"\"\n",
        "        # Convert to grayscale for processing\n",
        "        if len(drawing_array.shape) == 3:\n",
        "            gray = cv2.cvtColor(drawing_array, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = drawing_array\n",
        "\n",
        "        # Find contours in the drawing\n",
        "        contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        if not contours:\n",
        "            return []\n",
        "\n",
        "        # Get the largest contour\n",
        "        largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "        # Approximate the contour to reduce points\n",
        "        epsilon = 0.02 * cv2.arcLength(largest_contour, True)\n",
        "        approx = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
        "\n",
        "        # Convert to list of [x, y] points\n",
        "        points = [[int(point[0][0]), int(point[0][1])] for point in approx]\n",
        "\n",
        "        return points\n",
        "\n",
        "    def create_zone_visualization(self):\n",
        "        \"\"\"Create visualization with all zones\"\"\"\n",
        "        if self.current_frame is None:\n",
        "            return None\n",
        "\n",
        "        # Create a copy of the frame\n",
        "        vis_frame = self.current_frame.copy()\n",
        "\n",
        "        # Draw all zones with different colors\n",
        "        colors = [\n",
        "            (0, 255, 0),    # Green\n",
        "            (255, 0, 0),    # Red\n",
        "            (0, 0, 255),    # Blue\n",
        "            (255, 255, 0),  # Yellow\n",
        "            (255, 0, 255),  # Magenta\n",
        "            (0, 255, 255),  # Cyan\n",
        "            (255, 165, 0),  # Orange\n",
        "            (128, 0, 128),  # Purple\n",
        "        ]\n",
        "\n",
        "        for i, zone_points in enumerate(self.zones):\n",
        "            if len(zone_points) >= 3:\n",
        "                # Use different colors for different zones (cycle through colors)\n",
        "                color = colors[i % len(colors)]\n",
        "\n",
        "                # Convert points to numpy array\n",
        "                pts = np.array(zone_points, np.int32)\n",
        "\n",
        "                # Draw filled polygon with transparency\n",
        "                overlay = vis_frame.copy()\n",
        "                cv2.fillPoly(overlay, [pts], color)\n",
        "                cv2.addWeighted(vis_frame, 0.7, overlay, 0.3, 0, vis_frame)\n",
        "\n",
        "                # Draw border\n",
        "                cv2.polylines(vis_frame, [pts], isClosed=True, color=color, thickness=3)\n",
        "\n",
        "                # Add zone label (Z1, Z2, Z3, Z4)\n",
        "                center = np.mean(pts, axis=0).astype(int)\n",
        "                zone_label = f\"Z{i+1}\"\n",
        "                cv2.putText(vis_frame, zone_label, (center[0]-15, center[1]),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "                cv2.putText(vis_frame, zone_label, (center[0]-15, center[1]),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 1)\n",
        "\n",
        "        return vis_frame\n",
        "\n",
        "    def show_current_zones(self):\n",
        "        \"\"\"Show current zones without adding new ones\"\"\"\n",
        "        return self.create_zone_visualization()\n",
        "\n",
        "    def clear_last_zone(self):\n",
        "        \"\"\"Clear the last drawn zone\"\"\"\n",
        "        if self.zones:\n",
        "            removed_zone = len(self.zones)\n",
        "            self.zones.pop()\n",
        "            result_image = self.create_zone_visualization()\n",
        "            return result_image, f\"🗑️ Removed zone Z{removed_zone}. Remaining zones: {len(self.zones)}\"\n",
        "        else:\n",
        "            result_image = self.show_current_zones()\n",
        "            return result_image, \"⚠️ No zones to remove.\"\n",
        "\n",
        "    def clear_all_zones(self):\n",
        "        \"\"\"Clear all zones\"\"\"\n",
        "        total = len(self.zones)\n",
        "        self.zones = []\n",
        "        result_image = self.show_current_zones()\n",
        "        return result_image, f\"🔄 Cleared all {total} zones.\"\n",
        "\n",
        "    def get_zone_info(self):\n",
        "        \"\"\"Get current zone information\"\"\"\n",
        "        zone_count = len(self.zones)\n",
        "        zone_labels = ', '.join([f\"Z{i+1}\" for i in range(zone_count)]) if zone_count > 0 else \"None\"\n",
        "\n",
        "        info = (\n",
        "            f\"📊 **Zone Summary:**\\n\\n\"\n",
        "            f\"📍 **Total Zones:** {zone_count}\\n\"\n",
        "            f\"🏷️ **Zone Labels:** {zone_labels}\\n\\n\"\n",
        "            f\"📋 **Zone Coordinates:**\\n\"\n",
        "            f\"```json\\n\"\n",
        "            f\"{json.dumps(self.zones, indent=2)}\\n\"\n",
        "            f\"```\\n\\n\"\n",
        "            f\"🎯 **Ready for Analysis:** {'✅ Yes' if zone_count > 0 else '❌ Draw zones first'}\"\n",
        "        )\n",
        "\n",
        "        return info\n",
        "\n",
        "    def get_zones_for_pipeline(self):\n",
        "        \"\"\"Get zones in format expected by pipeline\"\"\"\n",
        "        return {'zones': self.zones}\n",
        "\n",
        "# Initialize the zone drawer\n",
        "zone_drawer = ZoneDrawer()\n",
        "\n",
        "def analyze_video_with_zones(video_file_path):\n",
        "    \"\"\"Analyze video using drawn zones\"\"\"\n",
        "    global global_pipeline, global_turn_json\n",
        "\n",
        "    if not video_file_path:\n",
        "        return None, \"Please upload a video file.\"\n",
        "\n",
        "    # Check if zones are drawn\n",
        "    zones_data = zone_drawer.get_zones_for_pipeline()\n",
        "    if not zones_data['zones']:\n",
        "        return None, \"❌ Please draw at least one zone before analyzing.\"\n",
        "\n",
        "    try:\n",
        "        # Create temporary file\n",
        "        with open(video_file_path, \"rb\") as source_file:\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as tmp_file:\n",
        "                tmp_file.write(source_file.read())\n",
        "                tmp_video_path = tmp_file.name\n",
        "\n",
        "        raw_output_path = \"raw_output.mp4\"\n",
        "        browser_safe_path = \"final_output_with_summary.mp4\"\n",
        "\n",
        "        # Save zones to temporary file for pipeline\n",
        "        zones_file = \"temp_zones.json\"\n",
        "        with open(zones_file, 'w') as f:\n",
        "            json.dump(zones_data, f, indent=2)\n",
        "\n",
        "        # Run the full vehicle turn detection pipeline with zones\n",
        "        global_turn_json = run_full_vehicle_turn_pipeline(\n",
        "            source_video_path=tmp_video_path,\n",
        "            final_output_path=raw_output_path,\n",
        "            zones=zones_data[\"zones\"]\n",
        "        )\n",
        "\n",
        "        # Re-encode video\n",
        "        encode_to_browser_safe_mp4(raw_output_path, browser_safe_path)\n",
        "\n",
        "        # # Create document store and pipeline for QA\n",
        "        text_data = convert_turn_stats_to_text(global_turn_json)\n",
        "        global_pipeline = create_pipeline(text_data)\n",
        "\n",
        "        # Clean up temporary files\n",
        "        if os.path.exists(zones_file):\n",
        "            os.remove(zones_file)\n",
        "        if os.path.exists(tmp_video_path):\n",
        "            os.remove(tmp_video_path)\n",
        "\n",
        "        zone_summary = f\"{len(zones_data['zones'])} zones (Z1-Z{len(zones_data['zones'])})\"\n",
        "        return browser_safe_path, f\"✅ Video analyzed successfully with {zone_summary}! You can now ask questions.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"❌ Error during analysis: {str(e)}\\nZones data: {zones_data}\"\n",
        "\n",
        "def answer_question(user_question):\n",
        "    \"\"\"Answer questions about the analyzed video\"\"\"\n",
        "    if not global_pipeline or not global_turn_json:\n",
        "        return \"Please analyze a video first.\"\n",
        "    return global_pipeline(user_question)\n",
        "\n",
        "def encode_to_browser_safe_mp4(input_path: str, output_path: str):\n",
        "    \"\"\"Convert video to browser-safe format\"\"\"\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-y\", \"-i\", input_path,\n",
        "        \"-vcodec\", \"libx264\", \"-preset\", \"ultrafast\",\n",
        "        \"-acodec\", \"aac\", \"-movflags\", \"+faststart\",\n",
        "        output_path\n",
        "    ]\n",
        "    try:\n",
        "        subprocess.run(cmd, check=True)\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"Error: ffmpeg failed to convert video to browser-safe format.\")\n",
        "\n",
        "# Create Gradio interface\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"Vehicle Turn Detection with Zone Drawing\", theme=gr.themes.Soft()) as interface:\n",
        "        gr.HTML(\"\"\"\n",
        "        <h1 style=\"text-align: center;\">🚗 Vehicle Turn Detection with Zone Drawing</h1>\n",
        "        <p style=\"text-align: center;\">Upload video → Draw zones in intersection pattern (Z1-Z4) → Analyze turns → Ask questions</p>\n",
        "        <div style=\"text-align: center; border: 1px solid #ffeaa7; border-radius: 5px; padding: 10px; margin: 10px 0;\">\n",
        "            <strong>⚠️ Important:</strong> Draw zones in intersection pattern (Z1-top, Z2-left, Z3-bottom, Z4-right) for accurate turn detection!\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tab(\"📹 Step 1: Upload & Draw Zones\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=2):\n",
        "                    # Video upload\n",
        "                    video_input = gr.File(\n",
        "                        label=\"📹 Upload Video\",\n",
        "                        file_types=[\".mp4\", \".avi\", \".mov\"],\n",
        "                        type=\"filepath\"\n",
        "                    )\n",
        "\n",
        "                    # Frame display and drawing area\n",
        "                    frame_display = gr.ImageEditor(\n",
        "                        label=\"🎨 Draw Zones in Intersection Pattern: Z1(top)→Z2(left)→Z3(bottom)→Z4(right)\",\n",
        "                        type=\"pil\",\n",
        "                        # tool=\"sketch\",\n",
        "                        height=400\n",
        "                    )\n",
        "\n",
        "                    # Control buttons\n",
        "                    with gr.Row():\n",
        "                        save_zone_btn = gr.Button(\"✅ Save Zone\", variant=\"primary\")\n",
        "                        clear_last_btn = gr.Button(\"🗑️ Remove Last Zone\")\n",
        "                        clear_all_btn = gr.Button(\"🔄 Clear All Zones\")\n",
        "\n",
        "                    # Status message\n",
        "                    zone_status = gr.Textbox(\n",
        "                        label=\"📢 Zone Status\",\n",
        "                        interactive=False,\n",
        "                        max_lines=2\n",
        "                    )\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    # Zone information\n",
        "                    zone_info = gr.Markdown(\n",
        "                        value=\"Upload a video to start drawing zones.\",\n",
        "                        label=\"📊 Zone Information\"\n",
        "                    )\n",
        "\n",
        "                    # Instructions\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ### 📖 Instructions:\n",
        "\n",
        "                    1. **Upload** a video file\n",
        "                    2. **Draw** polygons on the frame (labeled as Z1, Z2, Z3, Z4...)\n",
        "                    3. **Save** each zone after drawing\n",
        "                    4. Go to **Step 2** to analyze\n",
        "\n",
        "                    ### 🎨 Drawing Tips:\n",
        "                    - Use sketch tool to draw polygons\n",
        "                    - Zones are labeled as Z1, Z2, Z3, Z4...\n",
        "                    - Each zone gets a different color\n",
        "                    - Draw closed shapes for best results\n",
        "\n",
        "                    ### 🔄 **IMPORTANT - Zone Layout for Accurate Turn Detection:**\n",
        "\n",
        "                    **Recommended Zone Pattern:**\n",
        "                    ```\n",
        "                        Z1\n",
        "                    Z2      Z3\n",
        "                        Z4\n",
        "                    ```\n",
        "\n",
        "                    ### 📍 **Zone Positioning Guidelines:**\n",
        "                    - **Z1 (Top)**: North/Entry zone\n",
        "                    - **Z2 (Left)**: West/Left zone\n",
        "                    - **Z3 (Bottom)**: South/Exit zone\n",
        "                    - **Z4 (Right)**: East/Right zone\n",
        "                    - **Draw Anticlockwise Zones patterns**\n",
        "\n",
        "                    ### 🚗 Turn Detection Accuracy:\n",
        "                    - Zone drawing direction affects turn classification\n",
        "                    - Match drawing direction with vehicle flow for best results\n",
        "                    - Incorrect direction may cause turn misclassification\n",
        "                    \"\"\")\n",
        "\n",
        "        with gr.Tab(\"🔍 Step 2: Analyze Video\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    analyze_btn = gr.Button(\"🚀 Analyze Video with Zones\", variant=\"primary\", size=\"lg\")\n",
        "                    analysis_status = gr.Textbox(label=\"📊 Analysis Status\", interactive=False)\n",
        "\n",
        "                with gr.Column():\n",
        "                    video_output = gr.Video(label=\"📹 Processed Video\")\n",
        "\n",
        "        with gr.Tab(\"❓ Step 3: Ask Questions\"):\n",
        "            gr.Markdown(\"### Ask questions about the analyzed video:\")\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    question_input = gr.Textbox(\n",
        "                        label=\"💬 Your Question\",\n",
        "                        placeholder=\"e.g., How many U-turns were made? What was the most common turn type?\",\n",
        "                        lines=2\n",
        "                    )\n",
        "                with gr.Column():\n",
        "                    answer_output = gr.Textbox(\n",
        "                        label=\"🤖 Answer\",\n",
        "                        lines=4,\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "        # Event handlers for zone drawing\n",
        "        video_input.change(\n",
        "            fn=zone_drawer.process_video,\n",
        "            inputs=[video_input],\n",
        "            outputs=[frame_display, zone_status]\n",
        "        )\n",
        "\n",
        "        save_zone_btn.click(\n",
        "            fn=zone_drawer.process_drawing,\n",
        "            inputs=[frame_display],\n",
        "            outputs=[frame_display, zone_status]\n",
        "        ).then(\n",
        "            fn=zone_drawer.get_zone_info,\n",
        "            outputs=[zone_info]\n",
        "        )\n",
        "\n",
        "        clear_last_btn.click(\n",
        "            fn=zone_drawer.clear_last_zone,\n",
        "            outputs=[frame_display, zone_status]\n",
        "        ).then(\n",
        "            fn=zone_drawer.get_zone_info,\n",
        "            outputs=[zone_info]\n",
        "        )\n",
        "\n",
        "        clear_all_btn.click(\n",
        "            fn=zone_drawer.clear_all_zones,\n",
        "            outputs=[frame_display, zone_status]\n",
        "        ).then(\n",
        "            fn=zone_drawer.get_zone_info,\n",
        "            outputs=[zone_info]\n",
        "        )\n",
        "\n",
        "        # Event handler for video analysis\n",
        "        analyze_btn.click(\n",
        "            fn=lambda: analyze_video_with_zones(zone_drawer.video_path),\n",
        "            outputs=[video_output, analysis_status]\n",
        "        )\n",
        "\n",
        "        # Event handler for Q&A\n",
        "        question_input.submit(\n",
        "            fn=answer_question,\n",
        "            inputs=[question_input],\n",
        "            outputs=[answer_output]\n",
        "        )\n",
        "\n",
        "        question_input.change(\n",
        "            fn=answer_question,\n",
        "            inputs=[question_input],\n",
        "            outputs=[answer_output]\n",
        "        )\n",
        "\n",
        "    return interface\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface = create_interface()\n",
        "    interface.launch(\n",
        "        share=True,\n",
        "        debug=True,\n",
        "        show_error=True\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}