{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTmbZZnKxhjQ"
      },
      "source": [
        "# Cloning the Fine Tuned Model From my GitHub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0--KYU2xxdsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cd17da0-1026-47ff-9576-790b2e9e322a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'grootan_ai_task'...\n",
            "remote: Enumerating objects: 323, done.\u001b[K\n",
            "remote: Counting objects: 100% (170/170), done.\u001b[K\n",
            "remote: Compressing objects: 100% (137/137), done.\u001b[K\n",
            "remote: Total 323 (delta 126), reused 42 (delta 33), pack-reused 153 (from 1)\u001b[K\n",
            "Receiving objects: 100% (323/323), 231.23 MiB | 14.17 MiB/s, done.\n",
            "Resolving deltas: 100% (206/206), done.\n",
            "Updating files: 100% (7/7), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karthiKN-sk/grootan_ai_task.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdujpcM3xtm9"
      },
      "source": [
        "# Download the Library/Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhF1tps9wXBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab89d33f-49c3-44a2-f35f-cfe757535f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.148-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting supervision\n",
            "  Downloading supervision-0.25.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting rich.progress\n",
            "  Downloading rich_progress-0.4.0-py3-none-any.whl.metadata (797 bytes)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.32.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.31.4)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from supervision) (1.3.2)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: rich>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from rich.progress) (13.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.10.2 (from gradio)\n",
            "  Downloading gradio_client-1.10.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.0.0->rich.progress) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.0.0->rich.progress) (2.19.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.0.0->rich.progress) (0.1.2)\n",
            "Downloading ultralytics-8.3.148-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading supervision-0.25.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_progress-0.4.0-py3-none-any.whl (4.8 kB)\n",
            "Downloading gradio-5.32.1-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.2-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python ultralytics supervision numpy matplotlib rich.progress transformers gradio pillow huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwBPpqTxQ9HG"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nfkW4B2RBa6"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import supervision as sv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from rich.progress import Progress\n",
        "from typing import Dict, Iterable, List, Optional, Set, Any\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import os\n",
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdipK6N5IKIA"
      },
      "source": [
        "# Setting HF Token in Environment variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH4YRyCdIGvS"
      },
      "outputs": [],
      "source": [
        "token = None\n",
        "token_file = \"/content/grootan_ai_task/variables.py\"\n",
        "\n",
        "with open(token_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        if line.startswith(\"HF_TOKEN=\"):\n",
        "            token = line.strip().split(\"=\", 1)[1].strip('\"').strip(\"'\")\n",
        "            break\n",
        "\n",
        "if token:\n",
        "    os.environ[\"HF_TOKEN\"] = token\n",
        "    login(token=token)\n",
        "else:\n",
        "    raise ValueError(\"HF_TOKEN not found in variables.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95wvjpMzQqAk"
      },
      "source": [
        "# Configuration for Zone Definitions, Color Palette, and Vehicle Turn Classification.\n",
        "\n",
        "This code snippet sets up essential constants and configurations for a vehicle tracking and turn analysis system, including color settings, polygonal zone definitions, naming schemes, turn mapping logic, and a utility function for turn-based color annotation.\n",
        "\n",
        "1. Color Palette Definition\n",
        "    * Defines a reusable color palette used for drawing zones, bounding boxes, and labels.\n",
        "\n",
        "2. Turn Mapping Logic\n",
        "\n",
        "      * Uses predefined TURN_MAPPING to determine the turn type based on zone pairs (in → out)\n",
        "    \n",
        "This setup provides a foundational configuration layer for visual annotation, spatial zone management, and logical turn analysis in a computer vision-based traffic monitoring system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkVRIaZiQmA-"
      },
      "outputs": [],
      "source": [
        "COLORS = sv.ColorPalette.from_hex([\"#E6194B\", \"#3CB44B\", \"#FFE119\", \"#3C76D1\"])\n",
        "\n",
        "# Four way turn mapping\n",
        "FOUR_WAY_TURN_MAPPING =  {\n",
        "    # Entry from Z1\n",
        "    (\"Z1\", \"Z1\"): \"u_turn\",\n",
        "    (\"Z1\", \"Z2\"): \"right_turn\",\n",
        "    (\"Z1\", \"Z3\"): \"straight\",\n",
        "    (\"Z1\", \"Z4\"): \"left_turn\",\n",
        "\n",
        "    # Entry from Z2\n",
        "    (\"Z2\", \"Z2\"): \"u_turn\",\n",
        "    (\"Z2\", \"Z3\"): \"right_turn\",\n",
        "    (\"Z2\", \"Z4\"): \"straight\",\n",
        "    (\"Z2\", \"Z1\"): \"left_turn\",\n",
        "\n",
        "    # Entry from Z3\n",
        "    (\"Z3\", \"Z3\"): \"u_turn\",\n",
        "    (\"Z3\", \"Z4\"): \"right_turn\",\n",
        "    (\"Z3\", \"Z1\"): \"straight\",\n",
        "    (\"Z3\", \"Z2\"): \"left_turn\",\n",
        "\n",
        "    # Entry from Z4\n",
        "    (\"Z4\", \"Z4\"): \"u_turn\",\n",
        "    (\"Z4\", \"Z1\"): \"right_turn\",\n",
        "    (\"Z4\", \"Z2\"): \"straight\",\n",
        "    (\"Z4\", \"Z3\"): \"left_turn\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkO7vlUM-63E"
      },
      "source": [
        "# Global Detection State Initialization for Vehicle Turn Tracking.\n",
        "\n",
        "This snippet initializes a global dictionary named detections_state that is used to persist and manage tracking data across video frames in a vehicle monitoring pipeline.\n",
        "\n",
        "Dictionary Keys:\n",
        "\n",
        "* \"**tracker_id_to_zone_id**\":\n",
        "   Maps each unique vehicle (tracker_id) to the zone ID it first appeared in.\n",
        "   Used to group and identify vehicles during processing.\n",
        "\n",
        "* \"**vehicle_paths**\":\n",
        "Tracks both the entry (in) and exit (out) zones for each vehicle.\n",
        "Format: **{tracker_id: {\"in\": zone_name, \"out\": zone_name}}**.\n",
        "\n",
        "* \"**vehicle_turns**\":\n",
        "Stores the classified type of turn for each vehicle, such as \"left_turn\", \"right_turn\", \"u_turn\", or \"straight\".\n",
        "\n",
        "This stateful object is referenced and updated throughout the processing pipeline to enable accurate vehicle path tracking and turn classification across video frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tWD_02a-7cS"
      },
      "outputs": [],
      "source": [
        "detections_state = {\n",
        "    \"tracker_id_to_zone_id\": {},\n",
        "    \"vehicle_paths\": {},\n",
        "    \"vehicle_turns\": {},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7if-apVfOx0H"
      },
      "source": [
        "# Vehicle Entry-Exit Tracking and Turn Classification Logic\n",
        "\n",
        "This function, update_detections_state, manages and updates the internal state used to track vehicle movement through predefined zones, and classifies the type of turn each vehicle makes (left, right, U-turn, or straight).\n",
        "\n",
        "Key Responsibilities:\n",
        "\n",
        "1. Track Entry Zones:\n",
        "\n",
        "    * Maps each vehicle (via its tracker ID) to the in zone where it first appeared.\n",
        "\n",
        "    * Ensures the entry point is recorded only once per vehicle.\n",
        "\n",
        "    * Detects and records the out zone where the vehicle exits.\n",
        "\n",
        "2. Turn Detection:\n",
        "\n",
        "    * Uses predefined TURN_MAPPING to determine the turn type based on zone pairs (in → out).\n",
        "\n",
        "    * Updates the vehicle_turns dictionary only when both zones are known.\n",
        "\n",
        "3. Class ID Assignment:\n",
        "\n",
        "    * Associates each detection with a zone ID for visual annotation by mapping tracker_id to its zone_id.\n",
        "\n",
        "4. Filtering Valid Detections:\n",
        "\n",
        "    * Returns only detections that have been successfully associated with zones (i.e., not class ID -1).\n",
        "\n",
        "This function plays a central role in transforming low-level detection data into high-level vehicle movement understanding, essential for turn analysis in traffic videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IULBAHSO5Cc"
      },
      "outputs": [],
      "source": [
        "def update_detections_state(\n",
        "    detections_all: sv.Detections,\n",
        "    detections_in_zones: List[sv.Detections],\n",
        "    detections_out_zones: List[sv.Detections],\n",
        "    config: Dict[str, Any],\n",
        "    state: Dict[str, Any] = detections_state\n",
        ") -> sv.Detections:\n",
        "    tracker_id_to_zone_id = state[\"tracker_id_to_zone_id\"]\n",
        "    tracker_id_to_exit_zone_id = state.setdefault(\"tracker_id_to_exit_zone_id\", {})\n",
        "    vehicle_paths = state[\"vehicle_paths\"]\n",
        "    vehicle_turns = state.setdefault(\"vehicle_turns\", {})\n",
        "\n",
        "\n",
        "    # --- Assign entry zones and track vehicle IN zone names ---\n",
        "    zone_in_names = list(config[\"zones_in\"].keys())  # cache the keys list\n",
        "    for zone_in_id, detections_in_zone in enumerate(detections_in_zones):\n",
        "        zone_name = zone_in_names[zone_in_id]\n",
        "        for tracker_id in detections_in_zone.tracker_id:\n",
        "            tracker_id_to_zone_id.setdefault(tracker_id, zone_in_id)\n",
        "\n",
        "            # Initialize vehicle path if not already present\n",
        "            vehicle_paths.setdefault(tracker_id, {\"in\": None, \"out\": None})\n",
        "            if vehicle_paths[tracker_id][\"in\"] is None:\n",
        "                vehicle_paths[tracker_id][\"in\"] = zone_name\n",
        "\n",
        "    # --- Count exits grouped by entry zone and track vehicle OUT zone names ---\n",
        "    zone_out_names = list(config[\"zones_out\"].keys())\n",
        "    for zone_out_id, detections_out_zone in enumerate(detections_out_zones):\n",
        "        zone_name = zone_out_names[zone_out_id]\n",
        "        for tracker_id in detections_out_zone.tracker_id:\n",
        "            if tracker_id in tracker_id_to_zone_id:\n",
        "                if vehicle_paths[tracker_id][\"out\"] is None:\n",
        "                    vehicle_paths[tracker_id][\"out\"] = zone_name\n",
        "                tracker_id_to_exit_zone_id[tracker_id] = zone_out_id\n",
        "\n",
        "    # --- Detect turns ---\n",
        "    for tracker_id, path in vehicle_paths.items():\n",
        "        in_zone = path[\"in\"]\n",
        "        out_zone = path[\"out\"]\n",
        "        if in_zone and out_zone and tracker_id not in vehicle_turns:\n",
        "            turn_type = config[\"turn_mapping\"].get((in_zone, out_zone))\n",
        "            if turn_type:\n",
        "                vehicle_turns[tracker_id] = turn_type\n",
        "                print(f\"[TURN] Vehicle {tracker_id}: {in_zone} → {out_zone} → {turn_type}\")\n",
        "\n",
        "    # Assign class_id based on EXIT zone\n",
        "    if len(detections_all) > 0:\n",
        "        detections_all.class_id = np.vectorize(\n",
        "            lambda x: tracker_id_to_exit_zone_id.get(x, tracker_id_to_zone_id.get(x, -1))\n",
        "        )(detections_all.tracker_id)\n",
        "    else:\n",
        "        detections_all.class_id = np.array([], dtype=int)\n",
        "\n",
        "    return detections_all[detections_all.class_id != -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEUvpM3lQydq"
      },
      "source": [
        "# Zone Initialization and PolygonZone dictionary Utilities.\n",
        "\n",
        "This code provides two utility functions used in the vehicle turn detection system:\n",
        "1. **initiate_polygon_zones**:\n",
        "\n",
        "    * Takes a list of zone names and corresponding polygon coordinates.\n",
        "    * Initializes and returns a dictionary mapping each name to a PolygonZone object.\n",
        "\n",
        "\n",
        "These functions are key to defining spatial zones for detecting vehicle movement and labeling them appropriately in the video annotation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6ViGjiZQy6w"
      },
      "outputs": [],
      "source": [
        "def initiate_polygon_zones(\n",
        "    names: List[str],\n",
        "    polygons: List[np.ndarray],\n",
        "    triggering_anchors: Iterable[sv.Position] = [sv.Position.CENTER],\n",
        ") -> Dict[str, sv.PolygonZone]:\n",
        "    return {\n",
        "        name: sv.PolygonZone(polygon=polygon, triggering_anchors=triggering_anchors)\n",
        "        for name, polygon in zip(names, polygons)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS4KoQo1PlXc"
      },
      "source": [
        "# Setup Configuration for Vehicle Turn Detection Pipeline.\n",
        "\n",
        "The **setup_video_processor** function initializes and returns a configuration dictionary containing all essential components and parameters needed to process a video for vehicle turn detection. Here's what it sets up:\n",
        "\n",
        "1. Video Input/Output Paths:\n",
        "\n",
        "      * **source_video_path**: Path to the input video.\n",
        "\n",
        "      * **target_video_path**: Optional path to save the processed output.\n",
        "\n",
        "2. Detection Parameters:\n",
        "\n",
        "      * **confidence_threshold**: Minimum confidence level for YOLO model detections.\n",
        "\n",
        "      * **iou_threshold**: IOU threshold used during object tracking.\n",
        "\n",
        "3. Detection and Tracking Tools:\n",
        "\n",
        "      * **model**: A fine-tuned YOLO model for vehicle detection.\n",
        "\n",
        "      * **tracker**: ByteTrack tracker for maintaining vehicle identities.\n",
        "\n",
        "4. Video Metadata:\n",
        "\n",
        "      * **video_info**: Extracts frame rate, resolution, and frame count from the input video.\n",
        "\n",
        "5. Zone Definitions:\n",
        "\n",
        "      * **zones**: Get polygon areas from User to detect Zones for turn classification.\n",
        "\n",
        "6. Annotation Tools:\n",
        "\n",
        "      * **box_annotator**: Draws bounding boxes on detected vehicles.\n",
        "\n",
        "      * **label_annotator**: Displays vehicle IDs.\n",
        "\n",
        "      * **trace_annotator**: Adds trajectory traces to show vehicle movement paths.\n",
        "\n",
        "7. Detection State Handler:\n",
        "\n",
        "      * **detections_manager**: A function to manage turn state updates.\n",
        "\n",
        "8. Zone Definitions (Get From User Polygons )\n",
        "\n",
        "    * ZONE_POLYGONS: List of polygonal coordinates marking entry zones for vehicles.\n",
        "    * Each polygon is an array of 2D points (x, y).\n",
        "    * Generate Zone Names Using Polygon\n",
        "\n",
        "This setup function centralizes the configuration, making it easy to pass all necessary components to the video processing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n6qfJvMPqPB"
      },
      "outputs": [],
      "source": [
        "def setup_video_processor(\n",
        "    source_video_path: str,\n",
        "    target_video_path: Optional[str] = None,\n",
        "    zones: Dict[str, list] = {},\n",
        "    confidence_threshold: float = 0.4,\n",
        "    iou_threshold: float = 0.7,\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    if not zones or not zones.get(\"entry\") or not zones.get(\"exit\"):\n",
        "        raise ValueError(\"'zones' must contain both non-empty 'entry' and 'exit' lists.\")\n",
        "\n",
        "    # Convert polygon lists to NumPy arrays\n",
        "    ZONE_IN_POLYGONS = [np.array(polygon, dtype=np.int32) for polygon in zones[\"entry\"]]\n",
        "    ZONE_OUT_POLYGONS = [np.array(polygon, dtype=np.int32) for polygon in zones[\"exit\"]]\n",
        "\n",
        "    ZONE_IN_NAMES = [f\"Z{i+1}\" for i in range(len(ZONE_IN_POLYGONS))]\n",
        "    ZONE_OUT_NAMES = [f\"Z{i+1}\" for i in range(len(ZONE_OUT_POLYGONS))]\n",
        "\n",
        "    TURN_MAPPING = FOUR_WAY_TURN_MAPPING\n",
        "\n",
        "    return {\n",
        "        \"conf_threshold\": confidence_threshold,\n",
        "        \"iou_threshold\": iou_threshold,\n",
        "        \"source_video_path\": source_video_path,\n",
        "        \"target_video_path\": target_video_path,\n",
        "        \"model\": YOLO(\"/content/grootan_ai_task/models/YoloFineTunedV3.pt\"),\n",
        "        \"tracker\": sv.ByteTrack(),\n",
        "        \"video_info\": sv.VideoInfo.from_video_path(source_video_path),\n",
        "        \"zones_in\": initiate_polygon_zones(ZONE_IN_NAMES,ZONE_IN_POLYGONS),\n",
        "        \"zones_out\": initiate_polygon_zones(ZONE_OUT_NAMES,ZONE_OUT_POLYGONS),\n",
        "        \"box_annotator\": sv.BoxAnnotator(color=COLORS),\n",
        "        \"label_annotator\": sv.LabelAnnotator(color=COLORS, text_color=sv.Color.BLACK),\n",
        "        \"trace_annotator\": sv.TraceAnnotator(\n",
        "            color=COLORS, position=sv.Position.CENTER, trace_length=100, thickness=2\n",
        "        ),\n",
        "        \"detections_manager\": update_detections_state,\n",
        "        \"turn_mapping\": TURN_MAPPING\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HffnMCTqKCu"
      },
      "source": [
        "# Analyze and Visualize Vehicle Turn Statistics\n",
        "\n",
        "The **analyze_turns** function evaluates vehicle turn data to generate a summary of turn behavior and visual insights. Here's what it does:\n",
        "\n",
        "1. Summary Computation:\n",
        "    *  Counts total tracked vehicles.\n",
        "    *  Computes how many made right turns, left turns, U-turns, or went straight.\n",
        "\n",
        "2. Console Output:\n",
        "    *   Prints a summary report of the turn counts to the terminal.\n",
        "\n",
        "3. Data Packaging:\n",
        "    *   Creates a JSON-style dictionary (**turn_message**) containing:\n",
        "        *   A message,\n",
        "        *   Overall turn statistics (**turn_counts**),\n",
        "        *   Individual vehicle turn details by tracker ID (**turn_details**).\n",
        "\n",
        "\n",
        "4. Visualization:\n",
        "\n",
        "      *   Plots a bar chart using **matplotlib** to visually represent the count of each turn type.\n",
        "      *   Saves the chart as **turn_analysis.png** for later use (e.g., appending to video).\n",
        "\n",
        "5. Return:\n",
        "      *   Outputs the structured **turn_message**, suitable for downstream use in reports or QA systems.\n",
        "\n",
        "This function bridges raw detection data with user-friendly output, supporting both analysis and visualization of vehicle behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x9VkqF5qKm2"
      },
      "outputs": [],
      "source": [
        "def analyze_turns(vehicle_turns):\n",
        "    \"\"\"Analyze the turns and create summary statistics\"\"\"\n",
        "    total_vehicles = len(vehicle_turns)\n",
        "    if total_vehicles == 0:\n",
        "        raise ValueError(\"No vehicles were detected or tracked.\")\n",
        "    # Count the different types of turns\n",
        "    right_turns = sum(1 for turns in vehicle_turns.values() if turns == \"right_turn\" )\n",
        "    left_turns = sum(1 for turns in vehicle_turns.values() if turns == \"left_turn\" )\n",
        "    u_turns = sum(1 for turns in vehicle_turns.values() if turns == \"u_turn\" )\n",
        "    no_turns = sum(1 for turns in vehicle_turns.values() if turns == \"straight\")\n",
        "\n",
        "    print(\"\\n--- Turn Analysis Results ---\")\n",
        "    print(f\"Total unique vehicles tracked: {total_vehicles}\")\n",
        "    print(f\"Vehicles making right turns: {right_turns} \")\n",
        "    print(f\"Vehicles making left turns: {left_turns}\")\n",
        "    print(f\"Vehicles making U-turns: {u_turns}\")\n",
        "    print(f\"Vehicles with no detected turns: {no_turns}\")\n",
        "\n",
        "\n",
        "    # Create a visualization\n",
        "    turn_counts = {\n",
        "        'Right Turn': right_turns,\n",
        "        'Left Turn': left_turns,\n",
        "        'U-Turn': u_turns,\n",
        "        'No Turn': no_turns\n",
        "    }\n",
        "\n",
        "    turn_message = {\n",
        "    \"message\": \"Turn Analysis Results completed.\",\n",
        "    \"total_vehicles\": total_vehicles,\n",
        "    \"turn_counts\": {\n",
        "        \"Vehicles making right turns\" : right_turns,\n",
        "        \"Vehicles making left turns\": left_turns,\n",
        "        \"Vehicles making U-turns\": u_turns,\n",
        "        \"Vehicles with no detected turns (Straight)\": no_turns\n",
        "    },\n",
        "     \"turn_details\": [\n",
        "        {\"tracker_id\": tracker_id, \"turn\": turn}\n",
        "        for tracker_id, turn in vehicle_turns.items()\n",
        "     ]\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    colors = ['red', 'green', 'black', 'blue']\n",
        "    plt.bar(turn_counts.keys(), turn_counts.values(), color=colors)\n",
        "    plt.title('Vehicle Turn Analysis')\n",
        "    plt.ylabel('Number of Vehicles')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Add count labels on top of each bar\n",
        "    for i, (key, value) in enumerate(turn_counts.items()):\n",
        "        plt.text(i, value + 0.3, str(value), ha='center')\n",
        "\n",
        "    plt.savefig('turn_analysis.png')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    return turn_message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNh4F1DGRnav"
      },
      "source": [
        "# Annotate Video Frames with Zone Information and Vehicle Turn Statistics\n",
        "\n",
        "\n",
        "The **annotate_frame** function overlays comprehensive visual annotations on each video frame to aid in understanding vehicle movement and behavior. Here's what it does:\n",
        "\n",
        "1. Draws zones on the frame using polygons and labels with distinct colors.\n",
        "\n",
        "2. Generates labels for each detected vehicle using their tracker IDs (e.g., **\"Car #12\"**).\n",
        "\n",
        "3. Applies multiple annotation layers:\n",
        "    *   Trajectory traces via **trace_annotator**\n",
        "    *   Bounding boxes via **box_annotator**\n",
        "    *   Vehicle ID labels via **label_annotator**\n",
        "\n",
        "4. Computes turn statistics from the global detections_state:\n",
        "    *   Total vehicles tracked\n",
        "    *   Counts of right turns, left turns, U-turns, and straight movements\n",
        "\n",
        "5. Displays summary metrics visually on the frame, including:\n",
        "    *   A detection count badge\n",
        "    *   Fixed-position statistics on turn types, color-coded for clarity (e.g., red for right turns, green for left turns, etc.)\n",
        "\n",
        "This function is central to making the video output interpretable by overlaying both spatial (zones) and behavioral (turn types) information for each detected vehicle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Puz5Hu3mRlus"
      },
      "outputs": [],
      "source": [
        "def annotate_frame(frame: np.ndarray, detections: sv.Detections, config: Dict[str, Any]) -> np.ndarray:\n",
        "    frame_ = frame.copy()\n",
        "\n",
        "    # Draw zones\n",
        "    for i, ((zin_name, zin), (zout_name, zout)) in enumerate(zip(config[\"zones_in\"].items(), config[\"zones_out\"].items())):\n",
        "        color = COLORS.colors[i % len(COLORS.colors)]\n",
        "        zin_anchor = sv.get_polygon_center(zin.polygon)\n",
        "        zout_anchor = sv.get_polygon_center(zout.polygon)\n",
        "        frame_ = sv.draw_polygon(frame_, zin.polygon, color)\n",
        "        frame_ = sv.draw_text(frame_, text=zin_name, text_anchor=zin_anchor, text_color=color)\n",
        "        frame_ = sv.draw_polygon(frame_, zout.polygon, color)\n",
        "        frame_ = sv.draw_text(frame_, text=zout_name, text_anchor=zout_anchor, text_color=color)\n",
        "\n",
        "    vehicle_turns= detections_state[\"vehicle_turns\"]\n",
        "\n",
        "    labels = [f\"Car #{id_}\" for id_ in detections.tracker_id]\n",
        "    frame_ = config[\"trace_annotator\"].annotate(frame_, detections)\n",
        "    frame_ = config[\"box_annotator\"].annotate(frame_, detections)\n",
        "    frame_ = config[\"label_annotator\"].annotate(frame_, detections, labels)\n",
        "\n",
        "    # Count the different types of turns\n",
        "    total_vehicles = len(vehicle_turns)\n",
        "    right_turns = sum(1 for turns in vehicle_turns.values() if turns == \"right_turn\" )\n",
        "    left_turns = sum(1 for turns in vehicle_turns.values() if turns == \"left_turn\" )\n",
        "    u_turns = sum(1 for turns in vehicle_turns.values() if turns == \"u_turn\" )\n",
        "    no_turns = sum(1 for turns in vehicle_turns.values() if turns == \"straight\")\n",
        "\n",
        "    # Add detection count info\n",
        "    total_count = len(detections)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        f\"Detected: {total_count}\",\n",
        "        sv.Point(50, 50),\n",
        "        background_color=sv.Color.from_hex(\"#FF7F50\")\n",
        "    )\n",
        "    # Draw fixed turn statistics on the center-left of the frame\n",
        "    start_x = 80\n",
        "    start_y = 350\n",
        "    line_spacing = 40\n",
        "    text_color = sv.Color(r=255, g=255, b=255)\n",
        "\n",
        "    # Line 1: Total vehicles tracked\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"Total vehicles tracked: {total_vehicles}\",\n",
        "        text_anchor=sv.Point(start_x + 30, start_y),\n",
        "        background_color=sv.Color.from_hex(\"#DDDDDD\"),\n",
        "    )\n",
        "\n",
        "    # Line 2: Right turns (Red)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"Right turns: {right_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + line_spacing),\n",
        "        background_color=sv.Color(r=255, g=0, b=0),\n",
        "        text_color=text_color\n",
        "    )\n",
        "\n",
        "    # Line 3: Left turns (Green)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"Left turns: {left_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + 2 * line_spacing),\n",
        "        background_color=sv.Color(r=0, g=255, b=0),\n",
        "\n",
        "    )\n",
        "\n",
        "    # Line 4: U-turns (Black)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"U-turns: {u_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + 3 * line_spacing),\n",
        "        background_color=sv.Color(r=0, g=0, b=0),\n",
        "        text_color=text_color\n",
        "    )\n",
        "\n",
        "    # Line 5: No turns (Blue)\n",
        "    frame_ = sv.draw_text(\n",
        "        frame_,\n",
        "        text=f\"No turns: {no_turns}\",\n",
        "        text_anchor=sv.Point(start_x + 10, start_y + 4 * line_spacing),\n",
        "        background_color=sv.Color(r=0, g=0, b=255),\n",
        "        text_color=text_color\n",
        "    )\n",
        "\n",
        "\n",
        "    return frame_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFjmcj3g-wSx"
      },
      "source": [
        "# Process and Annotate Video Frame for Vehicle Turn Detection\n",
        "\n",
        "The function **process_frame** handles a single video frame in the vehicle turn detection pipeline. Here's a breakdown of its functionality:\n",
        "\n",
        "1. Runs object detection on the input frame using a **YOLO model (from config[\"model\"])**, with specified confidence and IoU thresholds.\n",
        "\n",
        "2. Converts detection results into a standardized format (**sv.Detections**) and forces all detected class IDs to zero (indicating a single-class tracking scenario, like vehicles).\n",
        "\n",
        "3. Updates object tracks using a tracking algorithm (**config[\"tracker\"]**).\n",
        "\n",
        "4. Checks zone: it filters detections currently inside these zones.\n",
        "\n",
        "5. Filters detections further using a custom **detections_manager** function that processes zone-based transitions to determine vehicle turns.\n",
        "\n",
        "6. Annotates the frame (e.g., drawing bounding boxes and turn labels) using the **annotate_frame** function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RAl0Wo5-vF2"
      },
      "outputs": [],
      "source": [
        "def process_frame(frame: np.ndarray, config: Dict[str, Any]) -> np.ndarray:\n",
        "    result = config[\"model\"](frame, verbose=False, conf=config[\"conf_threshold\"], iou=config[\"iou_threshold\"])[0]\n",
        "    detections = sv.Detections.from_ultralytics(result)\n",
        "    detections.class_id = np.zeros(len(detections))\n",
        "    detections = config[\"tracker\"].update_with_detections(detections)\n",
        "\n",
        "    detections_in_zones, detections_out_zones = [], []\n",
        "    for zone_in, zone_out in zip(config[\"zones_in\"].values(), config[\"zones_out\"].values()):\n",
        "        in_zone = detections[zone_in.trigger(detections)]\n",
        "        out_zone = detections[zone_out.trigger(detections)]\n",
        "        detections_in_zones.append(in_zone)\n",
        "        detections_out_zones.append(out_zone)\n",
        "\n",
        "    filtered = config[\"detections_manager\"](detections, detections_in_zones, detections_out_zones,config)\n",
        "    return annotate_frame(frame, filtered, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-dsim3RcgV"
      },
      "source": [
        "# Process and Annotate Video Frames for Vehicle Turn Detection\n",
        "\n",
        "The **process_video** function reads a video frame-by-frame, processes each frame to detect and annotate vehicles, and outputs the results either to a video file or a live display window. Here's what it does:\n",
        "\n",
        "1. Frame Extraction: Uses a frame generator to read frames from the source video.\n",
        "\n",
        "2. Progress Tracking: Displays a live progress bar using the rich library to monitor video processing.\n",
        "\n",
        "3. Annotation Pipeline:\n",
        "    *   For each frame, it calls **process_frame**() to detect vehicles, determine turn behavior, and apply annotations.\n",
        "\n",
        "4. Output Handling:\n",
        "\n",
        "    *   If output path is specified: Saves the annotated video to disk using VideoSink.\n",
        "    *   Otherwise: Displays annotated frames live using OpenCV (**cv2.imshow**).\n",
        "\n",
        "5. Sample Frame Export: Saves a single annotated frame as an image (annotated_output.png) for preview or debugging.\n",
        "\n",
        "6. Returns the dictionary of vehicle turn states (vehicle_turns) tracked during the video processing.\n",
        "\n",
        "This function is the core executor of the turn detection pipeline, enabling both real-time display and file output of the analyzed results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za412PJPRcA4"
      },
      "outputs": [],
      "source": [
        "def process_video(config: Dict[str, Any]) -> None:\n",
        "    frame_generator = sv.get_video_frames_generator(config[\"source_video_path\"])\n",
        "    total = config[\"video_info\"].total_frames\n",
        "\n",
        "    with Progress() as progress:\n",
        "        task = progress.add_task(\"[green]Processing video...\", total=total)\n",
        "        if config[\"target_video_path\"]:\n",
        "            with sv.VideoSink(config[\"target_video_path\"], config[\"video_info\"]) as sink:\n",
        "                saved_sample = False\n",
        "                for frame in frame_generator:\n",
        "                    annotated = process_frame(frame, config)\n",
        "                    sink.write_frame(annotated)\n",
        "                    if not saved_sample:\n",
        "                        cv2.imwrite(\"annotated_output.png\", annotated)\n",
        "                        saved_sample = True\n",
        "                    progress.advance(task)\n",
        "        else:\n",
        "            for frame in frame_generator:\n",
        "                annotated = process_frame(frame, config)\n",
        "                cv2.imshow(\"Processed Video\", annotated)\n",
        "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "                    break\n",
        "                progress.advance(task)\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "    return detections_state[\"vehicle_turns\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NG4ESNuq9Zs"
      },
      "source": [
        "# Append Turn Analysis Summary Chart to Video Output\n",
        "\n",
        "The function add_final_summary_to_video enhances a processed video by appending a visual summary of vehicle turn statistics at the end. Here's what it does:\n",
        "\n",
        "1. Analyzes turn data using the provided vehicle turn state.\n",
        "\n",
        "2. Loads a bar chart image (turn_analysis.png) that visually represents turn statistics.\n",
        "\n",
        "3. Reads and copies all frames from the original processed video.\n",
        "\n",
        "4. Appends the chart image as static frames for 5 seconds at the end of the video.\n",
        "\n",
        "5. Saves the new video with the summary chart to the specified output path.\n",
        "\n",
        "6. Returns a structured JSON summary of the vehicle turn data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm56mopSq6ts"
      },
      "outputs": [],
      "source": [
        "def add_final_summary_to_video(video_path, vehicle_turns, output_path=\"final_output.mp4\"):\n",
        "    \"\"\"Add a final summary frame to the end of the video\"\"\"\n",
        "\n",
        "    # First analyze the turns\n",
        "    vehicle_turn_json = analyze_turns(vehicle_turns)\n",
        "\n",
        "    # Load the bar chart image\n",
        "    chart_img = cv2.imread(\"turn_analysis.png\")\n",
        "    if chart_img is None:\n",
        "        raise FileNotFoundError(\"turn_analysis.png not found.\")\n",
        "\n",
        "    # Read the original video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Resize chart to match video resolution\n",
        "    chart_img = cv2.resize(chart_img, (width, height))\n",
        "\n",
        "    # Create the output video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') #mp4v h264\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Copy all frames from the original video\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        out.write(frame)\n",
        "\n",
        "    # Append chart image as 5 seconds of frames\n",
        "    for _ in range(int(fps * 5)):\n",
        "        out.write(chart_img)\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Final video with chart saved as '{output_path}'\")\n",
        "\n",
        "    return vehicle_turn_json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdmIGrtbq5eV"
      },
      "source": [
        "# Init Video Processing (Full Video-Based Vehicle Turn Detection and Summary Pipeline)\n",
        "\n",
        "This function **run_full_vehicle_turn_pipeline** performs the complete pipeline for analyzing vehicle movements in a video. It:\n",
        "\n",
        "1. Processes the input video using a configured video processor to detect and trace vehicle movements.\n",
        "\n",
        "2. Analyzes vehicle turns (left, right, U-turn, straight) and records them.\n",
        "\n",
        "3. Generates a summary chart of the turn statistics and appends it to the output video.\n",
        "\n",
        "4. Returns a JSON summary of vehicle turn analytics for further use (e.g., visualization or question answering).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek-5O71YrFnp"
      },
      "outputs": [],
      "source": [
        "def run_full_vehicle_turn_pipeline(\n",
        "    source_video_path: str,\n",
        "    final_output_path: str = \"final_output.mp4\",\n",
        "    zones: Dict[str, list] = {},\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs the full pipeline: processes video, tracks turns, and appends summary.\n",
        "    \"\"\"\n",
        "    if not zones or not zones.get(\"entry\") or not zones.get(\"exit\"):\n",
        "        raise ValueError(\"'zones' must contain both non-empty 'entry' and 'exit' lists.\")\n",
        "\n",
        "    # Step 1: Setup and process the video\n",
        "    config = setup_video_processor(\n",
        "        source_video_path=source_video_path,\n",
        "        target_video_path=\"output_traced.mp4\",\n",
        "        zones=zones\n",
        "    )\n",
        "    vehicle_turns_state = process_video(config)\n",
        "\n",
        "    # Step 2: Append summary chart to the traced video\n",
        "    vehicle_turn_json = add_final_summary_to_video(\n",
        "        video_path=\"output_traced.mp4\",\n",
        "        vehicle_turns=vehicle_turns_state,\n",
        "        output_path=final_output_path\n",
        "    )\n",
        "\n",
        "    return vehicle_turn_json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewXOvNlM-_26"
      },
      "source": [
        "# Vehicle Turn Detection Summary & AI-Powered Question Answering.\n",
        "**convert_turn_stats_to_text(analysis_result)**:\n",
        "Converts the vehicle turn detection results (a JSON dictionary) into a readable text summary, including:\n",
        "\n",
        "1. Total vehicle count\n",
        "\n",
        "2. Turn type counts (right, left, U-turn, straight)\n",
        "\n",
        "3. Per-vehicle turn information.\n",
        "\n",
        "**Use:**\n",
        "This summary is later passed to a language model for answering questions.\n",
        "\n",
        "**create_pipeline(text_data)**:\n",
        "Creates a custom question-answering function qa_pipeline(question) that:\n",
        "\n",
        "1. Takes a natural language question\n",
        "\n",
        "2. Feeds it to Qwen along with the vehicle turn summary\n",
        "\n",
        "3. Returns only the assistant's reply from the model output\n",
        "\n",
        "**Purpose:**\n",
        "This abstracts the model usage so the user can ask follow-up questions based on video analytics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es4286u__GMU"
      },
      "outputs": [],
      "source": [
        "def convert_turn_stats_to_text(analysis_result):\n",
        "    turn_counts = analysis_result.get(\"turn_counts\", {})\n",
        "    turn_details = analysis_result.get(\"turn_details\", [])\n",
        "\n",
        "    total = analysis_result.get(\"total_vehicles\", 0)\n",
        "    right = turn_counts.get(\"Vehicles making right turns\", 0)\n",
        "    left = turn_counts.get(\"Vehicles making left turns\", 0)\n",
        "    u_turn = turn_counts.get(\"Vehicles making U-turns\", 0)\n",
        "    straight = turn_counts.get(\"Vehicles with no detected turns (Straight)\", 0)\n",
        "\n",
        "    summary_text = (\n",
        "    f\"A total of {total} cars were tracked during the analysis. \"\n",
        "    f\"Among them, {right} made right turns, {left} made left turns, \"\n",
        "    f\"{u_turn} performed U-turns (also referred to as 'uturns' or 'reverse turns'), and {straight} continued straight without making any turns. (also referred to as 'no turns')\"\n",
        "    )\n",
        "\n",
        "    if turn_details:\n",
        "        detail_sentences = [\n",
        "            f\"Vehicle ID {item['tracker_id']} made a {item['turn'].replace('_', ' ').lower()}.\"\n",
        "            for item in turn_details\n",
        "        ]\n",
        "        details_text = \" \".join(detail_sentences)\n",
        "        return f\"{summary_text}{details_text}\"\n",
        "    else:\n",
        "        return f\"{summary_text}. No individual vehicle turn details were recorded.\"\n",
        "\n",
        "\n",
        "\n",
        "# Load Qwen model and tokenizer (only once globally)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "generation_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "def create_pipeline(text_data):\n",
        "    \"\"\"\n",
        "    Create a simple function to handle QA using Qwen with the full text_data\n",
        "    \"\"\"\n",
        "    def qa_pipeline(question):\n",
        "        text = f\"\"\"\n",
        "        You are an expert in analyzing traffic video data, specifically vehicle turn behavior.\n",
        "\n",
        "        Answer the question as thoroughly as possible using only the provided context. If the answer is not present in the context, respond with: \"Answer is not available in the context.\" Do not provide fabricated or assumed information.\n",
        "\n",
        "        Context:\n",
        "        {text_data}\n",
        "\n",
        "        Based on the above context, answer the following question clearly and concisely:\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert in vehicle turn analysis. Respond clearly and accurately using only the provided context.\"},\n",
        "            {\"role\": \"user\", \"content\": text},\n",
        "        ]\n",
        "        response = generation_pipe(messages, max_new_tokens=1000)[0]\n",
        "        print(response)\n",
        "        assistant_response = \"\"\n",
        "        for msg in response['generated_text']:\n",
        "            if msg.get(\"role\") == \"assistant\":\n",
        "                assistant_response = msg.get(\"content\", \"\")\n",
        "                break\n",
        "        return assistant_response\n",
        "\n",
        "    return qa_pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyJSSXxLS5VZ"
      },
      "source": [
        "# Vehicle Turn Detection with Interactive Zone Drawing Using Gradio\n",
        "Interactive Vehicle Turn Detection with Zone-Based Video Analysis\n",
        "\n",
        "**gradio** is used to create an interactive web UI for uploading a video, processing it, and asking questions.\n",
        "\n",
        "**tempfile** is used to handle temporary storage of the uploaded video.\n",
        "\n",
        "This Python application provides an interactive web interface to detect vehicle turns in traffic videos by allowing users to manually draw polygonal zones on the first video frame. Built using Gradio for the UI and OpenCV for video processing, the tool enables the following workflow:\n",
        "\n",
        "1. Upload a traffic video in common formats like MP4.\n",
        "\n",
        "2. Extract and display the first frame of the video for zone drawing.\n",
        "\n",
        "3. Draw multiple polygonal zones In/Out(Z1, Z2, Z3, Z4, etc.) on the frame to define regions of interest in an intersection pattern.\n",
        "\n",
        "4. Visualize the drawn zones with distinct colors and labels.\n",
        "\n",
        "5. Analyze the video based on the defined zones to detect vehicle turns and movements.\n",
        "\n",
        "6. View the processed video highlighting vehicle turn events.\n",
        "\n",
        "7. Interact with the analysis by asking questions about vehicle turns through a natural language interface powered by a custom pipeline.\n",
        "\n",
        "\n",
        "(**encode_to_browser_safe_mp4**): This function converts a video file to a browser-safe MP4 format using **ffmpeg**.To transcode a video (any format) into an MP4 file that's optimized for web playback in browsers (like Chrome, Firefox, Safari, etc.).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ICAxAcnukpS"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import tempfile\n",
        "import subprocess\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Persistent state for document store and pipeline\n",
        "global_turn_json = None\n",
        "global_pipeline = None\n",
        "\n",
        "class ZoneDrawer:\n",
        "    def __init__(self):\n",
        "        self.zones = {'entry': [], 'exit': []}\n",
        "        self.current_frame = None\n",
        "        self.current_zone_type = 'entry'\n",
        "        self.video_path = None\n",
        "\n",
        "    def process_video(self, video_file):\n",
        "        \"\"\"Extract first frame from uploaded video\"\"\"\n",
        "        if video_file is None:\n",
        "            return None, \"Please upload a video file first.\"\n",
        "\n",
        "        try:\n",
        "            # Store video path for later use\n",
        "            self.video_path = video_file\n",
        "\n",
        "            # Read video and extract first frame\n",
        "            cap = cv2.VideoCapture(video_file)\n",
        "            ret, frame = cap.read()\n",
        "            cap.release()\n",
        "\n",
        "            if not ret:\n",
        "                return None, \"❌ Could not extract frame from video.\"\n",
        "\n",
        "            # Convert BGR to RGB for display\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            self.current_frame = frame_rgb\n",
        "\n",
        "            return frame_rgb, f\"✅ Video loaded! Frame size: {frame_rgb.shape[1]}x{frame_rgb.shape[0]}. Draw zones then analyze.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"❌ Error processing video: {str(e)}\"\n",
        "\n",
        "    def set_zone_type(self, zone_type):\n",
        "        \"\"\"Set current zone type\"\"\"\n",
        "        self.current_zone_type = zone_type\n",
        "        color = \"🟢 GREEN\" if zone_type == 'entry' else \"🔴 RED\"\n",
        "        return f\"Drawing mode: {zone_type.upper()} zones ({color})\"\n",
        "\n",
        "    def process_drawing(self, image_data):\n",
        "        \"\"\"Process the drawn image and extract polygon points\"\"\"\n",
        "        if image_data is None:\n",
        "            return None, \"No drawing data received.\"\n",
        "\n",
        "        try:\n",
        "            # Convert the drawing to numpy array\n",
        "            if isinstance(image_data, dict) and 'layers' in image_data:\n",
        "                drawing = image_data['layers'][0] if image_data['layers'] else image_data['background']\n",
        "            else:\n",
        "                drawing = image_data\n",
        "\n",
        "            # Convert PIL Image to numpy array\n",
        "            if isinstance(drawing, Image.Image):\n",
        "                drawing_array = np.array(drawing)\n",
        "            else:\n",
        "                drawing_array = drawing\n",
        "\n",
        "            # Extract polygon points from the drawing\n",
        "            points = self.extract_polygon_points(drawing_array)\n",
        "\n",
        "            if len(points) < 3:\n",
        "                return self.show_current_zones(), \"⚠️ Please draw a polygon with at least 3 points.\"\n",
        "\n",
        "            # Add to current zone type\n",
        "            self.zones[self.current_zone_type].append(points)\n",
        "\n",
        "            # Create updated visualization\n",
        "            result_image = self.create_zone_visualization()\n",
        "\n",
        "            zone_count = len(self.zones[self.current_zone_type])\n",
        "            return result_image, f\"✅ Added {self.current_zone_type} zone #{zone_count}! Total: Entry={len(self.zones['entry'])}, Exit={len(self.zones['exit'])}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return self.show_current_zones(), f\"❌ Error processing drawing: {str(e)}\"\n",
        "\n",
        "    def extract_polygon_points(self, drawing_array):\n",
        "        \"\"\"Extract polygon points from drawn image\"\"\"\n",
        "        # Convert to grayscale for processing\n",
        "        if len(drawing_array.shape) == 3:\n",
        "            gray = cv2.cvtColor(drawing_array, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = drawing_array\n",
        "\n",
        "        # Find contours in the drawing\n",
        "        contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        if not contours:\n",
        "            return []\n",
        "\n",
        "        # Get the largest contour\n",
        "        largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "        # Approximate the contour to reduce points\n",
        "        epsilon = 0.02 * cv2.arcLength(largest_contour, True)\n",
        "        approx = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
        "\n",
        "        # Convert to list of [x, y] points\n",
        "        points = [[int(point[0][0]), int(point[0][1])] for point in approx]\n",
        "\n",
        "        return points\n",
        "\n",
        "    def create_zone_visualization(self):\n",
        "        \"\"\"Create visualization with all zones\"\"\"\n",
        "        if self.current_frame is None:\n",
        "            return None\n",
        "\n",
        "        # Create a copy of the frame\n",
        "        vis_frame = self.current_frame.copy()\n",
        "\n",
        "        # Draw all zones\n",
        "        for zone_type, zone_list in self.zones.items():\n",
        "            color = (0, 255, 0) if zone_type == 'entry' else (255, 0, 0)\n",
        "            zone_counter = 1\n",
        "            for zone_points in zone_list:\n",
        "                if len(zone_points) >= 3:\n",
        "                    # Convert points to numpy array\n",
        "                    pts = np.array(zone_points, np.int32)\n",
        "\n",
        "                    # Draw filled polygon with transparency\n",
        "                    overlay = vis_frame.copy()\n",
        "                    cv2.fillPoly(overlay, [pts], color)\n",
        "                    cv2.addWeighted(vis_frame, 0.7, overlay, 0.3, 0, vis_frame)\n",
        "\n",
        "                    # Draw border\n",
        "                    cv2.polylines(vis_frame, [pts], isClosed=True, color=color, thickness=3)\n",
        "\n",
        "                    # Add zone label (Z1, Z2, Z3, Z4)\n",
        "                    center = np.mean(pts, axis=0).astype(int)\n",
        "                    zone_label = f\"Z{zone_counter}\"\n",
        "                    cv2.putText(vis_frame, zone_label, (center[0]-15, center[1]),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "                    zone_counter += 1\n",
        "\n",
        "        return vis_frame\n",
        "\n",
        "    def show_current_zones(self):\n",
        "        \"\"\"Show current zones without adding new ones\"\"\"\n",
        "        return self.create_zone_visualization()\n",
        "\n",
        "    def clear_current_zones(self):\n",
        "        \"\"\"Clear all zones of the current selected type\"\"\"\n",
        "        zone_type = self.current_zone_type\n",
        "        if zone_type in self.zones:\n",
        "            count = len(self.zones[zone_type])\n",
        "            self.zones[zone_type] = []\n",
        "            result_image = self.create_zone_visualization()\n",
        "            return result_image, f\"🗑️ Cleared {count} {zone_type} zones.\"\n",
        "        else:\n",
        "            return self.show_current_zones(), f\"⚠️ No zones found for type '{zone_type}'.\"\n",
        "\n",
        "    def clear_last_zone(self):\n",
        "        \"\"\"Remove the most recently drawn zone from the current zone type\"\"\"\n",
        "        zone_type = self.current_zone_type\n",
        "        if zone_type in self.zones and self.zones[zone_type]:\n",
        "            removed = self.zones[zone_type].pop()\n",
        "            result_image = self.create_zone_visualization()\n",
        "            return result_image, f\"🗑️ Removed last {zone_type} zone. Remaining: {len(self.zones[zone_type])}\"\n",
        "        else:\n",
        "            result_image = self.show_current_zones()\n",
        "            return result_image, f\"⚠️ No {zone_type} zones to remove.\"\n",
        "\n",
        "    def clear_all_zones(self):\n",
        "        \"\"\"Clear all zones\"\"\"\n",
        "        total = len(self.zones['entry']) + len(self.zones['exit'])\n",
        "        self.zones = {'entry': [], 'exit': []}\n",
        "        result_image = self.show_current_zones()\n",
        "        return result_image, f\"🔄 Cleared all {total} zones.\"\n",
        "\n",
        "    def get_zone_info(self):\n",
        "        \"\"\"Get current zone information\"\"\"\n",
        "        entry_count = len(self.zones['entry'])\n",
        "        exit_count = len(self.zones['exit'])\n",
        "\n",
        "        info = (\n",
        "        f\"📊 **Zone Summary:**\\n\\n\"\n",
        "        f\"🟢 **Entry Zones:** {entry_count}\\n\"\n",
        "        f\"🔴 **Exit Zones:** {exit_count}\\n\"\n",
        "        f\"📍 **Total Zones:** {entry_count + exit_count}\\n\\n\"\n",
        "        f\"📋 **Zone Coordinates:**\\n\"\n",
        "        f\"```json\\n\"\n",
        "        f\"{json.dumps(self.zones, indent=2)}\\n\"\n",
        "        f\"```\\n\\n\"\n",
        "        f\"🎯 **Ready for Analysis:** {'✅ Yes' if (entry_count + exit_count) > 0 else '❌ Draw zones first'}\"\n",
        "        )\n",
        "        return info\n",
        "\n",
        "    def get_zones_for_pipeline(self):\n",
        "        \"\"\"Get zones in format expected by pipeline\"\"\"\n",
        "        return self.zones\n",
        "\n",
        "# Initialize the zone drawer\n",
        "zone_drawer = ZoneDrawer()\n",
        "\n",
        "def analyze_video_with_zones(video_file_path):\n",
        "    \"\"\"Analyze video using drawn zones\"\"\"\n",
        "    global global_pipeline, global_turn_json\n",
        "\n",
        "    if not video_file_path:\n",
        "        return None, \"Please upload a video file.\"\n",
        "\n",
        "    # Check if zones are drawn\n",
        "    zones = zone_drawer.get_zones_for_pipeline()\n",
        "    if not zones['entry'] and not zones['exit']:\n",
        "        return None, \"❌ Please draw entry and/or exit zones before analyzing.\"\n",
        "\n",
        "    try:\n",
        "        # Create temporary file\n",
        "        with open(video_file_path, \"rb\") as source_file:\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as tmp_file:\n",
        "                tmp_file.write(source_file.read())\n",
        "                tmp_video_path = tmp_file.name\n",
        "\n",
        "        raw_output_path = \"raw_output.mp4\"\n",
        "        browser_safe_path = \"final_output_with_summary.mp4\"\n",
        "\n",
        "        # Save zones to temporary file for pipeline\n",
        "        zones_file = \"temp_zones.json\"\n",
        "        with open(zones_file, 'w') as f:\n",
        "            json.dump(zones, f, indent=2)\n",
        "\n",
        "        # Run the full vehicle turn detection pipeline with zones\n",
        "        global_turn_json = run_full_vehicle_turn_pipeline(\n",
        "            source_video_path=tmp_video_path,\n",
        "            final_output_path=raw_output_path,\n",
        "            zones=zones\n",
        "        )\n",
        "\n",
        "        # Re-encode video\n",
        "        encode_to_browser_safe_mp4(raw_output_path, browser_safe_path)\n",
        "\n",
        "        # Create document store and pipeline for QA\n",
        "        text_data = convert_turn_stats_to_text(global_turn_json)\n",
        "        global_pipeline = create_pipeline(text_data)\n",
        "\n",
        "        # Clean up temporary files\n",
        "        if os.path.exists(zones_file):\n",
        "            os.remove(zones_file)\n",
        "        if os.path.exists(tmp_video_path):\n",
        "            os.remove(tmp_video_path)\n",
        "\n",
        "        zone_summary = f\"Entry zones: {len(zones['entry'])}, Exit zones: {len(zones['exit'])}\"\n",
        "        return browser_safe_path, f\"✅ Video analyzed successfully with {zone_summary}! You can now ask questions.\",\"/content/turn_analysis.png\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"❌ Error during analysis: {str(e)}\",None\n",
        "\n",
        "def answer_question(user_question):\n",
        "    \"\"\"Answer questions about the analyzed video\"\"\"\n",
        "    if not global_pipeline or not global_turn_json:\n",
        "        return \"Please analyze a video first.\"\n",
        "    return global_pipeline(user_question)\n",
        "\n",
        "def encode_to_browser_safe_mp4(input_path: str, output_path: str):\n",
        "    \"\"\"Convert video to browser-safe format\"\"\"\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-y\", \"-i\", input_path,\n",
        "        \"-vcodec\", \"libx264\", \"-preset\", \"ultrafast\",\n",
        "        \"-acodec\", \"aac\", \"-movflags\", \"+faststart\",\n",
        "        output_path\n",
        "    ]\n",
        "    try:\n",
        "        subprocess.run(cmd, check=True)\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"Error: ffmpeg failed to convert video to browser-safe format.\")\n",
        "\n",
        "# Create Gradio interface\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"🚗 Vehicle Turn Detection with Zone Drawing\", theme=gr.themes.Soft()) as interface:\n",
        "        gr.HTML(\"\"\"\n",
        "        <h1 style=\"text-align: center;\">🚗 Vehicle Turn Detection with Zone Drawing</h1>\n",
        "        <p style=\"text-align: center;\">Upload video → Draw zones → Analyze turns → Ask questions</p>\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tab(\"📹 Step 1: Upload & Draw Zones\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=2):\n",
        "                    # Video upload\n",
        "                    video_input = gr.File(\n",
        "                        label=\"📹 Upload Video\",\n",
        "                        file_types=[\".mp4\", \".avi\", \".mov\"],\n",
        "                        type=\"filepath\"\n",
        "                    )\n",
        "\n",
        "                    # Frame display and drawing area\n",
        "                    frame_display = gr.ImageEditor(\n",
        "                        label=\"🎨 Draw Zones (Click and drag to draw polygons)\",\n",
        "                        type=\"pil\",\n",
        "                        brush=gr.Brush(default_size=12),\n",
        "                        height=600\n",
        "                    )\n",
        "\n",
        "                    # Zone type selector\n",
        "                    zone_type = gr.Radio(\n",
        "                        choices=[\"entry\", \"exit\"],\n",
        "                        value=\"entry\",\n",
        "                        label=\"🎯 Zone Type\",\n",
        "                        info=\"Select whether to draw entry (green) or exit (red) zones\"\n",
        "                    )\n",
        "\n",
        "                    # Control buttons\n",
        "                    with gr.Row():\n",
        "                        save_zone_btn = gr.Button(\"✅ Save Zone\", variant=\"primary\")\n",
        "                        clear_last_btn = gr.Button(\"🗑️ Remove Last Zone\")\n",
        "                        clear_current_btn = gr.Button(\"🗑️ Clear Current Type\")\n",
        "                        clear_all_btn = gr.Button(\"🔄 Reset All\")\n",
        "\n",
        "                    # Status message\n",
        "                    zone_status = gr.Textbox(\n",
        "                        label=\"📢 Zone Status\",\n",
        "                        interactive=False,\n",
        "                        max_lines=2\n",
        "                    )\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    # Zone information\n",
        "                    zone_info = gr.Markdown(\n",
        "                        value=\"Upload a video to start drawing zones.\",\n",
        "                        label=\"📊 Zone Information\"\n",
        "                    )\n",
        "\n",
        "                    # Instructions\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ### 📖 Instructions:\n",
        "\n",
        "                    1. **Upload** a video file\n",
        "                    2. **Select** zone type (Entry/Exit)\n",
        "                    3. **Draw** polygons on the frame\n",
        "                    4. **Save** each zone after drawing\n",
        "                    5. Go to **Step 2** to analyze\n",
        "\n",
        "                    ### 🎨 Drawing Tips:\n",
        "                    - Use sketch tool to draw polygons\n",
        "                    - Entry zones → Green overlay\n",
        "                    - Exit zones → Red overlay\n",
        "                    - Draw closed shapes for best results\n",
        "\n",
        "                    ### 🔄 **IMPORTANT - Zone Layout for Accurate Turn Detection:**\n",
        "\n",
        "                    **Recommended Zone Pattern:**\n",
        "                    ```\n",
        "                               Z1\n",
        "                            (In/Out)\n",
        "                       Z2             Z3\n",
        "                    (In/Out)       (In/Out)\n",
        "                               Z4\n",
        "                            (In/Out)\n",
        "                    ```\n",
        "\n",
        "                    ### 📍 **Zone Positioning Guidelines:**\n",
        "                    - **Z1(In/Out) (Top)**: North/Top zone\n",
        "                    - **Z2(In/Out) (Left)**: West/Left zone\n",
        "                    - **Z3(In/Out) (Bottom)**: South/Bottom zone\n",
        "                    - **Z4(In/Out) (Right)**: East/Right zone\n",
        "                    - **Draw Anticlockwise Zones patterns**\n",
        "\n",
        "                    ### 🚗 Turn Detection Accuracy:\n",
        "                    - Zone drawing direction affects turn classification\n",
        "                    - Match drawing direction with vehicle flow for best results\n",
        "                    - Incorrect direction may cause turn misclassification\n",
        "\n",
        "                    \"\"\")\n",
        "\n",
        "        with gr.Tab(\"🔍 Step 2: Analyze Video\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    analyze_btn = gr.Button(\"🚀 Analyze Video with Zones\", variant=\"primary\", size=\"lg\")\n",
        "                    analysis_status = gr.Textbox(label=\"📊 Analysis Status\", interactive=False)\n",
        "                    turn_analysis_image = gr.Image(label=\"🖼️ Turn Analysis Overview\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    video_output = gr.Video(label=\"📹 Processed Video\")\n",
        "\n",
        "        with gr.Tab(\"❓ Step 3: Ask Questions\"):\n",
        "            gr.Markdown(\"### Ask questions about the analyzed video:\")\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    question_input = gr.Textbox(\n",
        "                        label=\"💬 Your Question\",\n",
        "                        placeholder=\"e.g., How many U-turns were made? What was the most common turn type?\",\n",
        "                        lines=2\n",
        "                    )\n",
        "                with gr.Column():\n",
        "                    answer_output = gr.Textbox(\n",
        "                        label=\"🤖 Answer\",\n",
        "                        lines=4,\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "        # Event handlers for zone drawing\n",
        "        video_input.change(\n",
        "            fn=zone_drawer.process_video,\n",
        "            inputs=[video_input],\n",
        "            outputs=[frame_display, zone_status]\n",
        "        )\n",
        "\n",
        "        zone_type.change(\n",
        "            fn=zone_drawer.set_zone_type,\n",
        "            inputs=[zone_type],\n",
        "            outputs=[zone_status]\n",
        "        )\n",
        "\n",
        "        save_zone_btn.click(\n",
        "            fn=zone_drawer.process_drawing,\n",
        "            inputs=[frame_display],\n",
        "            outputs=[frame_display, zone_status]\n",
        "        ).then(\n",
        "            fn=zone_drawer.get_zone_info,\n",
        "            outputs=[zone_info]\n",
        "        )\n",
        "\n",
        "        clear_last_btn.click(\n",
        "            fn=zone_drawer.clear_last_zone,\n",
        "            outputs=[frame_display, zone_status]\n",
        "        ).then(\n",
        "            fn=zone_drawer.get_zone_info,\n",
        "            outputs=[zone_info]\n",
        "        )\n",
        "\n",
        "        clear_current_btn.click(\n",
        "            fn=zone_drawer.clear_current_zones,\n",
        "            outputs=[frame_display, zone_status]\n",
        "        ).then(\n",
        "            fn=zone_drawer.get_zone_info,\n",
        "            outputs=[zone_info]\n",
        "        )\n",
        "\n",
        "\n",
        "        clear_all_btn.click(\n",
        "            fn=zone_drawer.clear_all_zones,\n",
        "            outputs=[frame_display, zone_status]\n",
        "        ).then(\n",
        "            fn=zone_drawer.get_zone_info,\n",
        "            outputs=[zone_info]\n",
        "        )\n",
        "\n",
        "        # Event handler for video analysis\n",
        "        analyze_btn.click(\n",
        "            fn=lambda: analyze_video_with_zones(zone_drawer.video_path),\n",
        "            outputs=[video_output, analysis_status, turn_analysis_image]\n",
        "        )\n",
        "\n",
        "        # Event handler for Q&A\n",
        "        question_input.submit(\n",
        "            fn=answer_question,\n",
        "            inputs=[question_input],\n",
        "            outputs=[answer_output]\n",
        "        )\n",
        "\n",
        "        question_input.change(\n",
        "            fn=answer_question,\n",
        "            inputs=[question_input],\n",
        "            outputs=[answer_output]\n",
        "        )\n",
        "\n",
        "    return interface\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface = create_interface()\n",
        "    interface.launch(\n",
        "        share=True,\n",
        "        debug=True,\n",
        "        show_error=True\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}